{"version":3,"file":"index.mjs","sources":["../../src/mastra/storage/index.ts","../../src/mastra/inngest/client.ts","../../src/mastra/inngest/index.ts","../../src/mastra/agents/jobApplicationAgent.ts","../../src/mastra/workflows/jobApplicationWorkflow.ts","../../src/mastra/index.ts","../../node_modules/hono/dist/utils/mime.js","../../node_modules/hono/dist/utils/html.js","../../node_modules/hono/dist/helper/html/index.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/server/a2a/store.js","../../node_modules/hono/dist/compose.js","../../node_modules/hono/dist/request/constants.js","../../node_modules/hono/dist/utils/body.js","../../node_modules/hono/dist/utils/url.js","../../node_modules/hono/dist/request.js","../../node_modules/hono/dist/context.js","../../node_modules/hono/dist/router.js","../../node_modules/hono/dist/utils/constants.js","../../node_modules/hono/dist/hono-base.js","../../node_modules/hono/dist/router/reg-exp-router/node.js","../../node_modules/hono/dist/router/reg-exp-router/trie.js","../../node_modules/hono/dist/router/reg-exp-router/router.js","../../node_modules/hono/dist/router/smart-router/router.js","../../node_modules/hono/dist/router/trie-router/node.js","../../node_modules/hono/dist/router/trie-router/router.js","../../node_modules/hono/dist/hono.js","../../node_modules/hono/dist/middleware/cors/index.js","../../node_modules/hono/dist/utils/color.js","../../node_modules/hono/dist/middleware/logger/index.js","../../node_modules/hono/dist/http-exception.js","../../node_modules/hono/dist/middleware/timeout/index.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-G3PMV62Z.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-5QUKZCEF.js","../../node_modules/hono/dist/utils/stream.js","../../node_modules/hono/dist/helper/streaming/utils.js","../../node_modules/hono/dist/helper/streaming/stream.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-MMROOK5J.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-OW4FX5TS.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-LF2ZLOFP.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-CY4TP3FK.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-GGCXLQ4J.js","../../node_modules/hono/dist/middleware/body-limit/index.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-A3ZHTDWB.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-QBWF6U7Z.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-ROA7BCHD.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-LUPY3MQY.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-B43YAQJR.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/server/handlers/vNextNetwork.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-R7NOGUZG.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-7QEJ5QG5.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-KV6VHX4V.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-WHN4VX55.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-OZGPYA7A.js","../../node_modules/mastra/node_modules/@mastra/deployer/node_modules/@mastra/server/dist/chunk-TTHEEIZ3.js","../../node_modules/mastra/node_modules/@mastra/deployer/dist/server/index.js","../../node_modules/mastra/dist/templates/dev.entry.js"],"sourcesContent":["import { PostgresStore } from \"@mastra/pg\";\n\n// Create a single shared PostgreSQL storage instance\nexport const sharedPostgresStorage = new PostgresStore({\n  connectionString:\n    process.env.DATABASE_URL || \"postgresql://localhost:5432/mastra\",\n});\n","import { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\n// Use development configuration when NODE_ENV is not \"production\"\nexport const inngest = new Inngest(\n  process.env.NODE_ENV === \"production\"\n    ? {\n        id: \"replit-agent-workflow\",\n        name: \"Replit Agent Workflow System\",\n      }\n    : {\n        id: \"mastra\",\n        baseUrl: \"http://localhost:3000\",\n        isDev: true,\n        middleware: [realtimeMiddleware()],\n      },\n);\n","import { inngest } from \"./client\";\nimport { init, InngestWorkflow } from \"@mastra/inngest\";\nimport { registerApiRoute as originalRegisterApiRoute } from \"@mastra/core/server\";\nimport { type Mastra } from \"@mastra/core\";\nimport { type Inngest, InngestFunction, NonRetriableError } from \"inngest\";\nimport { serve as originalInngestServe } from \"inngest/hono\";\n\n// Initialize Inngest with Mastra to get Inngest-compatible workflow helpers\nconst {\n  createWorkflow: originalCreateWorkflow,\n  createStep,\n  cloneStep,\n} = init(inngest);\n\nexport function createWorkflow(\n  params: Parameters<typeof originalCreateWorkflow>[0],\n): ReturnType<typeof originalCreateWorkflow> {\n  return originalCreateWorkflow({\n    ...params,\n    retryConfig: {\n      attempts: 3,\n      ...(params.retryConfig ?? {}),\n    },\n  });\n}\n\n// Export the Inngest client and Inngest-compatible workflow helpers\nexport { inngest, createStep, cloneStep };\n\nconst inngestFunctions: InngestFunction.Any[] = [];\n\n// Create a middleware for Inngest to be able to route triggers to Mastra directly.\nexport function registerApiRoute<P extends string>(\n  ...args: Parameters<typeof originalRegisterApiRoute<P>>\n): ReturnType<typeof originalRegisterApiRoute<P>> {\n  const [path, options] = args;\n  if (path.startsWith(\"/api/\") || typeof options !== \"object\") {\n    // This will throw an error.\n    return originalRegisterApiRoute(...args);\n  }\n  inngestFunctions.push(\n    inngest.createFunction(\n      {\n        id: `api-${path.replace(/^\\/+/, \"\").replaceAll(/\\/+/g, \"-\")}`,\n        name: path,\n      },\n      {\n        event: `event/api.${path.replace(/^\\/+/, \"\").replaceAll(/\\/+/g, \".\")}`,\n      },\n      async ({ event, step }) => {\n        await step.run(\"forward request to Mastra\", async () => {\n          // It is hard to obtain an internal handle on the Hono server,\n          // so we just forward the request to the local Mastra server.\n          const response = await fetch(`http://localhost:5000${path}`, {\n            method: event.data.method,\n            headers: event.data.headers,\n            body: event.data.body,\n          });\n\n          if (!response.ok) {\n            if (\n              (response.status >= 500 && response.status < 600) ||\n              response.status == 429 ||\n              response.status == 408\n            ) {\n              // 5XX, 429 (Rate-Limit Exceeded), 408 (Request Timeout) are retriable.\n              throw new Error(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            } else {\n              // All other errors are non-retriable.\n              throw new NonRetriableError(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            }\n          }\n        });\n      },\n    ),\n  );\n\n  return originalRegisterApiRoute(...args);\n}\n\nexport function registerCronWorkflow(cronExpression: string, workflow: any) {\n  const f = inngest.createFunction(\n    { id: \"cron-trigger\" },\n    [{ event: \"replit/cron.trigger\" }, { cron: cronExpression }],\n    async ({ event, step }) => {\n      const run = await workflow.createRunAsync();\n      const result = await run.start({ inputData: {} });\n      return result;\n    },\n  );\n  inngestFunctions.push(f);\n}\n\nexport function inngestServe({\n  mastra,\n  inngest,\n}: {\n  mastra: Mastra;\n  inngest: Inngest;\n}): ReturnType<typeof originalInngestServe> {\n  const wfs = mastra.getWorkflows();\n\n  const functions = new Set<InngestFunction.Any>();\n  for (const wf of Object.values(wfs)) {\n    if (!(wf instanceof InngestWorkflow)) {\n      continue;\n    }\n    wf.__registerMastra(mastra);\n    for (const f of wf.getFunctions()) {\n      functions.add(f);\n    }\n  }\n  for (const fn of inngestFunctions) {\n    functions.add(fn);\n  }\n  let serveHost: string | undefined = undefined;\n  if (process.env.NODE_ENV === \"production\") {\n    if (process.env.REPLIT_DOMAINS) {\n      serveHost = `https://${process.env.REPLIT_DOMAINS.split(\",\")[0]}`;\n    }\n  } else {\n    serveHost = \"http://localhost:5000\";\n  }\n  return originalInngestServe({\n    client: inngest,\n    functions: Array.from(functions),\n    serveHost,\n  });\n}\n","import { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { sharedPostgresStorage } from \"../storage\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport { jobSearchTool } from \"../tools/jobSearchTool\";\nimport { resumeParserTool } from \"../tools/resumeParserTool\";\nimport { applicationFillerTool } from \"../tools/applicationFillerTool\";\nimport { sheetsTrackerTool } from \"../tools/sheetsTrackerTool\";\n\nconst openai = createOpenAI({\n  baseURL: process.env.OPENAI_BASE_URL || undefined,\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nexport const jobApplicationAgent = new Agent({\n  name: \"Job Application Agent\",\n  instructions: `You are an intelligent job application automation agent. Your role is to:\n\n1. Search for relevant tech jobs on LinkedIn and Naukri that match the specified criteria\n2. Analyze job postings to determine if they're suitable for application\n3. Parse resume information to extract relevant profile data\n4. Automatically fill and submit job applications using the resume data\n5. Track all applications in a Google Sheets spreadsheet for monitoring\n\nYou should be thorough, professional, and ensure all applications are high-quality. Always provide detailed feedback about the job search and application process.\n\nWhen processing jobs:\n- Focus on positions that match the specified experience level\n- Prioritize jobs posted within the last week\n- Ensure applications are personalized with relevant skills and experience\n- Track success/failure rates and provide insights`,\n\n  model: openai(\"gpt-4o\"),\n  tools: {\n    jobSearchTool,\n    resumeParserTool,\n    applicationFillerTool,\n    sheetsTrackerTool,\n  },\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true,\n      },\n      lastMessages: 10,\n    },\n    storage: sharedPostgresStorage,\n  }),\n});","import { createWorkflow, createStep } from \"../inngest\";\nimport { z } from \"zod\";\nimport { jobApplicationAgent } from \"../agents/jobApplicationAgent\";\nimport { RuntimeContext } from \"@mastra/core/di\";\n\nconst searchJobsStep = createStep({\n  id: \"search-jobs-step\",\n  description: \"Search for tech jobs posted this week on LinkedIn and Naukri\",\n  inputSchema: z.object({\n    jobTitle: z.string().default(\"Software Developer\").describe(\"Job title to search for\"),\n    experienceLevel: z.enum([\"entry\", \"mid\", \"senior\"]).default(\"mid\").describe(\"Experience level requirement\"),\n  }),\n  outputSchema: z.object({\n    jobs: z.array(z.object({\n      title: z.string(),\n      company: z.string(),\n      location: z.string(),\n      description: z.string(),\n      url: z.string(),\n      postedDate: z.string(),\n      experience: z.string(),\n      source: z.string(),\n    })),\n    jobCount: z.number(),\n  }),\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { jobTitle, experienceLevel } = inputData;\n    \n    logger?.info('🔍 [JobWorkflow] Starting job search step', { jobTitle, experienceLevel });\n    \n    const runtimeContext = new RuntimeContext();\n    \n    const { text } = await jobApplicationAgent.generate([\n      { \n        role: \"user\", \n        content: `Search for ${jobTitle} positions with ${experienceLevel} experience level that were posted this week. Use the job search tool to find relevant opportunities on LinkedIn and Naukri.` \n      },\n    ], {\n      resourceId: \"job-search-bot\",\n      threadId: `job-search-${Date.now()}`,\n      maxSteps: 3,\n      runtimeContext,\n    });\n    \n    logger?.info('✅ [JobWorkflow] Job search completed', { response: text });\n    \n    // For demo purposes, return mock data since the tools are simulation\n    const mockJobs = [\n      {\n        title: \"Full Stack Developer\",\n        company: \"Tech Solutions Inc.\",\n        location: \"Mumbai, India\",\n        description: \"Looking for a skilled full stack developer with React and Node.js experience\",\n        url: \"https://linkedin.com/jobs/view/123456789\",\n        postedDate: \"2 days ago\",\n        experience: \"2-4 years\",\n        source: \"LinkedIn\"\n      },\n      {\n        title: \"React Developer\",\n        company: \"Innovation Labs\",\n        location: \"Bangalore, India\", \n        description: \"Join our team as a React developer to build modern web applications\",\n        url: \"https://naukri.com/job-listings/react-developer-987654321\",\n        postedDate: \"1 day ago\",\n        experience: \"1-3 years\",\n        source: \"Naukri\"\n      }\n    ];\n    \n    return {\n      jobs: mockJobs,\n      jobCount: mockJobs.length,\n    };\n  },\n});\n\nconst parseResumeStep = createStep({\n  id: \"parse-resume-step\",\n  description: \"Parse PDF resume and extract developer profile information\",\n  inputSchema: z.object({\n    resumePath: z.string().default(\"/tmp/resume.pdf\").describe(\"Path to the PDF resume file\"),\n    jobs: z.array(z.object({\n      title: z.string(),\n      company: z.string(),\n      location: z.string(),\n      description: z.string(),\n      url: z.string(),\n      postedDate: z.string(),\n      experience: z.string(),\n      source: z.string(),\n    })),\n    jobCount: z.number(),\n  }),\n  outputSchema: z.object({\n    jobs: z.array(z.object({\n      title: z.string(),\n      company: z.string(),\n      location: z.string(),\n      description: z.string(),\n      url: z.string(),\n      postedDate: z.string(),\n      experience: z.string(),\n      source: z.string(),\n    })),\n    jobCount: z.number(),\n    resumeData: z.object({\n      personalInfo: z.object({\n        name: z.string(),\n        email: z.string(),\n        phone: z.string(),\n        location: z.string(),\n        linkedIn: z.string().optional(),\n        github: z.string().optional(),\n      }),\n      summary: z.string(),\n      experience: z.array(z.object({\n        title: z.string(),\n        company: z.string(),\n        duration: z.string(),\n        description: z.string(),\n      })),\n      skills: z.array(z.string()),\n      education: z.array(z.object({\n        degree: z.string(),\n        institution: z.string(),\n        year: z.string(),\n      })),\n      projects: z.array(z.object({\n        name: z.string(),\n        description: z.string(),\n        technologies: z.array(z.string()),\n      })),\n    }),\n  }),\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { resumePath, jobs, jobCount } = inputData;\n    \n    logger?.info('📄 [JobWorkflow] Starting resume parsing step', { resumePath });\n    \n    const runtimeContext = new RuntimeContext();\n    \n    const { text } = await jobApplicationAgent.generate([\n      { \n        role: \"user\", \n        content: `Parse the resume at ${resumePath} and extract all relevant information including personal details, skills, experience, and education. Use the resume parser tool to extract structured data.` \n      },\n    ], {\n      resourceId: \"resume-parser-bot\",\n      threadId: `resume-parse-${Date.now()}`,\n      maxSteps: 3,\n      runtimeContext,\n    });\n    \n    logger?.info('✅ [JobWorkflow] Resume parsing completed', { response: text });\n    \n    // Return mock resume data for demo\n    const mockResumeData = {\n      personalInfo: {\n        name: \"Alex Developer\",\n        email: \"alex.developer@email.com\",\n        phone: \"+91-9876543210\",\n        location: \"Mumbai, India\",\n        linkedIn: \"linkedin.com/in/alexdeveloper\",\n        github: \"github.com/alexdeveloper\"\n      },\n      summary: \"Experienced Full Stack Developer with 3+ years in React, Node.js, and modern web technologies\",\n      experience: [\n        {\n          title: \"Full Stack Developer\",\n          company: \"Tech Solutions Inc.\",\n          duration: \"2+ years\",\n          description: \"Developed scalable web applications using React, Node.js, and MongoDB\"\n        }\n      ],\n      skills: [\"JavaScript\", \"TypeScript\", \"React\", \"Node.js\", \"MongoDB\", \"PostgreSQL\", \"AWS\"],\n      education: [\n        {\n          degree: \"B.Tech Computer Science\",\n          institution: \"Mumbai University\",\n          year: \"2021\"\n        }\n      ],\n      projects: [\n        {\n          name: \"E-commerce Platform\",\n          description: \"Full-stack application with payment integration\",\n          technologies: [\"React\", \"Node.js\", \"MongoDB\"]\n        }\n      ]\n    };\n    \n    return {\n      jobs,\n      jobCount,\n      resumeData: mockResumeData,\n    };\n  },\n});\n\nconst applyToJobsStep = createStep({\n  id: \"apply-to-jobs-step\",\n  description: \"Apply to jobs automatically using resume data and track applications\",\n  inputSchema: z.object({\n    jobs: z.array(z.object({\n      title: z.string(),\n      company: z.string(),\n      location: z.string(),\n      description: z.string(),\n      url: z.string(),\n      postedDate: z.string(),\n      experience: z.string(),\n      source: z.string(),\n    })),\n    resumeData: z.object({\n      personalInfo: z.object({\n        name: z.string(),\n        email: z.string(),\n        phone: z.string(),\n        location: z.string(),\n        linkedIn: z.string().optional(),\n        github: z.string().optional(),\n      }),\n      summary: z.string(),\n      experience: z.array(z.object({\n        title: z.string(),\n        company: z.string(),\n        duration: z.string(),\n        description: z.string(),\n      })),\n      skills: z.array(z.string()),\n      education: z.array(z.object({\n        degree: z.string(),\n        institution: z.string(),\n        year: z.string(),\n      })),\n      projects: z.array(z.object({\n        name: z.string(),\n        description: z.string(),\n        technologies: z.array(z.string()),\n      })),\n    }),\n    spreadsheetId: z.string().default(\"demo-job-tracking-sheet\").describe(\"Google Sheets ID for tracking applications\"),\n  }),\n  outputSchema: z.object({\n    success: z.boolean(),\n    summary: z.string(),\n    applicationsSubmitted: z.number(),\n    trackingUrl: z.string(),\n  }),\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { jobs, resumeData, spreadsheetId, jobCount } = inputData;\n    \n    // Defensive check for jobs array\n    if (!jobs || !Array.isArray(jobs) || jobs.length === 0) {\n      logger?.error('❌ [JobWorkflow] No jobs found in input data');\n      return {\n        success: false,\n        summary: 'No jobs were found to apply to',\n        applicationsSubmitted: 0,\n        trackingUrl: '',\n      };\n    }\n    \n    logger?.info('🚀 [JobWorkflow] Starting job application step', { \n      jobCount: jobs.length,\n      spreadsheetId \n    });\n    \n    const runtimeContext = new RuntimeContext();\n    \n    // Apply to each job\n    let applicationsSubmitted = 0;\n    const applicationResults = [];\n    \n    for (const job of jobs) {\n      logger?.info('📝 [JobWorkflow] Processing job application', { \n        jobTitle: job.title,\n        company: job.company,\n        source: job.source \n      });\n      \n      const { text } = await jobApplicationAgent.generate([\n        { \n          role: \"user\", \n          content: `Apply to the ${job.title} position at ${job.company} using the provided resume data. The job URL is ${job.url}. After applying, track the application status.` \n        },\n      ], {\n        resourceId: \"job-application-bot\",\n        threadId: `job-apply-${Date.now()}-${job.company.replace(/\\s+/g, '-')}`,\n        maxSteps: 5,\n        runtimeContext,\n      });\n      \n      // Simulate application result\n      const applicationStatus = Math.random() > 0.3 ? 'applied' : 'failed';\n      if (applicationStatus === 'applied') {\n        applicationsSubmitted++;\n      }\n      \n      applicationResults.push({\n        jobTitle: job.title,\n        company: job.company,\n        location: job.location,\n        source: job.source,\n        jobUrl: job.url,\n        applicationStatus,\n        appliedDate: new Date().toISOString().split('T')[0],\n        notes: `Automated application via ${job.source}. Resume matched ${resumeData.skills.slice(0, 3).join(', ')} requirements.`,\n      });\n      \n      logger?.info('✅ [JobWorkflow] Job application processed', { \n        jobTitle: job.title,\n        status: applicationStatus \n      });\n    }\n    \n    // Track applications in Google Sheets\n    const { text: trackingResponse } = await jobApplicationAgent.generate([\n      { \n        role: \"user\", \n        content: `Track all ${applicationResults.length} job applications in Google Sheets with ID ${spreadsheetId}. Include all application details and status information.` \n      },\n    ], {\n      resourceId: \"tracking-bot\",\n      threadId: `tracking-${Date.now()}`,\n      maxSteps: 3,\n      runtimeContext,\n    });\n    \n    const trackingUrl = `https://docs.google.com/spreadsheets/d/${spreadsheetId}`;\n    \n    logger?.info('✅ [JobWorkflow] Job application and tracking completed', { \n      applicationsSubmitted,\n      totalJobs: jobs.length,\n      trackingUrl \n    });\n    \n    return {\n      success: true,\n      summary: `Successfully processed ${jobs.length} job opportunities. Applied to ${applicationsSubmitted} positions and tracked all ${applicationResults.length} applications in Google Sheets. Applications were submitted to positions at: ${applicationResults.map(app => app.company).join(', ')}.`,\n      applicationsSubmitted,\n      trackingUrl,\n    };\n  },\n});\n\nexport const jobApplicationWorkflow = createWorkflow({\n  id: \"job-application-workflow\",\n  description: \"Automated job application workflow that searches, applies, and tracks tech job applications\",\n  inputSchema: z.object({}), // Empty for time-based workflows\n  outputSchema: z.object({\n    success: z.boolean(),\n    summary: z.string(),\n    applicationsSubmitted: z.number(),\n    trackingUrl: z.string(),\n  }),\n})\n  .then(searchJobsStep)\n  .then(parseResumeStep)\n  .then(applyToJobsStep)\n  .commit();","import { Mastra } from \"@mastra/core\";\nimport { MastraError } from \"@mastra/core/error\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LogLevel, MastraLogger } from \"@mastra/core/logger\";\nimport pino from \"pino\";\nimport { MCPServer } from \"@mastra/mcp\";\nimport { NonRetriableError } from \"inngest\";\nimport { z } from \"zod\";\n\nimport { sharedPostgresStorage } from \"./storage\";\nimport { inngest, inngestServe, registerCronWorkflow } from \"./inngest\";\nimport { jobApplicationAgent } from \"./agents/jobApplicationAgent\";\nimport { jobApplicationWorkflow } from \"./workflows/jobApplicationWorkflow\";\nimport { jobSearchTool } from \"./tools/jobSearchTool\";\nimport { resumeParserTool } from \"./tools/resumeParserTool\";\nimport { applicationFillerTool } from \"./tools/applicationFillerTool\";\nimport { sheetsTrackerTool } from \"./tools/sheetsTrackerTool\";\nimport { githubPushTool } from \"./tools/githubPushTool\";\n\nclass ProductionPinoLogger extends MastraLogger {\n  protected logger: pino.Logger;\n\n  constructor(\n    options: {\n      name?: string;\n      level?: LogLevel;\n    } = {},\n  ) {\n    super(options);\n\n    this.logger = pino({\n      name: options.name || \"app\",\n      level: options.level || LogLevel.INFO,\n      base: {},\n      formatters: {\n        level: (label: string, _number: number) => ({\n          level: label,\n        }),\n      },\n      timestamp: () => `,\"time\":\"${new Date(Date.now()).toISOString()}\"`,\n    });\n  }\n\n  debug(message: string, args: Record<string, any> = {}): void {\n    this.logger.debug(args, message);\n  }\n\n  info(message: string, args: Record<string, any> = {}): void {\n    this.logger.info(args, message);\n  }\n\n  warn(message: string, args: Record<string, any> = {}): void {\n    this.logger.warn(args, message);\n  }\n\n  error(message: string, args: Record<string, any> = {}): void {\n    this.logger.error(args, message);\n  }\n}\n\nexport const mastra = new Mastra({\n  storage: sharedPostgresStorage,\n  agents: { jobApplicationAgent },\n  workflows: { jobApplicationWorkflow },\n  mcpServers: {\n    allTools: new MCPServer({\n      name: \"allTools\",\n      version: \"1.0.0\",\n      tools: {\n        jobSearchTool,\n        resumeParserTool,\n        applicationFillerTool,\n        sheetsTrackerTool,\n        githubPushTool,\n      },\n    }),\n  },\n  bundler: {\n    // A few dependencies are not properly picked up by\n    // the bundler if they are not added directly to the\n    // entrypoint.\n    externals: [\n      \"@slack/web-api\",\n      \"inngest\",\n      \"inngest/hono\",\n      \"hono\",\n      \"hono/streaming\",\n    ],\n    // sourcemaps are good for debugging.\n    sourcemap: true,\n  },\n  server: {\n    host: \"0.0.0.0\",\n    port: 5000,\n    middleware: [\n      async (c, next) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n        logger?.debug(\"[Request]\", { method: c.req.method, url: c.req.url });\n        try {\n          await next();\n        } catch (error) {\n          logger?.error(\"[Response]\", {\n            method: c.req.method,\n            url: c.req.url,\n            error,\n          });\n          if (error instanceof MastraError) {\n            if (error.id === \"AGENT_MEMORY_MISSING_RESOURCE_ID\") {\n              // This is typically a non-retirable error. It means that the request was not\n              // setup correctly to pass in the necessary parameters.\n              throw new NonRetriableError(error.message, { cause: error });\n            }\n          } else if (error instanceof z.ZodError) {\n            // Validation errors are never retriable.\n            throw new NonRetriableError(error.message, { cause: error });\n          }\n\n          throw error;\n        }\n      },\n    ],\n    apiRoutes: [\n      // This API route is used to register the Mastra workflow (inngest function) on the inngest server\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({ mastra, inngest }),\n        // The inngestServe function integrates Mastra workflows with Inngest by:\n        // 1. Creating Inngest functions for each workflow with unique IDs (workflow.${workflowId})\n        // 2. Setting up event handlers that:\n        //    - Generate unique run IDs for each workflow execution\n        //    - Create an InngestExecutionEngine to manage step execution\n        //    - Handle workflow state persistence and real-time updates\n        // 3. Establishing a publish-subscribe system for real-time monitoring\n        //    through the workflow:${workflowId}:${runId} channel\n      },\n    ],\n  },\n  logger:\n    process.env.NODE_ENV === \"production\"\n      ? new ProductionPinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        })\n      : new PinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        }),\n});\n\n// Register the job application workflow as a cron job\n// Run every Monday at 9 AM (configurable via environment variables)\nregisterCronWorkflow(\n  `TZ=${process.env.SCHEDULE_CRON_TIMEZONE || 'America/Los_Angeles'} ${process.env.SCHEDULE_CRON_EXPRESSION || '0 9 * * 1'}`,\n  jobApplicationWorkflow\n);\n\n/*  Sanity check 1: Throw an error if there are more than 1 workflows.  */\n// !!!!!! Do not remove this check. !!!!!!\nif (Object.keys(mastra.getWorkflows()).length > 1) {\n  throw new Error(\n    \"More than 1 workflows found. Currently, more than 1 workflows are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n\n/*  Sanity check 2: Throw an error if there are more than 1 agents.  */\n// !!!!!! Do not remove this check. !!!!!!\nif (Object.keys(mastra.getAgents()).length > 1) {\n  throw new Error(\n    \"More than 1 agents found. Currently, more than 1 agents are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n","// src/utils/mime.ts\nvar getMimeType = (filename, mimes = baseMimes) => {\n  const regexp = /\\.([a-zA-Z0-9]+?)$/;\n  const match = filename.match(regexp);\n  if (!match) {\n    return;\n  }\n  let mimeType = mimes[match[1]];\n  if (mimeType && mimeType.startsWith(\"text\")) {\n    mimeType += \"; charset=utf-8\";\n  }\n  return mimeType;\n};\nvar getExtension = (mimeType) => {\n  for (const ext in baseMimes) {\n    if (baseMimes[ext] === mimeType) {\n      return ext;\n    }\n  }\n};\nvar _baseMimes = {\n  aac: \"audio/aac\",\n  avi: \"video/x-msvideo\",\n  avif: \"image/avif\",\n  av1: \"video/av1\",\n  bin: \"application/octet-stream\",\n  bmp: \"image/bmp\",\n  css: \"text/css\",\n  csv: \"text/csv\",\n  eot: \"application/vnd.ms-fontobject\",\n  epub: \"application/epub+zip\",\n  gif: \"image/gif\",\n  gz: \"application/gzip\",\n  htm: \"text/html\",\n  html: \"text/html\",\n  ico: \"image/x-icon\",\n  ics: \"text/calendar\",\n  jpeg: \"image/jpeg\",\n  jpg: \"image/jpeg\",\n  js: \"text/javascript\",\n  json: \"application/json\",\n  jsonld: \"application/ld+json\",\n  map: \"application/json\",\n  mid: \"audio/x-midi\",\n  midi: \"audio/x-midi\",\n  mjs: \"text/javascript\",\n  mp3: \"audio/mpeg\",\n  mp4: \"video/mp4\",\n  mpeg: \"video/mpeg\",\n  oga: \"audio/ogg\",\n  ogv: \"video/ogg\",\n  ogx: \"application/ogg\",\n  opus: \"audio/opus\",\n  otf: \"font/otf\",\n  pdf: \"application/pdf\",\n  png: \"image/png\",\n  rtf: \"application/rtf\",\n  svg: \"image/svg+xml\",\n  tif: \"image/tiff\",\n  tiff: \"image/tiff\",\n  ts: \"video/mp2t\",\n  ttf: \"font/ttf\",\n  txt: \"text/plain\",\n  wasm: \"application/wasm\",\n  webm: \"video/webm\",\n  weba: \"audio/webm\",\n  webmanifest: \"application/manifest+json\",\n  webp: \"image/webp\",\n  woff: \"font/woff\",\n  woff2: \"font/woff2\",\n  xhtml: \"application/xhtml+xml\",\n  xml: \"application/xml\",\n  zip: \"application/zip\",\n  \"3gp\": \"video/3gpp\",\n  \"3g2\": \"video/3gpp2\",\n  gltf: \"model/gltf+json\",\n  glb: \"model/gltf-binary\"\n};\nvar baseMimes = _baseMimes;\nexport {\n  getExtension,\n  getMimeType,\n  baseMimes as mimes\n};\n","// src/utils/html.ts\nvar HtmlEscapedCallbackPhase = {\n  Stringify: 1,\n  BeforeStream: 2,\n  Stream: 3\n};\nvar raw = (value, callbacks) => {\n  const escapedString = new String(value);\n  escapedString.isEscaped = true;\n  escapedString.callbacks = callbacks;\n  return escapedString;\n};\nvar escapeRe = /[&<>'\"]/;\nvar stringBufferToString = async (buffer, callbacks) => {\n  let str = \"\";\n  callbacks ||= [];\n  const resolvedBuffer = await Promise.all(buffer);\n  for (let i = resolvedBuffer.length - 1; ; i--) {\n    str += resolvedBuffer[i];\n    i--;\n    if (i < 0) {\n      break;\n    }\n    let r = resolvedBuffer[i];\n    if (typeof r === \"object\") {\n      callbacks.push(...r.callbacks || []);\n    }\n    const isEscaped = r.isEscaped;\n    r = await (typeof r === \"object\" ? r.toString() : r);\n    if (typeof r === \"object\") {\n      callbacks.push(...r.callbacks || []);\n    }\n    if (r.isEscaped ?? isEscaped) {\n      str += r;\n    } else {\n      const buf = [str];\n      escapeToBuffer(r, buf);\n      str = buf[0];\n    }\n  }\n  return raw(str, callbacks);\n};\nvar escapeToBuffer = (str, buffer) => {\n  const match = str.search(escapeRe);\n  if (match === -1) {\n    buffer[0] += str;\n    return;\n  }\n  let escape;\n  let index;\n  let lastIndex = 0;\n  for (index = match; index < str.length; index++) {\n    switch (str.charCodeAt(index)) {\n      case 34:\n        escape = \"&quot;\";\n        break;\n      case 39:\n        escape = \"&#39;\";\n        break;\n      case 38:\n        escape = \"&amp;\";\n        break;\n      case 60:\n        escape = \"&lt;\";\n        break;\n      case 62:\n        escape = \"&gt;\";\n        break;\n      default:\n        continue;\n    }\n    buffer[0] += str.substring(lastIndex, index) + escape;\n    lastIndex = index + 1;\n  }\n  buffer[0] += str.substring(lastIndex, index);\n};\nvar resolveCallbackSync = (str) => {\n  const callbacks = str.callbacks;\n  if (!callbacks?.length) {\n    return str;\n  }\n  const buffer = [str];\n  const context = {};\n  callbacks.forEach((c) => c({ phase: HtmlEscapedCallbackPhase.Stringify, buffer, context }));\n  return buffer[0];\n};\nvar resolveCallback = async (str, phase, preserveCallbacks, context, buffer) => {\n  if (typeof str === \"object\" && !(str instanceof String)) {\n    if (!(str instanceof Promise)) {\n      str = str.toString();\n    }\n    if (str instanceof Promise) {\n      str = await str;\n    }\n  }\n  const callbacks = str.callbacks;\n  if (!callbacks?.length) {\n    return Promise.resolve(str);\n  }\n  if (buffer) {\n    buffer[0] += str;\n  } else {\n    buffer = [str];\n  }\n  const resStr = Promise.all(callbacks.map((c) => c({ phase, buffer, context }))).then(\n    (res) => Promise.all(\n      res.filter(Boolean).map((str2) => resolveCallback(str2, phase, false, context, buffer))\n    ).then(() => buffer[0])\n  );\n  if (preserveCallbacks) {\n    return raw(await resStr, callbacks);\n  } else {\n    return resStr;\n  }\n};\nexport {\n  HtmlEscapedCallbackPhase,\n  escapeToBuffer,\n  raw,\n  resolveCallback,\n  resolveCallbackSync,\n  stringBufferToString\n};\n","// src/helper/html/index.ts\nimport { escapeToBuffer, raw, resolveCallbackSync, stringBufferToString } from \"../../utils/html.js\";\nvar html = (strings, ...values) => {\n  const buffer = [\"\"];\n  for (let i = 0, len = strings.length - 1; i < len; i++) {\n    buffer[0] += strings[i];\n    const children = Array.isArray(values[i]) ? values[i].flat(Infinity) : [values[i]];\n    for (let i2 = 0, len2 = children.length; i2 < len2; i2++) {\n      const child = children[i2];\n      if (typeof child === \"string\") {\n        escapeToBuffer(child, buffer);\n      } else if (typeof child === \"number\") {\n        ;\n        buffer[0] += child;\n      } else if (typeof child === \"boolean\" || child === null || child === void 0) {\n        continue;\n      } else if (typeof child === \"object\" && child.isEscaped) {\n        if (child.callbacks) {\n          buffer.unshift(\"\", child);\n        } else {\n          const tmp = child.toString();\n          if (tmp instanceof Promise) {\n            buffer.unshift(\"\", tmp);\n          } else {\n            buffer[0] += tmp;\n          }\n        }\n      } else if (child instanceof Promise) {\n        buffer.unshift(\"\", child);\n      } else {\n        escapeToBuffer(child.toString(), buffer);\n      }\n    }\n  }\n  buffer[0] += strings.at(-1);\n  return buffer.length === 1 ? \"callbacks\" in buffer ? raw(resolveCallbackSync(raw(buffer[0], buffer.callbacks))) : raw(buffer[0]) : stringBufferToString(buffer, buffer.callbacks);\n};\nexport {\n  html,\n  raw\n};\n","// src/server/a2a/store.ts\nvar InMemoryTaskStore = class {\n  store = /* @__PURE__ */ new Map();\n  activeCancellations = /* @__PURE__ */ new Set();\n  async load({ agentId, taskId }) {\n    const entry = this.store.get(`${agentId}-${taskId}`);\n    if (!entry) {\n      return null;\n    }\n    return { ...entry };\n  }\n  async save({ agentId, data }) {\n    const key = `${agentId}-${data.id}`;\n    if (!data.id) {\n      throw new Error(\"Task ID is required\");\n    }\n    this.store.set(key, { ...data });\n  }\n};\n\nexport { InMemoryTaskStore };\n//# sourceMappingURL=store.js.map\n//# sourceMappingURL=store.js.map","// src/compose.ts\nvar compose = (middleware, onError, onNotFound) => {\n  return (context, next) => {\n    let index = -1;\n    return dispatch(0);\n    async function dispatch(i) {\n      if (i <= index) {\n        throw new Error(\"next() called multiple times\");\n      }\n      index = i;\n      let res;\n      let isError = false;\n      let handler;\n      if (middleware[i]) {\n        handler = middleware[i][0][0];\n        context.req.routeIndex = i;\n      } else {\n        handler = i === middleware.length && next || void 0;\n      }\n      if (handler) {\n        try {\n          res = await handler(context, () => dispatch(i + 1));\n        } catch (err) {\n          if (err instanceof Error && onError) {\n            context.error = err;\n            res = await onError(err, context);\n            isError = true;\n          } else {\n            throw err;\n          }\n        }\n      } else {\n        if (context.finalized === false && onNotFound) {\n          res = await onNotFound(context);\n        }\n      }\n      if (res && (context.finalized === false || isError)) {\n        context.res = res;\n      }\n      return context;\n    }\n  };\n};\nexport {\n  compose\n};\n","// src/request/constants.ts\nvar GET_MATCH_RESULT = Symbol();\nexport {\n  GET_MATCH_RESULT\n};\n","// src/utils/body.ts\nimport { HonoRequest } from \"../request.js\";\nvar parseBody = async (request, options = /* @__PURE__ */ Object.create(null)) => {\n  const { all = false, dot = false } = options;\n  const headers = request instanceof HonoRequest ? request.raw.headers : request.headers;\n  const contentType = headers.get(\"Content-Type\");\n  if (contentType?.startsWith(\"multipart/form-data\") || contentType?.startsWith(\"application/x-www-form-urlencoded\")) {\n    return parseFormData(request, { all, dot });\n  }\n  return {};\n};\nasync function parseFormData(request, options) {\n  const formData = await request.formData();\n  if (formData) {\n    return convertFormDataToBodyData(formData, options);\n  }\n  return {};\n}\nfunction convertFormDataToBodyData(formData, options) {\n  const form = /* @__PURE__ */ Object.create(null);\n  formData.forEach((value, key) => {\n    const shouldParseAllValues = options.all || key.endsWith(\"[]\");\n    if (!shouldParseAllValues) {\n      form[key] = value;\n    } else {\n      handleParsingAllValues(form, key, value);\n    }\n  });\n  if (options.dot) {\n    Object.entries(form).forEach(([key, value]) => {\n      const shouldParseDotValues = key.includes(\".\");\n      if (shouldParseDotValues) {\n        handleParsingNestedValues(form, key, value);\n        delete form[key];\n      }\n    });\n  }\n  return form;\n}\nvar handleParsingAllValues = (form, key, value) => {\n  if (form[key] !== void 0) {\n    if (Array.isArray(form[key])) {\n      ;\n      form[key].push(value);\n    } else {\n      form[key] = [form[key], value];\n    }\n  } else {\n    if (!key.endsWith(\"[]\")) {\n      form[key] = value;\n    } else {\n      form[key] = [value];\n    }\n  }\n};\nvar handleParsingNestedValues = (form, key, value) => {\n  let nestedForm = form;\n  const keys = key.split(\".\");\n  keys.forEach((key2, index) => {\n    if (index === keys.length - 1) {\n      nestedForm[key2] = value;\n    } else {\n      if (!nestedForm[key2] || typeof nestedForm[key2] !== \"object\" || Array.isArray(nestedForm[key2]) || nestedForm[key2] instanceof File) {\n        nestedForm[key2] = /* @__PURE__ */ Object.create(null);\n      }\n      nestedForm = nestedForm[key2];\n    }\n  });\n};\nexport {\n  parseBody\n};\n","// src/utils/url.ts\nvar splitPath = (path) => {\n  const paths = path.split(\"/\");\n  if (paths[0] === \"\") {\n    paths.shift();\n  }\n  return paths;\n};\nvar splitRoutingPath = (routePath) => {\n  const { groups, path } = extractGroupsFromPath(routePath);\n  const paths = splitPath(path);\n  return replaceGroupMarks(paths, groups);\n};\nvar extractGroupsFromPath = (path) => {\n  const groups = [];\n  path = path.replace(/\\{[^}]+\\}/g, (match, index) => {\n    const mark = `@${index}`;\n    groups.push([mark, match]);\n    return mark;\n  });\n  return { groups, path };\n};\nvar replaceGroupMarks = (paths, groups) => {\n  for (let i = groups.length - 1; i >= 0; i--) {\n    const [mark] = groups[i];\n    for (let j = paths.length - 1; j >= 0; j--) {\n      if (paths[j].includes(mark)) {\n        paths[j] = paths[j].replace(mark, groups[i][1]);\n        break;\n      }\n    }\n  }\n  return paths;\n};\nvar patternCache = {};\nvar getPattern = (label, next) => {\n  if (label === \"*\") {\n    return \"*\";\n  }\n  const match = label.match(/^\\:([^\\{\\}]+)(?:\\{(.+)\\})?$/);\n  if (match) {\n    const cacheKey = `${label}#${next}`;\n    if (!patternCache[cacheKey]) {\n      if (match[2]) {\n        patternCache[cacheKey] = next && next[0] !== \":\" && next[0] !== \"*\" ? [cacheKey, match[1], new RegExp(`^${match[2]}(?=/${next})`)] : [label, match[1], new RegExp(`^${match[2]}$`)];\n      } else {\n        patternCache[cacheKey] = [label, match[1], true];\n      }\n    }\n    return patternCache[cacheKey];\n  }\n  return null;\n};\nvar tryDecode = (str, decoder) => {\n  try {\n    return decoder(str);\n  } catch {\n    return str.replace(/(?:%[0-9A-Fa-f]{2})+/g, (match) => {\n      try {\n        return decoder(match);\n      } catch {\n        return match;\n      }\n    });\n  }\n};\nvar tryDecodeURI = (str) => tryDecode(str, decodeURI);\nvar getPath = (request) => {\n  const url = request.url;\n  const start = url.indexOf(\"/\", url.indexOf(\":\") + 4);\n  let i = start;\n  for (; i < url.length; i++) {\n    const charCode = url.charCodeAt(i);\n    if (charCode === 37) {\n      const queryIndex = url.indexOf(\"?\", i);\n      const path = url.slice(start, queryIndex === -1 ? void 0 : queryIndex);\n      return tryDecodeURI(path.includes(\"%25\") ? path.replace(/%25/g, \"%2525\") : path);\n    } else if (charCode === 63) {\n      break;\n    }\n  }\n  return url.slice(start, i);\n};\nvar getQueryStrings = (url) => {\n  const queryIndex = url.indexOf(\"?\", 8);\n  return queryIndex === -1 ? \"\" : \"?\" + url.slice(queryIndex + 1);\n};\nvar getPathNoStrict = (request) => {\n  const result = getPath(request);\n  return result.length > 1 && result.at(-1) === \"/\" ? result.slice(0, -1) : result;\n};\nvar mergePath = (base, sub, ...rest) => {\n  if (rest.length) {\n    sub = mergePath(sub, ...rest);\n  }\n  return `${base?.[0] === \"/\" ? \"\" : \"/\"}${base}${sub === \"/\" ? \"\" : `${base?.at(-1) === \"/\" ? \"\" : \"/\"}${sub?.[0] === \"/\" ? sub.slice(1) : sub}`}`;\n};\nvar checkOptionalParameter = (path) => {\n  if (path.charCodeAt(path.length - 1) !== 63 || !path.includes(\":\")) {\n    return null;\n  }\n  const segments = path.split(\"/\");\n  const results = [];\n  let basePath = \"\";\n  segments.forEach((segment) => {\n    if (segment !== \"\" && !/\\:/.test(segment)) {\n      basePath += \"/\" + segment;\n    } else if (/\\:/.test(segment)) {\n      if (/\\?/.test(segment)) {\n        if (results.length === 0 && basePath === \"\") {\n          results.push(\"/\");\n        } else {\n          results.push(basePath);\n        }\n        const optionalSegment = segment.replace(\"?\", \"\");\n        basePath += \"/\" + optionalSegment;\n        results.push(basePath);\n      } else {\n        basePath += \"/\" + segment;\n      }\n    }\n  });\n  return results.filter((v, i, a) => a.indexOf(v) === i);\n};\nvar _decodeURI = (value) => {\n  if (!/[%+]/.test(value)) {\n    return value;\n  }\n  if (value.indexOf(\"+\") !== -1) {\n    value = value.replace(/\\+/g, \" \");\n  }\n  return value.indexOf(\"%\") !== -1 ? tryDecode(value, decodeURIComponent_) : value;\n};\nvar _getQueryParam = (url, key, multiple) => {\n  let encoded;\n  if (!multiple && key && !/[%+]/.test(key)) {\n    let keyIndex2 = url.indexOf(`?${key}`, 8);\n    if (keyIndex2 === -1) {\n      keyIndex2 = url.indexOf(`&${key}`, 8);\n    }\n    while (keyIndex2 !== -1) {\n      const trailingKeyCode = url.charCodeAt(keyIndex2 + key.length + 1);\n      if (trailingKeyCode === 61) {\n        const valueIndex = keyIndex2 + key.length + 2;\n        const endIndex = url.indexOf(\"&\", valueIndex);\n        return _decodeURI(url.slice(valueIndex, endIndex === -1 ? void 0 : endIndex));\n      } else if (trailingKeyCode == 38 || isNaN(trailingKeyCode)) {\n        return \"\";\n      }\n      keyIndex2 = url.indexOf(`&${key}`, keyIndex2 + 1);\n    }\n    encoded = /[%+]/.test(url);\n    if (!encoded) {\n      return void 0;\n    }\n  }\n  const results = {};\n  encoded ??= /[%+]/.test(url);\n  let keyIndex = url.indexOf(\"?\", 8);\n  while (keyIndex !== -1) {\n    const nextKeyIndex = url.indexOf(\"&\", keyIndex + 1);\n    let valueIndex = url.indexOf(\"=\", keyIndex);\n    if (valueIndex > nextKeyIndex && nextKeyIndex !== -1) {\n      valueIndex = -1;\n    }\n    let name = url.slice(\n      keyIndex + 1,\n      valueIndex === -1 ? nextKeyIndex === -1 ? void 0 : nextKeyIndex : valueIndex\n    );\n    if (encoded) {\n      name = _decodeURI(name);\n    }\n    keyIndex = nextKeyIndex;\n    if (name === \"\") {\n      continue;\n    }\n    let value;\n    if (valueIndex === -1) {\n      value = \"\";\n    } else {\n      value = url.slice(valueIndex + 1, nextKeyIndex === -1 ? void 0 : nextKeyIndex);\n      if (encoded) {\n        value = _decodeURI(value);\n      }\n    }\n    if (multiple) {\n      if (!(results[name] && Array.isArray(results[name]))) {\n        results[name] = [];\n      }\n      ;\n      results[name].push(value);\n    } else {\n      results[name] ??= value;\n    }\n  }\n  return key ? results[key] : results;\n};\nvar getQueryParam = _getQueryParam;\nvar getQueryParams = (url, key) => {\n  return _getQueryParam(url, key, true);\n};\nvar decodeURIComponent_ = decodeURIComponent;\nexport {\n  checkOptionalParameter,\n  decodeURIComponent_,\n  getPath,\n  getPathNoStrict,\n  getPattern,\n  getQueryParam,\n  getQueryParams,\n  getQueryStrings,\n  mergePath,\n  splitPath,\n  splitRoutingPath,\n  tryDecode\n};\n","// src/request.ts\nimport { GET_MATCH_RESULT } from \"./request/constants.js\";\nimport { parseBody } from \"./utils/body.js\";\nimport { decodeURIComponent_, getQueryParam, getQueryParams, tryDecode } from \"./utils/url.js\";\nvar tryDecodeURIComponent = (str) => tryDecode(str, decodeURIComponent_);\nvar HonoRequest = class {\n  raw;\n  #validatedData;\n  #matchResult;\n  routeIndex = 0;\n  path;\n  bodyCache = {};\n  constructor(request, path = \"/\", matchResult = [[]]) {\n    this.raw = request;\n    this.path = path;\n    this.#matchResult = matchResult;\n    this.#validatedData = {};\n  }\n  param(key) {\n    return key ? this.#getDecodedParam(key) : this.#getAllDecodedParams();\n  }\n  #getDecodedParam(key) {\n    const paramKey = this.#matchResult[0][this.routeIndex][1][key];\n    const param = this.#getParamValue(paramKey);\n    return param ? /\\%/.test(param) ? tryDecodeURIComponent(param) : param : void 0;\n  }\n  #getAllDecodedParams() {\n    const decoded = {};\n    const keys = Object.keys(this.#matchResult[0][this.routeIndex][1]);\n    for (const key of keys) {\n      const value = this.#getParamValue(this.#matchResult[0][this.routeIndex][1][key]);\n      if (value && typeof value === \"string\") {\n        decoded[key] = /\\%/.test(value) ? tryDecodeURIComponent(value) : value;\n      }\n    }\n    return decoded;\n  }\n  #getParamValue(paramKey) {\n    return this.#matchResult[1] ? this.#matchResult[1][paramKey] : paramKey;\n  }\n  query(key) {\n    return getQueryParam(this.url, key);\n  }\n  queries(key) {\n    return getQueryParams(this.url, key);\n  }\n  header(name) {\n    if (name) {\n      return this.raw.headers.get(name) ?? void 0;\n    }\n    const headerData = {};\n    this.raw.headers.forEach((value, key) => {\n      headerData[key] = value;\n    });\n    return headerData;\n  }\n  async parseBody(options) {\n    return this.bodyCache.parsedBody ??= await parseBody(this, options);\n  }\n  #cachedBody = (key) => {\n    const { bodyCache, raw } = this;\n    const cachedBody = bodyCache[key];\n    if (cachedBody) {\n      return cachedBody;\n    }\n    const anyCachedKey = Object.keys(bodyCache)[0];\n    if (anyCachedKey) {\n      return bodyCache[anyCachedKey].then((body) => {\n        if (anyCachedKey === \"json\") {\n          body = JSON.stringify(body);\n        }\n        return new Response(body)[key]();\n      });\n    }\n    return bodyCache[key] = raw[key]();\n  };\n  json() {\n    return this.#cachedBody(\"text\").then((text) => JSON.parse(text));\n  }\n  text() {\n    return this.#cachedBody(\"text\");\n  }\n  arrayBuffer() {\n    return this.#cachedBody(\"arrayBuffer\");\n  }\n  blob() {\n    return this.#cachedBody(\"blob\");\n  }\n  formData() {\n    return this.#cachedBody(\"formData\");\n  }\n  addValidatedData(target, data) {\n    this.#validatedData[target] = data;\n  }\n  valid(target) {\n    return this.#validatedData[target];\n  }\n  get url() {\n    return this.raw.url;\n  }\n  get method() {\n    return this.raw.method;\n  }\n  get [GET_MATCH_RESULT]() {\n    return this.#matchResult;\n  }\n  get matchedRoutes() {\n    return this.#matchResult[0].map(([[, route]]) => route);\n  }\n  get routePath() {\n    return this.#matchResult[0].map(([[, route]]) => route)[this.routeIndex].path;\n  }\n};\nexport {\n  HonoRequest\n};\n","// src/context.ts\nimport { HonoRequest } from \"./request.js\";\nimport { HtmlEscapedCallbackPhase, resolveCallback } from \"./utils/html.js\";\nvar TEXT_PLAIN = \"text/plain; charset=UTF-8\";\nvar setDefaultContentType = (contentType, headers) => {\n  return {\n    \"Content-Type\": contentType,\n    ...headers\n  };\n};\nvar Context = class {\n  #rawRequest;\n  #req;\n  env = {};\n  #var;\n  finalized = false;\n  error;\n  #status;\n  #executionCtx;\n  #res;\n  #layout;\n  #renderer;\n  #notFoundHandler;\n  #preparedHeaders;\n  #matchResult;\n  #path;\n  constructor(req, options) {\n    this.#rawRequest = req;\n    if (options) {\n      this.#executionCtx = options.executionCtx;\n      this.env = options.env;\n      this.#notFoundHandler = options.notFoundHandler;\n      this.#path = options.path;\n      this.#matchResult = options.matchResult;\n    }\n  }\n  get req() {\n    this.#req ??= new HonoRequest(this.#rawRequest, this.#path, this.#matchResult);\n    return this.#req;\n  }\n  get event() {\n    if (this.#executionCtx && \"respondWith\" in this.#executionCtx) {\n      return this.#executionCtx;\n    } else {\n      throw Error(\"This context has no FetchEvent\");\n    }\n  }\n  get executionCtx() {\n    if (this.#executionCtx) {\n      return this.#executionCtx;\n    } else {\n      throw Error(\"This context has no ExecutionContext\");\n    }\n  }\n  get res() {\n    return this.#res ||= new Response(null, {\n      headers: this.#preparedHeaders ??= new Headers()\n    });\n  }\n  set res(_res) {\n    if (this.#res && _res) {\n      _res = new Response(_res.body, _res);\n      for (const [k, v] of this.#res.headers.entries()) {\n        if (k === \"content-type\") {\n          continue;\n        }\n        if (k === \"set-cookie\") {\n          const cookies = this.#res.headers.getSetCookie();\n          _res.headers.delete(\"set-cookie\");\n          for (const cookie of cookies) {\n            _res.headers.append(\"set-cookie\", cookie);\n          }\n        } else {\n          _res.headers.set(k, v);\n        }\n      }\n    }\n    this.#res = _res;\n    this.finalized = true;\n  }\n  render = (...args) => {\n    this.#renderer ??= (content) => this.html(content);\n    return this.#renderer(...args);\n  };\n  setLayout = (layout) => this.#layout = layout;\n  getLayout = () => this.#layout;\n  setRenderer = (renderer) => {\n    this.#renderer = renderer;\n  };\n  header = (name, value, options) => {\n    if (this.finalized) {\n      this.#res = new Response(this.#res.body, this.#res);\n    }\n    const headers = this.#res ? this.#res.headers : this.#preparedHeaders ??= new Headers();\n    if (value === void 0) {\n      headers.delete(name);\n    } else if (options?.append) {\n      headers.append(name, value);\n    } else {\n      headers.set(name, value);\n    }\n  };\n  status = (status) => {\n    this.#status = status;\n  };\n  set = (key, value) => {\n    this.#var ??= /* @__PURE__ */ new Map();\n    this.#var.set(key, value);\n  };\n  get = (key) => {\n    return this.#var ? this.#var.get(key) : void 0;\n  };\n  get var() {\n    if (!this.#var) {\n      return {};\n    }\n    return Object.fromEntries(this.#var);\n  }\n  #newResponse(data, arg, headers) {\n    const responseHeaders = this.#res ? new Headers(this.#res.headers) : this.#preparedHeaders ?? new Headers();\n    if (typeof arg === \"object\" && \"headers\" in arg) {\n      const argHeaders = arg.headers instanceof Headers ? arg.headers : new Headers(arg.headers);\n      for (const [key, value] of argHeaders) {\n        if (key.toLowerCase() === \"set-cookie\") {\n          responseHeaders.append(key, value);\n        } else {\n          responseHeaders.set(key, value);\n        }\n      }\n    }\n    if (headers) {\n      for (const [k, v] of Object.entries(headers)) {\n        if (typeof v === \"string\") {\n          responseHeaders.set(k, v);\n        } else {\n          responseHeaders.delete(k);\n          for (const v2 of v) {\n            responseHeaders.append(k, v2);\n          }\n        }\n      }\n    }\n    const status = typeof arg === \"number\" ? arg : arg?.status ?? this.#status;\n    return new Response(data, { status, headers: responseHeaders });\n  }\n  newResponse = (...args) => this.#newResponse(...args);\n  body = (data, arg, headers) => this.#newResponse(data, arg, headers);\n  text = (text, arg, headers) => {\n    return !this.#preparedHeaders && !this.#status && !arg && !headers && !this.finalized ? new Response(text) : this.#newResponse(\n      text,\n      arg,\n      setDefaultContentType(TEXT_PLAIN, headers)\n    );\n  };\n  json = (object, arg, headers) => {\n    return this.#newResponse(\n      JSON.stringify(object),\n      arg,\n      setDefaultContentType(\"application/json\", headers)\n    );\n  };\n  html = (html, arg, headers) => {\n    const res = (html2) => this.#newResponse(html2, arg, setDefaultContentType(\"text/html; charset=UTF-8\", headers));\n    return typeof html === \"object\" ? resolveCallback(html, HtmlEscapedCallbackPhase.Stringify, false, {}).then(res) : res(html);\n  };\n  redirect = (location, status) => {\n    const locationString = String(location);\n    this.header(\n      \"Location\",\n      !/[^\\x00-\\xFF]/.test(locationString) ? locationString : encodeURI(locationString)\n    );\n    return this.newResponse(null, status ?? 302);\n  };\n  notFound = () => {\n    this.#notFoundHandler ??= () => new Response();\n    return this.#notFoundHandler(this);\n  };\n};\nexport {\n  Context,\n  TEXT_PLAIN\n};\n","// src/router.ts\nvar METHOD_NAME_ALL = \"ALL\";\nvar METHOD_NAME_ALL_LOWERCASE = \"all\";\nvar METHODS = [\"get\", \"post\", \"put\", \"delete\", \"options\", \"patch\"];\nvar MESSAGE_MATCHER_IS_ALREADY_BUILT = \"Can not add a route since the matcher is already built.\";\nvar UnsupportedPathError = class extends Error {\n};\nexport {\n  MESSAGE_MATCHER_IS_ALREADY_BUILT,\n  METHODS,\n  METHOD_NAME_ALL,\n  METHOD_NAME_ALL_LOWERCASE,\n  UnsupportedPathError\n};\n","// src/utils/constants.ts\nvar COMPOSED_HANDLER = \"__COMPOSED_HANDLER\";\nexport {\n  COMPOSED_HANDLER\n};\n","// src/hono-base.ts\nimport { compose } from \"./compose.js\";\nimport { Context } from \"./context.js\";\nimport { METHODS, METHOD_NAME_ALL, METHOD_NAME_ALL_LOWERCASE } from \"./router.js\";\nimport { COMPOSED_HANDLER } from \"./utils/constants.js\";\nimport { getPath, getPathNoStrict, mergePath } from \"./utils/url.js\";\nvar notFoundHandler = (c) => {\n  return c.text(\"404 Not Found\", 404);\n};\nvar errorHandler = (err, c) => {\n  if (\"getResponse\" in err) {\n    const res = err.getResponse();\n    return c.newResponse(res.body, res);\n  }\n  console.error(err);\n  return c.text(\"Internal Server Error\", 500);\n};\nvar Hono = class {\n  get;\n  post;\n  put;\n  delete;\n  options;\n  patch;\n  all;\n  on;\n  use;\n  router;\n  getPath;\n  _basePath = \"/\";\n  #path = \"/\";\n  routes = [];\n  constructor(options = {}) {\n    const allMethods = [...METHODS, METHOD_NAME_ALL_LOWERCASE];\n    allMethods.forEach((method) => {\n      this[method] = (args1, ...args) => {\n        if (typeof args1 === \"string\") {\n          this.#path = args1;\n        } else {\n          this.#addRoute(method, this.#path, args1);\n        }\n        args.forEach((handler) => {\n          this.#addRoute(method, this.#path, handler);\n        });\n        return this;\n      };\n    });\n    this.on = (method, path, ...handlers) => {\n      for (const p of [path].flat()) {\n        this.#path = p;\n        for (const m of [method].flat()) {\n          handlers.map((handler) => {\n            this.#addRoute(m.toUpperCase(), this.#path, handler);\n          });\n        }\n      }\n      return this;\n    };\n    this.use = (arg1, ...handlers) => {\n      if (typeof arg1 === \"string\") {\n        this.#path = arg1;\n      } else {\n        this.#path = \"*\";\n        handlers.unshift(arg1);\n      }\n      handlers.forEach((handler) => {\n        this.#addRoute(METHOD_NAME_ALL, this.#path, handler);\n      });\n      return this;\n    };\n    const { strict, ...optionsWithoutStrict } = options;\n    Object.assign(this, optionsWithoutStrict);\n    this.getPath = strict ?? true ? options.getPath ?? getPath : getPathNoStrict;\n  }\n  #clone() {\n    const clone = new Hono({\n      router: this.router,\n      getPath: this.getPath\n    });\n    clone.errorHandler = this.errorHandler;\n    clone.#notFoundHandler = this.#notFoundHandler;\n    clone.routes = this.routes;\n    return clone;\n  }\n  #notFoundHandler = notFoundHandler;\n  errorHandler = errorHandler;\n  route(path, app) {\n    const subApp = this.basePath(path);\n    app.routes.map((r) => {\n      let handler;\n      if (app.errorHandler === errorHandler) {\n        handler = r.handler;\n      } else {\n        handler = async (c, next) => (await compose([], app.errorHandler)(c, () => r.handler(c, next))).res;\n        handler[COMPOSED_HANDLER] = r.handler;\n      }\n      subApp.#addRoute(r.method, r.path, handler);\n    });\n    return this;\n  }\n  basePath(path) {\n    const subApp = this.#clone();\n    subApp._basePath = mergePath(this._basePath, path);\n    return subApp;\n  }\n  onError = (handler) => {\n    this.errorHandler = handler;\n    return this;\n  };\n  notFound = (handler) => {\n    this.#notFoundHandler = handler;\n    return this;\n  };\n  mount(path, applicationHandler, options) {\n    let replaceRequest;\n    let optionHandler;\n    if (options) {\n      if (typeof options === \"function\") {\n        optionHandler = options;\n      } else {\n        optionHandler = options.optionHandler;\n        if (options.replaceRequest === false) {\n          replaceRequest = (request) => request;\n        } else {\n          replaceRequest = options.replaceRequest;\n        }\n      }\n    }\n    const getOptions = optionHandler ? (c) => {\n      const options2 = optionHandler(c);\n      return Array.isArray(options2) ? options2 : [options2];\n    } : (c) => {\n      let executionContext = void 0;\n      try {\n        executionContext = c.executionCtx;\n      } catch {\n      }\n      return [c.env, executionContext];\n    };\n    replaceRequest ||= (() => {\n      const mergedPath = mergePath(this._basePath, path);\n      const pathPrefixLength = mergedPath === \"/\" ? 0 : mergedPath.length;\n      return (request) => {\n        const url = new URL(request.url);\n        url.pathname = url.pathname.slice(pathPrefixLength) || \"/\";\n        return new Request(url, request);\n      };\n    })();\n    const handler = async (c, next) => {\n      const res = await applicationHandler(replaceRequest(c.req.raw), ...getOptions(c));\n      if (res) {\n        return res;\n      }\n      await next();\n    };\n    this.#addRoute(METHOD_NAME_ALL, mergePath(path, \"*\"), handler);\n    return this;\n  }\n  #addRoute(method, path, handler) {\n    method = method.toUpperCase();\n    path = mergePath(this._basePath, path);\n    const r = { basePath: this._basePath, path, method, handler };\n    this.router.add(method, path, [handler, r]);\n    this.routes.push(r);\n  }\n  #handleError(err, c) {\n    if (err instanceof Error) {\n      return this.errorHandler(err, c);\n    }\n    throw err;\n  }\n  #dispatch(request, executionCtx, env, method) {\n    if (method === \"HEAD\") {\n      return (async () => new Response(null, await this.#dispatch(request, executionCtx, env, \"GET\")))();\n    }\n    const path = this.getPath(request, { env });\n    const matchResult = this.router.match(method, path);\n    const c = new Context(request, {\n      path,\n      matchResult,\n      env,\n      executionCtx,\n      notFoundHandler: this.#notFoundHandler\n    });\n    if (matchResult[0].length === 1) {\n      let res;\n      try {\n        res = matchResult[0][0][0][0](c, async () => {\n          c.res = await this.#notFoundHandler(c);\n        });\n      } catch (err) {\n        return this.#handleError(err, c);\n      }\n      return res instanceof Promise ? res.then(\n        (resolved) => resolved || (c.finalized ? c.res : this.#notFoundHandler(c))\n      ).catch((err) => this.#handleError(err, c)) : res ?? this.#notFoundHandler(c);\n    }\n    const composed = compose(matchResult[0], this.errorHandler, this.#notFoundHandler);\n    return (async () => {\n      try {\n        const context = await composed(c);\n        if (!context.finalized) {\n          throw new Error(\n            \"Context is not finalized. Did you forget to return a Response object or `await next()`?\"\n          );\n        }\n        return context.res;\n      } catch (err) {\n        return this.#handleError(err, c);\n      }\n    })();\n  }\n  fetch = (request, ...rest) => {\n    return this.#dispatch(request, rest[1], rest[0], request.method);\n  };\n  request = (input, requestInit, Env, executionCtx) => {\n    if (input instanceof Request) {\n      return this.fetch(requestInit ? new Request(input, requestInit) : input, Env, executionCtx);\n    }\n    input = input.toString();\n    return this.fetch(\n      new Request(\n        /^https?:\\/\\//.test(input) ? input : `http://localhost${mergePath(\"/\", input)}`,\n        requestInit\n      ),\n      Env,\n      executionCtx\n    );\n  };\n  fire = () => {\n    addEventListener(\"fetch\", (event) => {\n      event.respondWith(this.#dispatch(event.request, event, void 0, event.request.method));\n    });\n  };\n};\nexport {\n  Hono as HonoBase\n};\n","// src/router/reg-exp-router/node.ts\nvar LABEL_REG_EXP_STR = \"[^/]+\";\nvar ONLY_WILDCARD_REG_EXP_STR = \".*\";\nvar TAIL_WILDCARD_REG_EXP_STR = \"(?:|/.*)\";\nvar PATH_ERROR = Symbol();\nvar regExpMetaChars = new Set(\".\\\\+*[^]$()\");\nfunction compareKey(a, b) {\n  if (a.length === 1) {\n    return b.length === 1 ? a < b ? -1 : 1 : -1;\n  }\n  if (b.length === 1) {\n    return 1;\n  }\n  if (a === ONLY_WILDCARD_REG_EXP_STR || a === TAIL_WILDCARD_REG_EXP_STR) {\n    return 1;\n  } else if (b === ONLY_WILDCARD_REG_EXP_STR || b === TAIL_WILDCARD_REG_EXP_STR) {\n    return -1;\n  }\n  if (a === LABEL_REG_EXP_STR) {\n    return 1;\n  } else if (b === LABEL_REG_EXP_STR) {\n    return -1;\n  }\n  return a.length === b.length ? a < b ? -1 : 1 : b.length - a.length;\n}\nvar Node = class {\n  #index;\n  #varIndex;\n  #children = /* @__PURE__ */ Object.create(null);\n  insert(tokens, index, paramMap, context, pathErrorCheckOnly) {\n    if (tokens.length === 0) {\n      if (this.#index !== void 0) {\n        throw PATH_ERROR;\n      }\n      if (pathErrorCheckOnly) {\n        return;\n      }\n      this.#index = index;\n      return;\n    }\n    const [token, ...restTokens] = tokens;\n    const pattern = token === \"*\" ? restTokens.length === 0 ? [\"\", \"\", ONLY_WILDCARD_REG_EXP_STR] : [\"\", \"\", LABEL_REG_EXP_STR] : token === \"/*\" ? [\"\", \"\", TAIL_WILDCARD_REG_EXP_STR] : token.match(/^\\:([^\\{\\}]+)(?:\\{(.+)\\})?$/);\n    let node;\n    if (pattern) {\n      const name = pattern[1];\n      let regexpStr = pattern[2] || LABEL_REG_EXP_STR;\n      if (name && pattern[2]) {\n        if (regexpStr === \".*\") {\n          throw PATH_ERROR;\n        }\n        regexpStr = regexpStr.replace(/^\\((?!\\?:)(?=[^)]+\\)$)/, \"(?:\");\n        if (/\\((?!\\?:)/.test(regexpStr)) {\n          throw PATH_ERROR;\n        }\n      }\n      node = this.#children[regexpStr];\n      if (!node) {\n        if (Object.keys(this.#children).some(\n          (k) => k !== ONLY_WILDCARD_REG_EXP_STR && k !== TAIL_WILDCARD_REG_EXP_STR\n        )) {\n          throw PATH_ERROR;\n        }\n        if (pathErrorCheckOnly) {\n          return;\n        }\n        node = this.#children[regexpStr] = new Node();\n        if (name !== \"\") {\n          node.#varIndex = context.varIndex++;\n        }\n      }\n      if (!pathErrorCheckOnly && name !== \"\") {\n        paramMap.push([name, node.#varIndex]);\n      }\n    } else {\n      node = this.#children[token];\n      if (!node) {\n        if (Object.keys(this.#children).some(\n          (k) => k.length > 1 && k !== ONLY_WILDCARD_REG_EXP_STR && k !== TAIL_WILDCARD_REG_EXP_STR\n        )) {\n          throw PATH_ERROR;\n        }\n        if (pathErrorCheckOnly) {\n          return;\n        }\n        node = this.#children[token] = new Node();\n      }\n    }\n    node.insert(restTokens, index, paramMap, context, pathErrorCheckOnly);\n  }\n  buildRegExpStr() {\n    const childKeys = Object.keys(this.#children).sort(compareKey);\n    const strList = childKeys.map((k) => {\n      const c = this.#children[k];\n      return (typeof c.#varIndex === \"number\" ? `(${k})@${c.#varIndex}` : regExpMetaChars.has(k) ? `\\\\${k}` : k) + c.buildRegExpStr();\n    });\n    if (typeof this.#index === \"number\") {\n      strList.unshift(`#${this.#index}`);\n    }\n    if (strList.length === 0) {\n      return \"\";\n    }\n    if (strList.length === 1) {\n      return strList[0];\n    }\n    return \"(?:\" + strList.join(\"|\") + \")\";\n  }\n};\nexport {\n  Node,\n  PATH_ERROR\n};\n","// src/router/reg-exp-router/trie.ts\nimport { Node } from \"./node.js\";\nvar Trie = class {\n  #context = { varIndex: 0 };\n  #root = new Node();\n  insert(path, index, pathErrorCheckOnly) {\n    const paramAssoc = [];\n    const groups = [];\n    for (let i = 0; ; ) {\n      let replaced = false;\n      path = path.replace(/\\{[^}]+\\}/g, (m) => {\n        const mark = `@\\\\${i}`;\n        groups[i] = [mark, m];\n        i++;\n        replaced = true;\n        return mark;\n      });\n      if (!replaced) {\n        break;\n      }\n    }\n    const tokens = path.match(/(?::[^\\/]+)|(?:\\/\\*$)|./g) || [];\n    for (let i = groups.length - 1; i >= 0; i--) {\n      const [mark] = groups[i];\n      for (let j = tokens.length - 1; j >= 0; j--) {\n        if (tokens[j].indexOf(mark) !== -1) {\n          tokens[j] = tokens[j].replace(mark, groups[i][1]);\n          break;\n        }\n      }\n    }\n    this.#root.insert(tokens, index, paramAssoc, this.#context, pathErrorCheckOnly);\n    return paramAssoc;\n  }\n  buildRegExp() {\n    let regexp = this.#root.buildRegExpStr();\n    if (regexp === \"\") {\n      return [/^$/, [], []];\n    }\n    let captureIndex = 0;\n    const indexReplacementMap = [];\n    const paramReplacementMap = [];\n    regexp = regexp.replace(/#(\\d+)|@(\\d+)|\\.\\*\\$/g, (_, handlerIndex, paramIndex) => {\n      if (handlerIndex !== void 0) {\n        indexReplacementMap[++captureIndex] = Number(handlerIndex);\n        return \"$()\";\n      }\n      if (paramIndex !== void 0) {\n        paramReplacementMap[Number(paramIndex)] = ++captureIndex;\n        return \"\";\n      }\n      return \"\";\n    });\n    return [new RegExp(`^${regexp}`), indexReplacementMap, paramReplacementMap];\n  }\n};\nexport {\n  Trie\n};\n","// src/router/reg-exp-router/router.ts\nimport {\n  MESSAGE_MATCHER_IS_ALREADY_BUILT,\n  METHOD_NAME_ALL,\n  UnsupportedPathError\n} from \"../../router.js\";\nimport { checkOptionalParameter } from \"../../utils/url.js\";\nimport { PATH_ERROR } from \"./node.js\";\nimport { Trie } from \"./trie.js\";\nvar emptyParam = [];\nvar nullMatcher = [/^$/, [], /* @__PURE__ */ Object.create(null)];\nvar wildcardRegExpCache = /* @__PURE__ */ Object.create(null);\nfunction buildWildcardRegExp(path) {\n  return wildcardRegExpCache[path] ??= new RegExp(\n    path === \"*\" ? \"\" : `^${path.replace(\n      /\\/\\*$|([.\\\\+*[^\\]$()])/g,\n      (_, metaChar) => metaChar ? `\\\\${metaChar}` : \"(?:|/.*)\"\n    )}$`\n  );\n}\nfunction clearWildcardRegExpCache() {\n  wildcardRegExpCache = /* @__PURE__ */ Object.create(null);\n}\nfunction buildMatcherFromPreprocessedRoutes(routes) {\n  const trie = new Trie();\n  const handlerData = [];\n  if (routes.length === 0) {\n    return nullMatcher;\n  }\n  const routesWithStaticPathFlag = routes.map(\n    (route) => [!/\\*|\\/:/.test(route[0]), ...route]\n  ).sort(\n    ([isStaticA, pathA], [isStaticB, pathB]) => isStaticA ? 1 : isStaticB ? -1 : pathA.length - pathB.length\n  );\n  const staticMap = /* @__PURE__ */ Object.create(null);\n  for (let i = 0, j = -1, len = routesWithStaticPathFlag.length; i < len; i++) {\n    const [pathErrorCheckOnly, path, handlers] = routesWithStaticPathFlag[i];\n    if (pathErrorCheckOnly) {\n      staticMap[path] = [handlers.map(([h]) => [h, /* @__PURE__ */ Object.create(null)]), emptyParam];\n    } else {\n      j++;\n    }\n    let paramAssoc;\n    try {\n      paramAssoc = trie.insert(path, j, pathErrorCheckOnly);\n    } catch (e) {\n      throw e === PATH_ERROR ? new UnsupportedPathError(path) : e;\n    }\n    if (pathErrorCheckOnly) {\n      continue;\n    }\n    handlerData[j] = handlers.map(([h, paramCount]) => {\n      const paramIndexMap = /* @__PURE__ */ Object.create(null);\n      paramCount -= 1;\n      for (; paramCount >= 0; paramCount--) {\n        const [key, value] = paramAssoc[paramCount];\n        paramIndexMap[key] = value;\n      }\n      return [h, paramIndexMap];\n    });\n  }\n  const [regexp, indexReplacementMap, paramReplacementMap] = trie.buildRegExp();\n  for (let i = 0, len = handlerData.length; i < len; i++) {\n    for (let j = 0, len2 = handlerData[i].length; j < len2; j++) {\n      const map = handlerData[i][j]?.[1];\n      if (!map) {\n        continue;\n      }\n      const keys = Object.keys(map);\n      for (let k = 0, len3 = keys.length; k < len3; k++) {\n        map[keys[k]] = paramReplacementMap[map[keys[k]]];\n      }\n    }\n  }\n  const handlerMap = [];\n  for (const i in indexReplacementMap) {\n    handlerMap[i] = handlerData[indexReplacementMap[i]];\n  }\n  return [regexp, handlerMap, staticMap];\n}\nfunction findMiddleware(middleware, path) {\n  if (!middleware) {\n    return void 0;\n  }\n  for (const k of Object.keys(middleware).sort((a, b) => b.length - a.length)) {\n    if (buildWildcardRegExp(k).test(path)) {\n      return [...middleware[k]];\n    }\n  }\n  return void 0;\n}\nvar RegExpRouter = class {\n  name = \"RegExpRouter\";\n  #middleware;\n  #routes;\n  constructor() {\n    this.#middleware = { [METHOD_NAME_ALL]: /* @__PURE__ */ Object.create(null) };\n    this.#routes = { [METHOD_NAME_ALL]: /* @__PURE__ */ Object.create(null) };\n  }\n  add(method, path, handler) {\n    const middleware = this.#middleware;\n    const routes = this.#routes;\n    if (!middleware || !routes) {\n      throw new Error(MESSAGE_MATCHER_IS_ALREADY_BUILT);\n    }\n    if (!middleware[method]) {\n      ;\n      [middleware, routes].forEach((handlerMap) => {\n        handlerMap[method] = /* @__PURE__ */ Object.create(null);\n        Object.keys(handlerMap[METHOD_NAME_ALL]).forEach((p) => {\n          handlerMap[method][p] = [...handlerMap[METHOD_NAME_ALL][p]];\n        });\n      });\n    }\n    if (path === \"/*\") {\n      path = \"*\";\n    }\n    const paramCount = (path.match(/\\/:/g) || []).length;\n    if (/\\*$/.test(path)) {\n      const re = buildWildcardRegExp(path);\n      if (method === METHOD_NAME_ALL) {\n        Object.keys(middleware).forEach((m) => {\n          middleware[m][path] ||= findMiddleware(middleware[m], path) || findMiddleware(middleware[METHOD_NAME_ALL], path) || [];\n        });\n      } else {\n        middleware[method][path] ||= findMiddleware(middleware[method], path) || findMiddleware(middleware[METHOD_NAME_ALL], path) || [];\n      }\n      Object.keys(middleware).forEach((m) => {\n        if (method === METHOD_NAME_ALL || method === m) {\n          Object.keys(middleware[m]).forEach((p) => {\n            re.test(p) && middleware[m][p].push([handler, paramCount]);\n          });\n        }\n      });\n      Object.keys(routes).forEach((m) => {\n        if (method === METHOD_NAME_ALL || method === m) {\n          Object.keys(routes[m]).forEach(\n            (p) => re.test(p) && routes[m][p].push([handler, paramCount])\n          );\n        }\n      });\n      return;\n    }\n    const paths = checkOptionalParameter(path) || [path];\n    for (let i = 0, len = paths.length; i < len; i++) {\n      const path2 = paths[i];\n      Object.keys(routes).forEach((m) => {\n        if (method === METHOD_NAME_ALL || method === m) {\n          routes[m][path2] ||= [\n            ...findMiddleware(middleware[m], path2) || findMiddleware(middleware[METHOD_NAME_ALL], path2) || []\n          ];\n          routes[m][path2].push([handler, paramCount - len + i + 1]);\n        }\n      });\n    }\n  }\n  match(method, path) {\n    clearWildcardRegExpCache();\n    const matchers = this.#buildAllMatchers();\n    this.match = (method2, path2) => {\n      const matcher = matchers[method2] || matchers[METHOD_NAME_ALL];\n      const staticMatch = matcher[2][path2];\n      if (staticMatch) {\n        return staticMatch;\n      }\n      const match = path2.match(matcher[0]);\n      if (!match) {\n        return [[], emptyParam];\n      }\n      const index = match.indexOf(\"\", 1);\n      return [matcher[1][index], match];\n    };\n    return this.match(method, path);\n  }\n  #buildAllMatchers() {\n    const matchers = /* @__PURE__ */ Object.create(null);\n    Object.keys(this.#routes).concat(Object.keys(this.#middleware)).forEach((method) => {\n      matchers[method] ||= this.#buildMatcher(method);\n    });\n    this.#middleware = this.#routes = void 0;\n    return matchers;\n  }\n  #buildMatcher(method) {\n    const routes = [];\n    let hasOwnRoute = method === METHOD_NAME_ALL;\n    [this.#middleware, this.#routes].forEach((r) => {\n      const ownRoute = r[method] ? Object.keys(r[method]).map((path) => [path, r[method][path]]) : [];\n      if (ownRoute.length !== 0) {\n        hasOwnRoute ||= true;\n        routes.push(...ownRoute);\n      } else if (method !== METHOD_NAME_ALL) {\n        routes.push(\n          ...Object.keys(r[METHOD_NAME_ALL]).map((path) => [path, r[METHOD_NAME_ALL][path]])\n        );\n      }\n    });\n    if (!hasOwnRoute) {\n      return null;\n    } else {\n      return buildMatcherFromPreprocessedRoutes(routes);\n    }\n  }\n};\nexport {\n  RegExpRouter\n};\n","// src/router/smart-router/router.ts\nimport { MESSAGE_MATCHER_IS_ALREADY_BUILT, UnsupportedPathError } from \"../../router.js\";\nvar SmartRouter = class {\n  name = \"SmartRouter\";\n  #routers = [];\n  #routes = [];\n  constructor(init) {\n    this.#routers = init.routers;\n  }\n  add(method, path, handler) {\n    if (!this.#routes) {\n      throw new Error(MESSAGE_MATCHER_IS_ALREADY_BUILT);\n    }\n    this.#routes.push([method, path, handler]);\n  }\n  match(method, path) {\n    if (!this.#routes) {\n      throw new Error(\"Fatal error\");\n    }\n    const routers = this.#routers;\n    const routes = this.#routes;\n    const len = routers.length;\n    let i = 0;\n    let res;\n    for (; i < len; i++) {\n      const router = routers[i];\n      try {\n        for (let i2 = 0, len2 = routes.length; i2 < len2; i2++) {\n          router.add(...routes[i2]);\n        }\n        res = router.match(method, path);\n      } catch (e) {\n        if (e instanceof UnsupportedPathError) {\n          continue;\n        }\n        throw e;\n      }\n      this.match = router.match.bind(router);\n      this.#routers = [router];\n      this.#routes = void 0;\n      break;\n    }\n    if (i === len) {\n      throw new Error(\"Fatal error\");\n    }\n    this.name = `SmartRouter + ${this.activeRouter.name}`;\n    return res;\n  }\n  get activeRouter() {\n    if (this.#routes || this.#routers.length !== 1) {\n      throw new Error(\"No active router has been determined yet.\");\n    }\n    return this.#routers[0];\n  }\n};\nexport {\n  SmartRouter\n};\n","// src/router/trie-router/node.ts\nimport { METHOD_NAME_ALL } from \"../../router.js\";\nimport { getPattern, splitPath, splitRoutingPath } from \"../../utils/url.js\";\nvar emptyParams = /* @__PURE__ */ Object.create(null);\nvar Node = class {\n  #methods;\n  #children;\n  #patterns;\n  #order = 0;\n  #params = emptyParams;\n  constructor(method, handler, children) {\n    this.#children = children || /* @__PURE__ */ Object.create(null);\n    this.#methods = [];\n    if (method && handler) {\n      const m = /* @__PURE__ */ Object.create(null);\n      m[method] = { handler, possibleKeys: [], score: 0 };\n      this.#methods = [m];\n    }\n    this.#patterns = [];\n  }\n  insert(method, path, handler) {\n    this.#order = ++this.#order;\n    let curNode = this;\n    const parts = splitRoutingPath(path);\n    const possibleKeys = [];\n    for (let i = 0, len = parts.length; i < len; i++) {\n      const p = parts[i];\n      const nextP = parts[i + 1];\n      const pattern = getPattern(p, nextP);\n      const key = Array.isArray(pattern) ? pattern[0] : p;\n      if (key in curNode.#children) {\n        curNode = curNode.#children[key];\n        if (pattern) {\n          possibleKeys.push(pattern[1]);\n        }\n        continue;\n      }\n      curNode.#children[key] = new Node();\n      if (pattern) {\n        curNode.#patterns.push(pattern);\n        possibleKeys.push(pattern[1]);\n      }\n      curNode = curNode.#children[key];\n    }\n    curNode.#methods.push({\n      [method]: {\n        handler,\n        possibleKeys: possibleKeys.filter((v, i, a) => a.indexOf(v) === i),\n        score: this.#order\n      }\n    });\n    return curNode;\n  }\n  #getHandlerSets(node, method, nodeParams, params) {\n    const handlerSets = [];\n    for (let i = 0, len = node.#methods.length; i < len; i++) {\n      const m = node.#methods[i];\n      const handlerSet = m[method] || m[METHOD_NAME_ALL];\n      const processedSet = {};\n      if (handlerSet !== void 0) {\n        handlerSet.params = /* @__PURE__ */ Object.create(null);\n        handlerSets.push(handlerSet);\n        if (nodeParams !== emptyParams || params && params !== emptyParams) {\n          for (let i2 = 0, len2 = handlerSet.possibleKeys.length; i2 < len2; i2++) {\n            const key = handlerSet.possibleKeys[i2];\n            const processed = processedSet[handlerSet.score];\n            handlerSet.params[key] = params?.[key] && !processed ? params[key] : nodeParams[key] ?? params?.[key];\n            processedSet[handlerSet.score] = true;\n          }\n        }\n      }\n    }\n    return handlerSets;\n  }\n  search(method, path) {\n    const handlerSets = [];\n    this.#params = emptyParams;\n    const curNode = this;\n    let curNodes = [curNode];\n    const parts = splitPath(path);\n    const curNodesQueue = [];\n    for (let i = 0, len = parts.length; i < len; i++) {\n      const part = parts[i];\n      const isLast = i === len - 1;\n      const tempNodes = [];\n      for (let j = 0, len2 = curNodes.length; j < len2; j++) {\n        const node = curNodes[j];\n        const nextNode = node.#children[part];\n        if (nextNode) {\n          nextNode.#params = node.#params;\n          if (isLast) {\n            if (nextNode.#children[\"*\"]) {\n              handlerSets.push(\n                ...this.#getHandlerSets(nextNode.#children[\"*\"], method, node.#params)\n              );\n            }\n            handlerSets.push(...this.#getHandlerSets(nextNode, method, node.#params));\n          } else {\n            tempNodes.push(nextNode);\n          }\n        }\n        for (let k = 0, len3 = node.#patterns.length; k < len3; k++) {\n          const pattern = node.#patterns[k];\n          const params = node.#params === emptyParams ? {} : { ...node.#params };\n          if (pattern === \"*\") {\n            const astNode = node.#children[\"*\"];\n            if (astNode) {\n              handlerSets.push(...this.#getHandlerSets(astNode, method, node.#params));\n              astNode.#params = params;\n              tempNodes.push(astNode);\n            }\n            continue;\n          }\n          const [key, name, matcher] = pattern;\n          if (!part && !(matcher instanceof RegExp)) {\n            continue;\n          }\n          const child = node.#children[key];\n          const restPathString = parts.slice(i).join(\"/\");\n          if (matcher instanceof RegExp) {\n            const m = matcher.exec(restPathString);\n            if (m) {\n              params[name] = m[0];\n              handlerSets.push(...this.#getHandlerSets(child, method, node.#params, params));\n              if (Object.keys(child.#children).length) {\n                child.#params = params;\n                const componentCount = m[0].match(/\\//)?.length ?? 0;\n                const targetCurNodes = curNodesQueue[componentCount] ||= [];\n                targetCurNodes.push(child);\n              }\n              continue;\n            }\n          }\n          if (matcher === true || matcher.test(part)) {\n            params[name] = part;\n            if (isLast) {\n              handlerSets.push(...this.#getHandlerSets(child, method, params, node.#params));\n              if (child.#children[\"*\"]) {\n                handlerSets.push(\n                  ...this.#getHandlerSets(child.#children[\"*\"], method, params, node.#params)\n                );\n              }\n            } else {\n              child.#params = params;\n              tempNodes.push(child);\n            }\n          }\n        }\n      }\n      curNodes = tempNodes.concat(curNodesQueue.shift() ?? []);\n    }\n    if (handlerSets.length > 1) {\n      handlerSets.sort((a, b) => {\n        return a.score - b.score;\n      });\n    }\n    return [handlerSets.map(({ handler, params }) => [handler, params])];\n  }\n};\nexport {\n  Node\n};\n","// src/router/trie-router/router.ts\nimport { checkOptionalParameter } from \"../../utils/url.js\";\nimport { Node } from \"./node.js\";\nvar TrieRouter = class {\n  name = \"TrieRouter\";\n  #node;\n  constructor() {\n    this.#node = new Node();\n  }\n  add(method, path, handler) {\n    const results = checkOptionalParameter(path);\n    if (results) {\n      for (let i = 0, len = results.length; i < len; i++) {\n        this.#node.insert(method, results[i], handler);\n      }\n      return;\n    }\n    this.#node.insert(method, path, handler);\n  }\n  match(method, path) {\n    return this.#node.search(method, path);\n  }\n};\nexport {\n  TrieRouter\n};\n","// src/hono.ts\nimport { HonoBase } from \"./hono-base.js\";\nimport { RegExpRouter } from \"./router/reg-exp-router/index.js\";\nimport { SmartRouter } from \"./router/smart-router/index.js\";\nimport { TrieRouter } from \"./router/trie-router/index.js\";\nvar Hono = class extends HonoBase {\n  constructor(options = {}) {\n    super(options);\n    this.router = options.router ?? new SmartRouter({\n      routers: [new RegExpRouter(), new TrieRouter()]\n    });\n  }\n};\nexport {\n  Hono\n};\n","// src/middleware/cors/index.ts\nvar cors = (options) => {\n  const defaults = {\n    origin: \"*\",\n    allowMethods: [\"GET\", \"HEAD\", \"PUT\", \"POST\", \"DELETE\", \"PATCH\"],\n    allowHeaders: [],\n    exposeHeaders: []\n  };\n  const opts = {\n    ...defaults,\n    ...options\n  };\n  const findAllowOrigin = ((optsOrigin) => {\n    if (typeof optsOrigin === \"string\") {\n      if (optsOrigin === \"*\") {\n        return () => optsOrigin;\n      } else {\n        return (origin) => optsOrigin === origin ? origin : null;\n      }\n    } else if (typeof optsOrigin === \"function\") {\n      return optsOrigin;\n    } else {\n      return (origin) => optsOrigin.includes(origin) ? origin : null;\n    }\n  })(opts.origin);\n  const findAllowMethods = ((optsAllowMethods) => {\n    if (typeof optsAllowMethods === \"function\") {\n      return optsAllowMethods;\n    } else if (Array.isArray(optsAllowMethods)) {\n      return () => optsAllowMethods;\n    } else {\n      return () => [];\n    }\n  })(opts.allowMethods);\n  return async function cors2(c, next) {\n    function set(key, value) {\n      c.res.headers.set(key, value);\n    }\n    const allowOrigin = await findAllowOrigin(c.req.header(\"origin\") || \"\", c);\n    if (allowOrigin) {\n      set(\"Access-Control-Allow-Origin\", allowOrigin);\n    }\n    if (opts.origin !== \"*\") {\n      const existingVary = c.req.header(\"Vary\");\n      if (existingVary) {\n        set(\"Vary\", existingVary);\n      } else {\n        set(\"Vary\", \"Origin\");\n      }\n    }\n    if (opts.credentials) {\n      set(\"Access-Control-Allow-Credentials\", \"true\");\n    }\n    if (opts.exposeHeaders?.length) {\n      set(\"Access-Control-Expose-Headers\", opts.exposeHeaders.join(\",\"));\n    }\n    if (c.req.method === \"OPTIONS\") {\n      if (opts.maxAge != null) {\n        set(\"Access-Control-Max-Age\", opts.maxAge.toString());\n      }\n      const allowMethods = await findAllowMethods(c.req.header(\"origin\") || \"\", c);\n      if (allowMethods.length) {\n        set(\"Access-Control-Allow-Methods\", allowMethods.join(\",\"));\n      }\n      let headers = opts.allowHeaders;\n      if (!headers?.length) {\n        const requestHeaders = c.req.header(\"Access-Control-Request-Headers\");\n        if (requestHeaders) {\n          headers = requestHeaders.split(/\\s*,\\s*/);\n        }\n      }\n      if (headers?.length) {\n        set(\"Access-Control-Allow-Headers\", headers.join(\",\"));\n        c.res.headers.append(\"Vary\", \"Access-Control-Request-Headers\");\n      }\n      c.res.headers.delete(\"Content-Length\");\n      c.res.headers.delete(\"Content-Type\");\n      return new Response(null, {\n        headers: c.res.headers,\n        status: 204,\n        statusText: \"No Content\"\n      });\n    }\n    await next();\n  };\n};\nexport {\n  cors\n};\n","// src/utils/color.ts\nfunction getColorEnabled() {\n  const { process, Deno } = globalThis;\n  const isNoColor = typeof Deno?.noColor === \"boolean\" ? Deno.noColor : process !== void 0 ? \"NO_COLOR\" in process?.env : false;\n  return !isNoColor;\n}\nasync function getColorEnabledAsync() {\n  const { navigator } = globalThis;\n  const cfWorkers = \"cloudflare:workers\";\n  const isNoColor = navigator !== void 0 && navigator.userAgent === \"Cloudflare-Workers\" ? await (async () => {\n    try {\n      return \"NO_COLOR\" in ((await import(cfWorkers)).env ?? {});\n    } catch {\n      return false;\n    }\n  })() : !getColorEnabled();\n  return !isNoColor;\n}\nexport {\n  getColorEnabled,\n  getColorEnabledAsync\n};\n","// src/middleware/logger/index.ts\nimport { getColorEnabledAsync } from \"../../utils/color.js\";\nvar humanize = (times) => {\n  const [delimiter, separator] = [\",\", \".\"];\n  const orderTimes = times.map((v) => v.replace(/(\\d)(?=(\\d\\d\\d)+(?!\\d))/g, \"$1\" + delimiter));\n  return orderTimes.join(separator);\n};\nvar time = (start) => {\n  const delta = Date.now() - start;\n  return humanize([delta < 1e3 ? delta + \"ms\" : Math.round(delta / 1e3) + \"s\"]);\n};\nvar colorStatus = async (status) => {\n  const colorEnabled = await getColorEnabledAsync();\n  if (colorEnabled) {\n    switch (status / 100 | 0) {\n      case 5:\n        return `\\x1B[31m${status}\\x1B[0m`;\n      case 4:\n        return `\\x1B[33m${status}\\x1B[0m`;\n      case 3:\n        return `\\x1B[36m${status}\\x1B[0m`;\n      case 2:\n        return `\\x1B[32m${status}\\x1B[0m`;\n    }\n  }\n  return `${status}`;\n};\nasync function log(fn, prefix, method, path, status = 0, elapsed) {\n  const out = prefix === \"<--\" /* Incoming */ ? `${prefix} ${method} ${path}` : `${prefix} ${method} ${path} ${await colorStatus(status)} ${elapsed}`;\n  fn(out);\n}\nvar logger = (fn = console.log) => {\n  return async function logger2(c, next) {\n    const { method, url } = c.req;\n    const path = url.slice(url.indexOf(\"/\", 8));\n    await log(fn, \"<--\" /* Incoming */, method, path);\n    const start = Date.now();\n    await next();\n    await log(fn, \"-->\" /* Outgoing */, method, path, c.res.status, time(start));\n  };\n};\nexport {\n  logger\n};\n","// src/http-exception.ts\nvar HTTPException = class extends Error {\n  res;\n  status;\n  constructor(status = 500, options) {\n    super(options?.message, { cause: options?.cause });\n    this.res = options?.res;\n    this.status = status;\n  }\n  getResponse() {\n    if (this.res) {\n      const newResponse = new Response(this.res.body, {\n        status: this.status,\n        headers: this.res.headers\n      });\n      return newResponse;\n    }\n    return new Response(this.message, {\n      status: this.status\n    });\n  }\n};\nexport {\n  HTTPException\n};\n","// src/middleware/timeout/index.ts\nimport { HTTPException } from \"../../http-exception.js\";\nvar defaultTimeoutException = new HTTPException(504, {\n  message: \"Gateway Timeout\"\n});\nvar timeout = (duration, exception = defaultTimeoutException) => {\n  return async function timeout2(context, next) {\n    let timer;\n    const timeoutPromise = new Promise((_, reject) => {\n      timer = setTimeout(() => {\n        reject(typeof exception === \"function\" ? exception(context) : exception);\n      }, duration);\n    });\n    try {\n      await Promise.race([next(), timeoutPromise]);\n    } finally {\n      if (timer !== void 0) {\n        clearTimeout(timer);\n      }\n    }\n  };\n};\nexport {\n  timeout\n};\n","var __create = Object.create;\nvar __defProp = Object.defineProperty;\nvar __getOwnPropDesc = Object.getOwnPropertyDescriptor;\nvar __getOwnPropNames = Object.getOwnPropertyNames;\nvar __getProtoOf = Object.getPrototypeOf;\nvar __hasOwnProp = Object.prototype.hasOwnProperty;\nvar __commonJS = (cb, mod) => function __require() {\n  return mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;\n};\nvar __export = (target, all) => {\n  for (var name in all)\n    __defProp(target, name, { get: all[name], enumerable: true });\n};\nvar __copyProps = (to, from, except, desc) => {\n  if (from && typeof from === \"object\" || typeof from === \"function\") {\n    for (let key of __getOwnPropNames(from))\n      if (!__hasOwnProp.call(to, key) && key !== except)\n        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });\n  }\n  return to;\n};\nvar __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(\n  // If the importer is in node compatibility mode or this is not an ESM\n  // file that has been converted to a CommonJS file using a Babel-\n  // compatible transform (i.e. \"__esModule\" has not been set), then set\n  // \"default\" to the CommonJS \"module.exports\" for node compatibility.\n  isNodeMode || !mod || !mod.__esModule ? __defProp(target, \"default\", { value: mod, enumerable: true }) : target,\n  mod\n));\n\nexport { __commonJS, __export, __toESM };\n//# sourceMappingURL=chunk-G3PMV62Z.js.map\n//# sourceMappingURL=chunk-G3PMV62Z.js.map","import { __export } from './chunk-G3PMV62Z.js';\nimport { MastraA2AError } from '@mastra/core/a2a';\nimport { z } from 'zod';\n\n// src/server/handlers/a2a.ts\nvar a2a_exports = {};\n__export(a2a_exports, {\n  getAgentCardByIdHandler: () => getAgentCardByIdHandler,\n  getAgentExecutionHandler: () => getAgentExecutionHandler,\n  handleMessageSend: () => handleMessageSend,\n  handleMessageStream: () => handleMessageStream,\n  handleTaskCancel: () => handleTaskCancel,\n  handleTaskGet: () => handleTaskGet\n});\nfunction normalizeError(error, reqId, taskId, logger) {\n  let a2aError;\n  if (error instanceof MastraA2AError) {\n    a2aError = error;\n  } else if (error instanceof Error) {\n    a2aError = MastraA2AError.internalError(error.message, { stack: error.stack });\n  } else {\n    a2aError = MastraA2AError.internalError(\"An unknown error occurred.\", error);\n  }\n  if (taskId && !a2aError.taskId) {\n    a2aError.taskId = taskId;\n  }\n  logger?.error(`Error processing request (Task: ${a2aError.taskId ?? \"N/A\"}, ReqID: ${reqId ?? \"N/A\"}):`, a2aError);\n  return createErrorResponse(reqId, a2aError.toJSONRPCError());\n}\nfunction createErrorResponse(id, error) {\n  return {\n    jsonrpc: \"2.0\",\n    id,\n    // Can be null if request ID was invalid/missing\n    error\n  };\n}\nfunction createSuccessResponse(id, result) {\n  if (!id) {\n    throw MastraA2AError.internalError(\"Cannot create success response for null ID.\");\n  }\n  return {\n    jsonrpc: \"2.0\",\n    id,\n    result\n  };\n}\nfunction convertToCoreMessage(message) {\n  return {\n    role: message.role === \"user\" ? \"user\" : \"assistant\",\n    content: message.parts.map((msg) => convertToCoreMessagePart(msg))\n  };\n}\nfunction convertToCoreMessagePart(part) {\n  switch (part.kind) {\n    case \"text\":\n      return {\n        type: \"text\",\n        text: part.text\n      };\n    case \"file\":\n      return {\n        type: \"file\",\n        data: \"uri\" in part.file ? new URL(part.file.uri) : part.file.bytes,\n        mimeType: part.file.mimeType\n      };\n    case \"data\":\n      throw new Error(\"Data parts are not supported in core messages\");\n  }\n}\n\n// src/server/a2a/tasks.ts\nfunction isTaskStatusUpdate(update) {\n  return \"state\" in update && !(\"parts\" in update);\n}\nfunction isArtifactUpdate(update) {\n  return \"kind\" in update && update.kind === \"artifact-update\";\n}\nfunction applyUpdateToTask(current, update) {\n  let newTask = structuredClone(current);\n  if (isTaskStatusUpdate(update)) {\n    newTask.status = {\n      ...newTask.status,\n      // Keep existing properties if not overwritten\n      ...update,\n      // Apply updates\n      timestamp: (/* @__PURE__ */ new Date()).toISOString()\n    };\n  } else if (isArtifactUpdate(update)) {\n    if (!newTask.artifacts) {\n      newTask.artifacts = [];\n    } else {\n      newTask.artifacts = [...newTask.artifacts];\n    }\n    const artifact = update.artifact;\n    const existingIndex = newTask.artifacts.findIndex((a) => a.name === artifact.name);\n    const existingArtifact = newTask.artifacts[existingIndex];\n    if (existingArtifact) {\n      if (update.append) {\n        const appendedArtifact = JSON.parse(JSON.stringify(existingArtifact));\n        appendedArtifact.parts.push(...artifact.parts);\n        if (artifact.metadata) {\n          appendedArtifact.metadata = {\n            ...appendedArtifact.metadata || {},\n            ...artifact.metadata\n          };\n        }\n        if (artifact.description) appendedArtifact.description = artifact.description;\n        newTask.artifacts[existingIndex] = appendedArtifact;\n      } else {\n        newTask.artifacts[existingIndex] = { ...artifact };\n      }\n    } else {\n      newTask.artifacts.push({ ...artifact });\n    }\n  }\n  return newTask;\n}\nasync function loadOrCreateTask({\n  agentId,\n  taskId,\n  taskStore,\n  message,\n  contextId,\n  metadata,\n  logger\n}) {\n  const data = await taskStore.load({ agentId, taskId });\n  if (!data) {\n    const initialTask = {\n      id: taskId,\n      contextId: contextId || crypto.randomUUID(),\n      status: {\n        state: \"submitted\",\n        timestamp: (/* @__PURE__ */ new Date()).toISOString(),\n        message: void 0\n      },\n      artifacts: [],\n      history: [message],\n      metadata,\n      kind: \"task\"\n    };\n    logger?.info(`[Task ${taskId}] Created new task.`);\n    await taskStore.save({ agentId, data: initialTask });\n    return initialTask;\n  }\n  logger?.info(`[Task ${taskId}] Loaded existing task.`);\n  let updatedData = data;\n  updatedData.history = [...data.history || [], message];\n  const { status } = data;\n  const finalStates = [\"completed\", \"failed\", \"canceled\"];\n  if (finalStates.includes(status.state)) {\n    logger?.warn(`[Task ${taskId}] Received message for task in final state ${status.state}. Restarting.`);\n    updatedData = applyUpdateToTask(updatedData, {\n      state: \"submitted\",\n      message: void 0\n    });\n  } else if (status.state === \"input-required\") {\n    logger?.info(`[Task ${taskId}] Changing state from 'input-required' to 'working'.`);\n    updatedData = applyUpdateToTask(updatedData, { state: \"working\" });\n  } else if (status.state === \"working\") {\n    logger?.warn(`[Task ${taskId}] Received message while already 'working'. Proceeding.`);\n  }\n  await taskStore.save({ agentId, data: updatedData });\n  return updatedData;\n}\nfunction createTaskContext({\n  task,\n  userMessage,\n  history,\n  activeCancellations\n}) {\n  return {\n    task: structuredClone(task),\n    userMessage,\n    history: structuredClone(history),\n    isCancelled: () => activeCancellations.has(task.id)\n  };\n}\n\n// src/server/handlers/a2a.ts\nvar messageSendParamsSchema = z.object({\n  message: z.object({\n    role: z.enum([\"user\", \"agent\"]),\n    parts: z.array(\n      z.object({\n        kind: z.enum([\"text\"]),\n        text: z.string()\n      })\n    ),\n    kind: z.literal(\"message\"),\n    messageId: z.string(),\n    contextId: z.string().optional(),\n    taskId: z.string().optional(),\n    referenceTaskIds: z.array(z.string()).optional(),\n    extensions: z.array(z.string()).optional(),\n    metadata: z.record(z.any()).optional()\n  })\n});\nasync function getAgentCardByIdHandler({\n  mastra,\n  agentId,\n  executionUrl = `/a2a/${agentId}`,\n  provider = {\n    organization: \"Mastra\",\n    url: \"https://mastra.ai\"\n  },\n  version = \"1.0\",\n  runtimeContext\n}) {\n  const agent = mastra.getAgent(agentId);\n  if (!agent) {\n    throw new Error(`Agent with ID ${agentId} not found`);\n  }\n  const [instructions, tools] = await Promise.all([\n    agent.getInstructions({ runtimeContext }),\n    agent.getTools({ runtimeContext })\n  ]);\n  const agentCard = {\n    name: agent.id || agentId,\n    description: instructions,\n    url: executionUrl,\n    provider,\n    version,\n    capabilities: {\n      streaming: true,\n      // All agents support streaming\n      pushNotifications: false,\n      stateTransitionHistory: false\n    },\n    defaultInputModes: [\"text\"],\n    defaultOutputModes: [\"text\"],\n    // Convert agent tools to skills format for A2A protocol\n    skills: Object.entries(tools).map(([toolId, tool]) => ({\n      id: toolId,\n      name: toolId,\n      description: tool.description || `Tool: ${toolId}`,\n      // Optional fields\n      tags: [\"tool\"]\n    }))\n  };\n  return agentCard;\n}\nfunction validateMessageSendParams(params) {\n  try {\n    messageSendParamsSchema.parse(params);\n  } catch (error) {\n    if (error instanceof z.ZodError) {\n      throw MastraA2AError.invalidParams(error.errors[0].message);\n    }\n    throw error;\n  }\n}\nasync function handleMessageSend({\n  requestId,\n  params,\n  taskStore,\n  agent,\n  agentId,\n  logger,\n  runtimeContext\n}) {\n  validateMessageSendParams(params);\n  const { message, metadata } = params;\n  const { contextId } = message;\n  const taskId = message.taskId || crypto.randomUUID();\n  let currentData = await loadOrCreateTask({\n    taskId,\n    taskStore,\n    agentId,\n    message,\n    contextId,\n    metadata\n  });\n  createTaskContext({\n    task: currentData,\n    userMessage: message,\n    history: currentData.history || [],\n    activeCancellations: taskStore.activeCancellations\n  });\n  try {\n    const { text } = await agent.generate([convertToCoreMessage(message)], {\n      runId: taskId,\n      runtimeContext\n    });\n    currentData = applyUpdateToTask(currentData, {\n      state: \"completed\",\n      message: {\n        messageId: crypto.randomUUID(),\n        role: \"agent\",\n        parts: [\n          {\n            kind: \"text\",\n            text\n          }\n        ],\n        kind: \"message\"\n      }\n    });\n    await taskStore.save({ agentId, data: currentData });\n  } catch (handlerError) {\n    const failureStatusUpdate = {\n      state: \"failed\",\n      message: {\n        messageId: crypto.randomUUID(),\n        role: \"agent\",\n        parts: [\n          {\n            kind: \"text\",\n            text: `Handler failed: ${handlerError instanceof Error ? handlerError.message : String(handlerError)}`\n          }\n        ],\n        kind: \"message\"\n      }\n    };\n    currentData = applyUpdateToTask(currentData, failureStatusUpdate);\n    try {\n      await taskStore.save({ agentId, data: currentData });\n    } catch (saveError) {\n      logger?.error(`Failed to save task ${currentData.id} after handler error:`, saveError?.message);\n    }\n    return normalizeError(handlerError, requestId, currentData.id, logger);\n  }\n  return createSuccessResponse(requestId, currentData);\n}\nasync function handleTaskGet({\n  requestId,\n  taskStore,\n  agentId,\n  taskId\n}) {\n  const task = await taskStore.load({ agentId, taskId });\n  if (!task) {\n    throw MastraA2AError.taskNotFound(taskId);\n  }\n  return createSuccessResponse(requestId, task);\n}\nasync function* handleMessageStream({\n  requestId,\n  params,\n  taskStore,\n  agent,\n  agentId,\n  logger,\n  runtimeContext\n}) {\n  yield createSuccessResponse(requestId, {\n    state: \"working\",\n    message: {\n      messageId: crypto.randomUUID(),\n      kind: \"message\",\n      role: \"agent\",\n      parts: [{ kind: \"text\", text: \"Generating response...\" }]\n    }\n  });\n  let result;\n  try {\n    result = await handleMessageSend({\n      requestId,\n      params,\n      taskStore,\n      agent,\n      agentId,\n      runtimeContext,\n      logger\n    });\n  } catch (err) {\n    if (!(err instanceof MastraA2AError)) {\n      throw err;\n    }\n    result = createErrorResponse(requestId, err.toJSONRPCError());\n  }\n  yield result;\n}\nasync function handleTaskCancel({\n  requestId,\n  taskStore,\n  agentId,\n  taskId,\n  logger\n}) {\n  let data = await taskStore.load({\n    agentId,\n    taskId\n  });\n  if (!data) {\n    throw MastraA2AError.taskNotFound(taskId);\n  }\n  const finalStates = [\"completed\", \"failed\", \"canceled\"];\n  if (finalStates.includes(data.status.state)) {\n    logger?.info(`Task ${taskId} already in final state ${data.status.state}, cannot cancel.`);\n    return createSuccessResponse(requestId, data);\n  }\n  taskStore.activeCancellations.add(taskId);\n  const cancelUpdate = {\n    state: \"canceled\",\n    message: {\n      role: \"agent\",\n      parts: [{ kind: \"text\", text: \"Task cancelled by request.\" }],\n      kind: \"message\",\n      messageId: crypto.randomUUID()\n    }\n  };\n  data = applyUpdateToTask(data, cancelUpdate);\n  await taskStore.save({ agentId, data });\n  taskStore.activeCancellations.delete(taskId);\n  return createSuccessResponse(requestId, data);\n}\nasync function getAgentExecutionHandler({\n  requestId,\n  mastra,\n  agentId,\n  runtimeContext,\n  method,\n  params,\n  taskStore,\n  logger\n}) {\n  const agent = mastra.getAgent(agentId);\n  let taskId;\n  try {\n    taskId = \"id\" in params ? params.id : params.message?.taskId || \"No task ID provided\";\n    switch (method) {\n      case \"message/send\": {\n        const result2 = await handleMessageSend({\n          requestId,\n          params,\n          taskStore,\n          agent,\n          agentId,\n          runtimeContext\n        });\n        return result2;\n      }\n      case \"message/stream\":\n        const result = await handleMessageStream({\n          requestId,\n          taskStore,\n          params,\n          agent,\n          agentId,\n          runtimeContext\n        });\n        return result;\n      case \"tasks/get\": {\n        const result2 = await handleTaskGet({\n          requestId,\n          taskStore,\n          agentId,\n          taskId\n        });\n        return result2;\n      }\n      case \"tasks/cancel\": {\n        const result2 = await handleTaskCancel({\n          requestId,\n          taskStore,\n          agentId,\n          taskId\n        });\n        return result2;\n      }\n      default:\n        throw MastraA2AError.methodNotFound(method);\n    }\n  } catch (error) {\n    if (error instanceof MastraA2AError && taskId && !error.taskId) {\n      error.taskId = taskId;\n    }\n    return normalizeError(error, requestId, taskId, logger);\n  }\n}\n\nexport { a2a_exports, getAgentCardByIdHandler, getAgentExecutionHandler, handleMessageSend, handleMessageStream, handleTaskCancel, handleTaskGet };\n//# sourceMappingURL=chunk-5QUKZCEF.js.map\n//# sourceMappingURL=chunk-5QUKZCEF.js.map","// src/utils/stream.ts\nvar StreamingApi = class {\n  writer;\n  encoder;\n  writable;\n  abortSubscribers = [];\n  responseReadable;\n  aborted = false;\n  closed = false;\n  constructor(writable, _readable) {\n    this.writable = writable;\n    this.writer = writable.getWriter();\n    this.encoder = new TextEncoder();\n    const reader = _readable.getReader();\n    this.abortSubscribers.push(async () => {\n      await reader.cancel();\n    });\n    this.responseReadable = new ReadableStream({\n      async pull(controller) {\n        const { done, value } = await reader.read();\n        done ? controller.close() : controller.enqueue(value);\n      },\n      cancel: () => {\n        this.abort();\n      }\n    });\n  }\n  async write(input) {\n    try {\n      if (typeof input === \"string\") {\n        input = this.encoder.encode(input);\n      }\n      await this.writer.write(input);\n    } catch {\n    }\n    return this;\n  }\n  async writeln(input) {\n    await this.write(input + \"\\n\");\n    return this;\n  }\n  sleep(ms) {\n    return new Promise((res) => setTimeout(res, ms));\n  }\n  async close() {\n    try {\n      await this.writer.close();\n    } catch {\n    }\n    this.closed = true;\n  }\n  async pipe(body) {\n    this.writer.releaseLock();\n    await body.pipeTo(this.writable, { preventClose: true });\n    this.writer = this.writable.getWriter();\n  }\n  onAbort(listener) {\n    this.abortSubscribers.push(listener);\n  }\n  abort() {\n    if (!this.aborted) {\n      this.aborted = true;\n      this.abortSubscribers.forEach((subscriber) => subscriber());\n    }\n  }\n};\nexport {\n  StreamingApi\n};\n","// src/helper/streaming/utils.ts\nvar isOldBunVersion = () => {\n  const version = typeof Bun !== \"undefined\" ? Bun.version : void 0;\n  if (version === void 0) {\n    return false;\n  }\n  const result = version.startsWith(\"1.1\") || version.startsWith(\"1.0\") || version.startsWith(\"0.\");\n  isOldBunVersion = () => result;\n  return result;\n};\nexport {\n  isOldBunVersion\n};\n","// src/helper/streaming/stream.ts\nimport { StreamingApi } from \"../../utils/stream.js\";\nimport { isOldBunVersion } from \"./utils.js\";\nvar contextStash = /* @__PURE__ */ new WeakMap();\nvar stream = (c, cb, onError) => {\n  const { readable, writable } = new TransformStream();\n  const stream2 = new StreamingApi(writable, readable);\n  if (isOldBunVersion()) {\n    c.req.raw.signal.addEventListener(\"abort\", () => {\n      if (!stream2.closed) {\n        stream2.abort();\n      }\n    });\n  }\n  contextStash.set(stream2.responseReadable, c);\n  (async () => {\n    try {\n      await cb(stream2);\n    } catch (e) {\n      if (e === void 0) {\n      } else if (e instanceof Error && onError) {\n        await onError(e, stream2);\n      } else {\n        console.error(e);\n      }\n    } finally {\n      stream2.close();\n    }\n  })();\n  return c.newResponse(stream2.responseReadable);\n};\nexport {\n  stream\n};\n","// src/server/http-exception.ts\nvar HTTPException = class extends Error {\n  res;\n  status;\n  /**\n   * Creates an instance of `HTTPException`.\n   * @param status - HTTP status code for the exception. Defaults to 500.\n   * @param options - Additional options for the exception.\n   */\n  constructor(status = 500, options) {\n    super(options?.message, { cause: options?.cause });\n    this.res = options?.res;\n    this.status = status;\n    this.stack = options?.stack || this.stack;\n  }\n  /**\n   * Returns the response object associated with the exception.\n   * If a response object is not provided, a new response is created with the error message and status code.\n   * @returns The response object.\n   */\n  getResponse() {\n    if (this.res) {\n      const newResponse = new Response(this.res.body, {\n        status: this.status,\n        headers: this.res.headers\n      });\n      return newResponse;\n    }\n    return new Response(this.message, {\n      status: this.status\n    });\n  }\n};\n\nexport { HTTPException };\n//# sourceMappingURL=chunk-MMROOK5J.js.map\n//# sourceMappingURL=chunk-MMROOK5J.js.map","import { HTTPException } from './chunk-MMROOK5J.js';\n\n// src/server/handlers/utils.ts\nfunction validateBody(body) {\n  const errorResponse = Object.entries(body).reduce((acc, [key, value]) => {\n    if (!value) {\n      acc[key] = `Argument \"${key}\" is required`;\n    }\n    return acc;\n  }, {});\n  if (Object.keys(errorResponse).length > 0) {\n    throw new HTTPException(400, { message: Object.values(errorResponse)[0] });\n  }\n}\n\nexport { validateBody };\n//# sourceMappingURL=chunk-OW4FX5TS.js.map\n//# sourceMappingURL=chunk-OW4FX5TS.js.map","// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/double-indexed-kv.js\nvar DoubleIndexedKV = class {\n  constructor() {\n    this.keyToValue = /* @__PURE__ */ new Map();\n    this.valueToKey = /* @__PURE__ */ new Map();\n  }\n  set(key, value) {\n    this.keyToValue.set(key, value);\n    this.valueToKey.set(value, key);\n  }\n  getByKey(key) {\n    return this.keyToValue.get(key);\n  }\n  getByValue(value) {\n    return this.valueToKey.get(value);\n  }\n  clear() {\n    this.keyToValue.clear();\n    this.valueToKey.clear();\n  }\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/registry.js\nvar Registry = class {\n  constructor(generateIdentifier) {\n    this.generateIdentifier = generateIdentifier;\n    this.kv = new DoubleIndexedKV();\n  }\n  register(value, identifier) {\n    if (this.kv.getByValue(value)) {\n      return;\n    }\n    if (!identifier) {\n      identifier = this.generateIdentifier(value);\n    }\n    this.kv.set(identifier, value);\n  }\n  clear() {\n    this.kv.clear();\n  }\n  getIdentifier(value) {\n    return this.kv.getByValue(value);\n  }\n  getValue(identifier) {\n    return this.kv.getByKey(identifier);\n  }\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/class-registry.js\nvar ClassRegistry = class extends Registry {\n  constructor() {\n    super((c) => c.name);\n    this.classToAllowedProps = /* @__PURE__ */ new Map();\n  }\n  register(value, options) {\n    if (typeof options === \"object\") {\n      if (options.allowProps) {\n        this.classToAllowedProps.set(value, options.allowProps);\n      }\n      super.register(value, options.identifier);\n    } else {\n      super.register(value, options);\n    }\n  }\n  getAllowedProps(value) {\n    return this.classToAllowedProps.get(value);\n  }\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/util.js\nfunction valuesOfObj(record) {\n  if (\"values\" in Object) {\n    return Object.values(record);\n  }\n  const values = [];\n  for (const key in record) {\n    if (record.hasOwnProperty(key)) {\n      values.push(record[key]);\n    }\n  }\n  return values;\n}\nfunction find(record, predicate) {\n  const values = valuesOfObj(record);\n  if (\"find\" in values) {\n    return values.find(predicate);\n  }\n  const valuesNotNever = values;\n  for (let i = 0; i < valuesNotNever.length; i++) {\n    const value = valuesNotNever[i];\n    if (predicate(value)) {\n      return value;\n    }\n  }\n  return void 0;\n}\nfunction forEach(record, run) {\n  Object.entries(record).forEach(([key, value]) => run(value, key));\n}\nfunction includes(arr, value) {\n  return arr.indexOf(value) !== -1;\n}\nfunction findArr(record, predicate) {\n  for (let i = 0; i < record.length; i++) {\n    const value = record[i];\n    if (predicate(value)) {\n      return value;\n    }\n  }\n  return void 0;\n}\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/custom-transformer-registry.js\nvar CustomTransformerRegistry = class {\n  constructor() {\n    this.transfomers = {};\n  }\n  register(transformer) {\n    this.transfomers[transformer.name] = transformer;\n  }\n  findApplicable(v) {\n    return find(this.transfomers, (transformer) => transformer.isApplicable(v));\n  }\n  findByName(name) {\n    return this.transfomers[name];\n  }\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/is.js\nvar getType = (payload) => Object.prototype.toString.call(payload).slice(8, -1);\nvar isUndefined = (payload) => typeof payload === \"undefined\";\nvar isNull = (payload) => payload === null;\nvar isPlainObject = (payload) => {\n  if (typeof payload !== \"object\" || payload === null)\n    return false;\n  if (payload === Object.prototype)\n    return false;\n  if (Object.getPrototypeOf(payload) === null)\n    return true;\n  return Object.getPrototypeOf(payload) === Object.prototype;\n};\nvar isEmptyObject = (payload) => isPlainObject(payload) && Object.keys(payload).length === 0;\nvar isArray = (payload) => Array.isArray(payload);\nvar isString = (payload) => typeof payload === \"string\";\nvar isNumber = (payload) => typeof payload === \"number\" && !isNaN(payload);\nvar isBoolean = (payload) => typeof payload === \"boolean\";\nvar isRegExp = (payload) => payload instanceof RegExp;\nvar isMap = (payload) => payload instanceof Map;\nvar isSet = (payload) => payload instanceof Set;\nvar isSymbol = (payload) => getType(payload) === \"Symbol\";\nvar isDate = (payload) => payload instanceof Date && !isNaN(payload.valueOf());\nvar isError = (payload) => payload instanceof Error;\nvar isNaNValue = (payload) => typeof payload === \"number\" && isNaN(payload);\nvar isPrimitive = (payload) => isBoolean(payload) || isNull(payload) || isUndefined(payload) || isNumber(payload) || isString(payload) || isSymbol(payload);\nvar isBigint = (payload) => typeof payload === \"bigint\";\nvar isInfinite = (payload) => payload === Infinity || payload === -Infinity;\nvar isTypedArray = (payload) => ArrayBuffer.isView(payload) && !(payload instanceof DataView);\nvar isURL = (payload) => payload instanceof URL;\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/pathstringifier.js\nvar escapeKey = (key) => key.replace(/\\./g, \"\\\\.\");\nvar stringifyPath = (path) => path.map(String).map(escapeKey).join(\".\");\nvar parsePath = (string) => {\n  const result = [];\n  let segment = \"\";\n  for (let i = 0; i < string.length; i++) {\n    let char = string.charAt(i);\n    const isEscapedDot = char === \"\\\\\" && string.charAt(i + 1) === \".\";\n    if (isEscapedDot) {\n      segment += \".\";\n      i++;\n      continue;\n    }\n    const isEndOfSegment = char === \".\";\n    if (isEndOfSegment) {\n      result.push(segment);\n      segment = \"\";\n      continue;\n    }\n    segment += char;\n  }\n  const lastSegment = segment;\n  result.push(lastSegment);\n  return result;\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/transformer.js\nfunction simpleTransformation(isApplicable, annotation, transform, untransform) {\n  return {\n    isApplicable,\n    annotation,\n    transform,\n    untransform\n  };\n}\nvar simpleRules = [\n  simpleTransformation(isUndefined, \"undefined\", () => null, () => void 0),\n  simpleTransformation(isBigint, \"bigint\", (v) => v.toString(), (v) => {\n    if (typeof BigInt !== \"undefined\") {\n      return BigInt(v);\n    }\n    console.error(\"Please add a BigInt polyfill.\");\n    return v;\n  }),\n  simpleTransformation(isDate, \"Date\", (v) => v.toISOString(), (v) => new Date(v)),\n  simpleTransformation(isError, \"Error\", (v, superJson) => {\n    const baseError = {\n      name: v.name,\n      message: v.message\n    };\n    superJson.allowedErrorProps.forEach((prop) => {\n      baseError[prop] = v[prop];\n    });\n    return baseError;\n  }, (v, superJson) => {\n    const e = new Error(v.message);\n    e.name = v.name;\n    e.stack = v.stack;\n    superJson.allowedErrorProps.forEach((prop) => {\n      e[prop] = v[prop];\n    });\n    return e;\n  }),\n  simpleTransformation(isRegExp, \"regexp\", (v) => \"\" + v, (regex) => {\n    const body = regex.slice(1, regex.lastIndexOf(\"/\"));\n    const flags = regex.slice(regex.lastIndexOf(\"/\") + 1);\n    return new RegExp(body, flags);\n  }),\n  simpleTransformation(\n    isSet,\n    \"set\",\n    // (sets only exist in es6+)\n    // eslint-disable-next-line es5/no-es6-methods\n    (v) => [...v.values()],\n    (v) => new Set(v)\n  ),\n  simpleTransformation(isMap, \"map\", (v) => [...v.entries()], (v) => new Map(v)),\n  simpleTransformation((v) => isNaNValue(v) || isInfinite(v), \"number\", (v) => {\n    if (isNaNValue(v)) {\n      return \"NaN\";\n    }\n    if (v > 0) {\n      return \"Infinity\";\n    } else {\n      return \"-Infinity\";\n    }\n  }, Number),\n  simpleTransformation((v) => v === 0 && 1 / v === -Infinity, \"number\", () => {\n    return \"-0\";\n  }, Number),\n  simpleTransformation(isURL, \"URL\", (v) => v.toString(), (v) => new URL(v))\n];\nfunction compositeTransformation(isApplicable, annotation, transform, untransform) {\n  return {\n    isApplicable,\n    annotation,\n    transform,\n    untransform\n  };\n}\nvar symbolRule = compositeTransformation((s, superJson) => {\n  if (isSymbol(s)) {\n    const isRegistered = !!superJson.symbolRegistry.getIdentifier(s);\n    return isRegistered;\n  }\n  return false;\n}, (s, superJson) => {\n  const identifier = superJson.symbolRegistry.getIdentifier(s);\n  return [\"symbol\", identifier];\n}, (v) => v.description, (_, a, superJson) => {\n  const value = superJson.symbolRegistry.getValue(a[1]);\n  if (!value) {\n    throw new Error(\"Trying to deserialize unknown symbol\");\n  }\n  return value;\n});\nvar constructorToName = [\n  Int8Array,\n  Uint8Array,\n  Int16Array,\n  Uint16Array,\n  Int32Array,\n  Uint32Array,\n  Float32Array,\n  Float64Array,\n  Uint8ClampedArray\n].reduce((obj, ctor) => {\n  obj[ctor.name] = ctor;\n  return obj;\n}, {});\nvar typedArrayRule = compositeTransformation(isTypedArray, (v) => [\"typed-array\", v.constructor.name], (v) => [...v], (v, a) => {\n  const ctor = constructorToName[a[1]];\n  if (!ctor) {\n    throw new Error(\"Trying to deserialize unknown typed array\");\n  }\n  return new ctor(v);\n});\nfunction isInstanceOfRegisteredClass(potentialClass, superJson) {\n  if (potentialClass?.constructor) {\n    const isRegistered = !!superJson.classRegistry.getIdentifier(potentialClass.constructor);\n    return isRegistered;\n  }\n  return false;\n}\nvar classRule = compositeTransformation(isInstanceOfRegisteredClass, (clazz, superJson) => {\n  const identifier = superJson.classRegistry.getIdentifier(clazz.constructor);\n  return [\"class\", identifier];\n}, (clazz, superJson) => {\n  const allowedProps = superJson.classRegistry.getAllowedProps(clazz.constructor);\n  if (!allowedProps) {\n    return { ...clazz };\n  }\n  const result = {};\n  allowedProps.forEach((prop) => {\n    result[prop] = clazz[prop];\n  });\n  return result;\n}, (v, a, superJson) => {\n  const clazz = superJson.classRegistry.getValue(a[1]);\n  if (!clazz) {\n    throw new Error(`Trying to deserialize unknown class '${a[1]}' - check https://github.com/blitz-js/superjson/issues/116#issuecomment-773996564`);\n  }\n  return Object.assign(Object.create(clazz.prototype), v);\n});\nvar customRule = compositeTransformation((value, superJson) => {\n  return !!superJson.customTransformerRegistry.findApplicable(value);\n}, (value, superJson) => {\n  const transformer = superJson.customTransformerRegistry.findApplicable(value);\n  return [\"custom\", transformer.name];\n}, (value, superJson) => {\n  const transformer = superJson.customTransformerRegistry.findApplicable(value);\n  return transformer.serialize(value);\n}, (v, a, superJson) => {\n  const transformer = superJson.customTransformerRegistry.findByName(a[1]);\n  if (!transformer) {\n    throw new Error(\"Trying to deserialize unknown custom value\");\n  }\n  return transformer.deserialize(v);\n});\nvar compositeRules = [classRule, symbolRule, customRule, typedArrayRule];\nvar transformValue = (value, superJson) => {\n  const applicableCompositeRule = findArr(compositeRules, (rule) => rule.isApplicable(value, superJson));\n  if (applicableCompositeRule) {\n    return {\n      value: applicableCompositeRule.transform(value, superJson),\n      type: applicableCompositeRule.annotation(value, superJson)\n    };\n  }\n  const applicableSimpleRule = findArr(simpleRules, (rule) => rule.isApplicable(value, superJson));\n  if (applicableSimpleRule) {\n    return {\n      value: applicableSimpleRule.transform(value, superJson),\n      type: applicableSimpleRule.annotation\n    };\n  }\n  return void 0;\n};\nvar simpleRulesByAnnotation = {};\nsimpleRules.forEach((rule) => {\n  simpleRulesByAnnotation[rule.annotation] = rule;\n});\nvar untransformValue = (json, type, superJson) => {\n  if (isArray(type)) {\n    switch (type[0]) {\n      case \"symbol\":\n        return symbolRule.untransform(json, type, superJson);\n      case \"class\":\n        return classRule.untransform(json, type, superJson);\n      case \"custom\":\n        return customRule.untransform(json, type, superJson);\n      case \"typed-array\":\n        return typedArrayRule.untransform(json, type, superJson);\n      default:\n        throw new Error(\"Unknown transformation: \" + type);\n    }\n  } else {\n    const transformation = simpleRulesByAnnotation[type];\n    if (!transformation) {\n      throw new Error(\"Unknown transformation: \" + type);\n    }\n    return transformation.untransform(json, superJson);\n  }\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/accessDeep.js\nvar getNthKey = (value, n) => {\n  if (n > value.size)\n    throw new Error(\"index out of bounds\");\n  const keys = value.keys();\n  while (n > 0) {\n    keys.next();\n    n--;\n  }\n  return keys.next().value;\n};\nfunction validatePath(path) {\n  if (includes(path, \"__proto__\")) {\n    throw new Error(\"__proto__ is not allowed as a property\");\n  }\n  if (includes(path, \"prototype\")) {\n    throw new Error(\"prototype is not allowed as a property\");\n  }\n  if (includes(path, \"constructor\")) {\n    throw new Error(\"constructor is not allowed as a property\");\n  }\n}\nvar getDeep = (object, path) => {\n  validatePath(path);\n  for (let i = 0; i < path.length; i++) {\n    const key = path[i];\n    if (isSet(object)) {\n      object = getNthKey(object, +key);\n    } else if (isMap(object)) {\n      const row = +key;\n      const type = +path[++i] === 0 ? \"key\" : \"value\";\n      const keyOfRow = getNthKey(object, row);\n      switch (type) {\n        case \"key\":\n          object = keyOfRow;\n          break;\n        case \"value\":\n          object = object.get(keyOfRow);\n          break;\n      }\n    } else {\n      object = object[key];\n    }\n  }\n  return object;\n};\nvar setDeep = (object, path, mapper) => {\n  validatePath(path);\n  if (path.length === 0) {\n    return mapper(object);\n  }\n  let parent = object;\n  for (let i = 0; i < path.length - 1; i++) {\n    const key = path[i];\n    if (isArray(parent)) {\n      const index = +key;\n      parent = parent[index];\n    } else if (isPlainObject(parent)) {\n      parent = parent[key];\n    } else if (isSet(parent)) {\n      const row = +key;\n      parent = getNthKey(parent, row);\n    } else if (isMap(parent)) {\n      const isEnd = i === path.length - 2;\n      if (isEnd) {\n        break;\n      }\n      const row = +key;\n      const type = +path[++i] === 0 ? \"key\" : \"value\";\n      const keyOfRow = getNthKey(parent, row);\n      switch (type) {\n        case \"key\":\n          parent = keyOfRow;\n          break;\n        case \"value\":\n          parent = parent.get(keyOfRow);\n          break;\n      }\n    }\n  }\n  const lastKey = path[path.length - 1];\n  if (isArray(parent)) {\n    parent[+lastKey] = mapper(parent[+lastKey]);\n  } else if (isPlainObject(parent)) {\n    parent[lastKey] = mapper(parent[lastKey]);\n  }\n  if (isSet(parent)) {\n    const oldValue = getNthKey(parent, +lastKey);\n    const newValue = mapper(oldValue);\n    if (oldValue !== newValue) {\n      parent.delete(oldValue);\n      parent.add(newValue);\n    }\n  }\n  if (isMap(parent)) {\n    const row = +path[path.length - 2];\n    const keyToRow = getNthKey(parent, row);\n    const type = +lastKey === 0 ? \"key\" : \"value\";\n    switch (type) {\n      case \"key\": {\n        const newKey = mapper(keyToRow);\n        parent.set(newKey, parent.get(keyToRow));\n        if (newKey !== keyToRow) {\n          parent.delete(keyToRow);\n        }\n        break;\n      }\n      case \"value\": {\n        parent.set(keyToRow, mapper(parent.get(keyToRow)));\n        break;\n      }\n    }\n  }\n  return object;\n};\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/plainer.js\nfunction traverse(tree, walker2, origin = []) {\n  if (!tree) {\n    return;\n  }\n  if (!isArray(tree)) {\n    forEach(tree, (subtree, key) => traverse(subtree, walker2, [...origin, ...parsePath(key)]));\n    return;\n  }\n  const [nodeValue, children] = tree;\n  if (children) {\n    forEach(children, (child, key) => {\n      traverse(child, walker2, [...origin, ...parsePath(key)]);\n    });\n  }\n  walker2(nodeValue, origin);\n}\nfunction applyValueAnnotations(plain, annotations, superJson) {\n  traverse(annotations, (type, path) => {\n    plain = setDeep(plain, path, (v) => untransformValue(v, type, superJson));\n  });\n  return plain;\n}\nfunction applyReferentialEqualityAnnotations(plain, annotations) {\n  function apply(identicalPaths, path) {\n    const object = getDeep(plain, parsePath(path));\n    identicalPaths.map(parsePath).forEach((identicalObjectPath) => {\n      plain = setDeep(plain, identicalObjectPath, () => object);\n    });\n  }\n  if (isArray(annotations)) {\n    const [root, other] = annotations;\n    root.forEach((identicalPath) => {\n      plain = setDeep(plain, parsePath(identicalPath), () => plain);\n    });\n    if (other) {\n      forEach(other, apply);\n    }\n  } else {\n    forEach(annotations, apply);\n  }\n  return plain;\n}\nvar isDeep = (object, superJson) => isPlainObject(object) || isArray(object) || isMap(object) || isSet(object) || isInstanceOfRegisteredClass(object, superJson);\nfunction addIdentity(object, path, identities) {\n  const existingSet = identities.get(object);\n  if (existingSet) {\n    existingSet.push(path);\n  } else {\n    identities.set(object, [path]);\n  }\n}\nfunction generateReferentialEqualityAnnotations(identitites, dedupe) {\n  const result = {};\n  let rootEqualityPaths = void 0;\n  identitites.forEach((paths) => {\n    if (paths.length <= 1) {\n      return;\n    }\n    if (!dedupe) {\n      paths = paths.map((path) => path.map(String)).sort((a, b) => a.length - b.length);\n    }\n    const [representativePath, ...identicalPaths] = paths;\n    if (representativePath.length === 0) {\n      rootEqualityPaths = identicalPaths.map(stringifyPath);\n    } else {\n      result[stringifyPath(representativePath)] = identicalPaths.map(stringifyPath);\n    }\n  });\n  if (rootEqualityPaths) {\n    if (isEmptyObject(result)) {\n      return [rootEqualityPaths];\n    } else {\n      return [rootEqualityPaths, result];\n    }\n  } else {\n    return isEmptyObject(result) ? void 0 : result;\n  }\n}\nvar walker = (object, identities, superJson, dedupe, path = [], objectsInThisPath = [], seenObjects = /* @__PURE__ */ new Map()) => {\n  const primitive = isPrimitive(object);\n  if (!primitive) {\n    addIdentity(object, path, identities);\n    const seen = seenObjects.get(object);\n    if (seen) {\n      return dedupe ? {\n        transformedValue: null\n      } : seen;\n    }\n  }\n  if (!isDeep(object, superJson)) {\n    const transformed2 = transformValue(object, superJson);\n    const result2 = transformed2 ? {\n      transformedValue: transformed2.value,\n      annotations: [transformed2.type]\n    } : {\n      transformedValue: object\n    };\n    if (!primitive) {\n      seenObjects.set(object, result2);\n    }\n    return result2;\n  }\n  if (includes(objectsInThisPath, object)) {\n    return {\n      transformedValue: null\n    };\n  }\n  const transformationResult = transformValue(object, superJson);\n  const transformed = transformationResult?.value ?? object;\n  const transformedValue = isArray(transformed) ? [] : {};\n  const innerAnnotations = {};\n  forEach(transformed, (value, index) => {\n    if (index === \"__proto__\" || index === \"constructor\" || index === \"prototype\") {\n      throw new Error(`Detected property ${index}. This is a prototype pollution risk, please remove it from your object.`);\n    }\n    const recursiveResult = walker(value, identities, superJson, dedupe, [...path, index], [...objectsInThisPath, object], seenObjects);\n    transformedValue[index] = recursiveResult.transformedValue;\n    if (isArray(recursiveResult.annotations)) {\n      innerAnnotations[index] = recursiveResult.annotations;\n    } else if (isPlainObject(recursiveResult.annotations)) {\n      forEach(recursiveResult.annotations, (tree, key) => {\n        innerAnnotations[escapeKey(index) + \".\" + key] = tree;\n      });\n    }\n  });\n  const result = isEmptyObject(innerAnnotations) ? {\n    transformedValue,\n    annotations: !!transformationResult ? [transformationResult.type] : void 0\n  } : {\n    transformedValue,\n    annotations: !!transformationResult ? [transformationResult.type, innerAnnotations] : innerAnnotations\n  };\n  if (!primitive) {\n    seenObjects.set(object, result);\n  }\n  return result;\n};\n\n// ../../node_modules/.pnpm/is-what@4.1.16/node_modules/is-what/dist/index.js\nfunction getType2(payload) {\n  return Object.prototype.toString.call(payload).slice(8, -1);\n}\nfunction isArray2(payload) {\n  return getType2(payload) === \"Array\";\n}\nfunction isPlainObject2(payload) {\n  if (getType2(payload) !== \"Object\")\n    return false;\n  const prototype = Object.getPrototypeOf(payload);\n  return !!prototype && prototype.constructor === Object && prototype === Object.prototype;\n}\n\n// ../../node_modules/.pnpm/copy-anything@3.0.5/node_modules/copy-anything/dist/index.js\nfunction assignProp(carry, key, newVal, originalObject, includeNonenumerable) {\n  const propType = {}.propertyIsEnumerable.call(originalObject, key) ? \"enumerable\" : \"nonenumerable\";\n  if (propType === \"enumerable\")\n    carry[key] = newVal;\n  if (includeNonenumerable && propType === \"nonenumerable\") {\n    Object.defineProperty(carry, key, {\n      value: newVal,\n      enumerable: false,\n      writable: true,\n      configurable: true\n    });\n  }\n}\nfunction copy(target, options = {}) {\n  if (isArray2(target)) {\n    return target.map((item) => copy(item, options));\n  }\n  if (!isPlainObject2(target)) {\n    return target;\n  }\n  const props = Object.getOwnPropertyNames(target);\n  const symbols = Object.getOwnPropertySymbols(target);\n  return [...props, ...symbols].reduce((carry, key) => {\n    if (isArray2(options.props) && !options.props.includes(key)) {\n      return carry;\n    }\n    const val = target[key];\n    const newVal = copy(val, options);\n    assignProp(carry, key, newVal, target, options.nonenumerable);\n    return carry;\n  }, {});\n}\n\n// ../../node_modules/.pnpm/superjson@2.2.2/node_modules/superjson/dist/index.js\nvar SuperJSON = class {\n  /**\n   * @param dedupeReferentialEqualities  If true, SuperJSON will make sure only one instance of referentially equal objects are serialized and the rest are replaced with `null`.\n   */\n  constructor({ dedupe = false } = {}) {\n    this.classRegistry = new ClassRegistry();\n    this.symbolRegistry = new Registry((s) => s.description ?? \"\");\n    this.customTransformerRegistry = new CustomTransformerRegistry();\n    this.allowedErrorProps = [];\n    this.dedupe = dedupe;\n  }\n  serialize(object) {\n    const identities = /* @__PURE__ */ new Map();\n    const output = walker(object, identities, this, this.dedupe);\n    const res = {\n      json: output.transformedValue\n    };\n    if (output.annotations) {\n      res.meta = {\n        ...res.meta,\n        values: output.annotations\n      };\n    }\n    const equalityAnnotations = generateReferentialEqualityAnnotations(identities, this.dedupe);\n    if (equalityAnnotations) {\n      res.meta = {\n        ...res.meta,\n        referentialEqualities: equalityAnnotations\n      };\n    }\n    return res;\n  }\n  deserialize(payload) {\n    const { json, meta } = payload;\n    let result = copy(json);\n    if (meta?.values) {\n      result = applyValueAnnotations(result, meta.values, this);\n    }\n    if (meta?.referentialEqualities) {\n      result = applyReferentialEqualityAnnotations(result, meta.referentialEqualities);\n    }\n    return result;\n  }\n  stringify(object) {\n    return JSON.stringify(this.serialize(object));\n  }\n  parse(string) {\n    return this.deserialize(JSON.parse(string));\n  }\n  registerClass(v, options) {\n    this.classRegistry.register(v, options);\n  }\n  registerSymbol(v, identifier) {\n    this.symbolRegistry.register(v, identifier);\n  }\n  registerCustom(transformer, name) {\n    this.customTransformerRegistry.register({\n      name,\n      ...transformer\n    });\n  }\n  allowErrorProps(...props) {\n    this.allowedErrorProps.push(...props);\n  }\n};\nSuperJSON.defaultInstance = new SuperJSON();\nSuperJSON.serialize = SuperJSON.defaultInstance.serialize.bind(SuperJSON.defaultInstance);\nSuperJSON.deserialize = SuperJSON.defaultInstance.deserialize.bind(SuperJSON.defaultInstance);\nSuperJSON.stringify = SuperJSON.defaultInstance.stringify.bind(SuperJSON.defaultInstance);\nSuperJSON.parse = SuperJSON.defaultInstance.parse.bind(SuperJSON.defaultInstance);\nSuperJSON.registerClass = SuperJSON.defaultInstance.registerClass.bind(SuperJSON.defaultInstance);\nSuperJSON.registerSymbol = SuperJSON.defaultInstance.registerSymbol.bind(SuperJSON.defaultInstance);\nSuperJSON.registerCustom = SuperJSON.defaultInstance.registerCustom.bind(SuperJSON.defaultInstance);\nSuperJSON.allowErrorProps = SuperJSON.defaultInstance.allowErrorProps.bind(SuperJSON.defaultInstance);\nvar stringify = SuperJSON.stringify;\n\nexport { stringify };\n//# sourceMappingURL=chunk-LF2ZLOFP.js.map\n//# sourceMappingURL=chunk-LF2ZLOFP.js.map","import { HTTPException } from './chunk-MMROOK5J.js';\n\n// src/server/handlers/error.ts\nfunction handleError(error, defaultMessage) {\n  const apiError = error;\n  const apiErrorStatus = apiError.status || apiError.details?.status || 500;\n  throw new HTTPException(apiErrorStatus, {\n    message: apiError.message || defaultMessage,\n    stack: apiError.stack,\n    cause: apiError.cause\n  });\n}\n\nexport { handleError };\n//# sourceMappingURL=chunk-CY4TP3FK.js.map\n//# sourceMappingURL=chunk-CY4TP3FK.js.map","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { stringify } from './chunk-LF2ZLOFP.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __commonJS, __export, __toESM } from './chunk-G3PMV62Z.js';\nimport { z } from 'zod';\nimport z62, { z as z$1 } from 'zod/v4';\nimport { RuntimeContext } from '@mastra/core/runtime-context';\nimport { zodToJsonSchema } from '@mastra/core/utils/zod-to-json';\n\n// ../../node_modules/.pnpm/secure-json-parse@2.7.0/node_modules/secure-json-parse/index.js\nvar require_secure_json_parse = __commonJS({\n  \"../../node_modules/.pnpm/secure-json-parse@2.7.0/node_modules/secure-json-parse/index.js\"(exports, module) {\n    var hasBuffer = typeof Buffer !== \"undefined\";\n    var suspectProtoRx2 = /\"(?:_|\\\\u005[Ff])(?:_|\\\\u005[Ff])(?:p|\\\\u0070)(?:r|\\\\u0072)(?:o|\\\\u006[Ff])(?:t|\\\\u0074)(?:o|\\\\u006[Ff])(?:_|\\\\u005[Ff])(?:_|\\\\u005[Ff])\"\\s*:/;\n    var suspectConstructorRx2 = /\"(?:c|\\\\u0063)(?:o|\\\\u006[Ff])(?:n|\\\\u006[Ee])(?:s|\\\\u0073)(?:t|\\\\u0074)(?:r|\\\\u0072)(?:u|\\\\u0075)(?:c|\\\\u0063)(?:t|\\\\u0074)(?:o|\\\\u006[Ff])(?:r|\\\\u0072)\"\\s*:/;\n    function _parse2(text, reviver, options) {\n      if (options == null) {\n        if (reviver !== null && typeof reviver === \"object\") {\n          options = reviver;\n          reviver = void 0;\n        }\n      }\n      if (hasBuffer && Buffer.isBuffer(text)) {\n        text = text.toString();\n      }\n      if (text && text.charCodeAt(0) === 65279) {\n        text = text.slice(1);\n      }\n      const obj = JSON.parse(text, reviver);\n      if (obj === null || typeof obj !== \"object\") {\n        return obj;\n      }\n      const protoAction = options && options.protoAction || \"error\";\n      const constructorAction = options && options.constructorAction || \"error\";\n      if (protoAction === \"ignore\" && constructorAction === \"ignore\") {\n        return obj;\n      }\n      if (protoAction !== \"ignore\" && constructorAction !== \"ignore\") {\n        if (suspectProtoRx2.test(text) === false && suspectConstructorRx2.test(text) === false) {\n          return obj;\n        }\n      } else if (protoAction !== \"ignore\" && constructorAction === \"ignore\") {\n        if (suspectProtoRx2.test(text) === false) {\n          return obj;\n        }\n      } else {\n        if (suspectConstructorRx2.test(text) === false) {\n          return obj;\n        }\n      }\n      return filter2(obj, { protoAction, constructorAction, safe: options && options.safe });\n    }\n    function filter2(obj, { protoAction = \"error\", constructorAction = \"error\", safe } = {}) {\n      let next = [obj];\n      while (next.length) {\n        const nodes = next;\n        next = [];\n        for (const node of nodes) {\n          if (protoAction !== \"ignore\" && Object.prototype.hasOwnProperty.call(node, \"__proto__\")) {\n            if (safe === true) {\n              return null;\n            } else if (protoAction === \"error\") {\n              throw new SyntaxError(\"Object contains forbidden prototype property\");\n            }\n            delete node.__proto__;\n          }\n          if (constructorAction !== \"ignore\" && Object.prototype.hasOwnProperty.call(node, \"constructor\") && Object.prototype.hasOwnProperty.call(node.constructor, \"prototype\")) {\n            if (safe === true) {\n              return null;\n            } else if (constructorAction === \"error\") {\n              throw new SyntaxError(\"Object contains forbidden prototype property\");\n            }\n            delete node.constructor;\n          }\n          for (const key in node) {\n            const value = node[key];\n            if (value && typeof value === \"object\") {\n              next.push(value);\n            }\n          }\n        }\n      }\n      return obj;\n    }\n    function parse(text, reviver, options) {\n      const stackTraceLimit = Error.stackTraceLimit;\n      Error.stackTraceLimit = 0;\n      try {\n        return _parse2(text, reviver, options);\n      } finally {\n        Error.stackTraceLimit = stackTraceLimit;\n      }\n    }\n    function safeParse(text, reviver) {\n      const stackTraceLimit = Error.stackTraceLimit;\n      Error.stackTraceLimit = 0;\n      try {\n        return _parse2(text, reviver, { safe: true });\n      } catch (_e) {\n        return null;\n      } finally {\n        Error.stackTraceLimit = stackTraceLimit;\n      }\n    }\n    module.exports = parse;\n    module.exports.default = parse;\n    module.exports.parse = parse;\n    module.exports.safeParse = safeParse;\n    module.exports.scan = filter2;\n  }\n});\n\n// src/server/handlers/agents.ts\nvar agents_exports = {};\n__export(agents_exports, {\n  generateHandler: () => generateHandler,\n  generateLegacyHandler: () => generateLegacyHandler,\n  generateVNextHandler: () => generateVNextHandler,\n  getAgentByIdHandler: () => getAgentByIdHandler,\n  getAgentsHandler: () => getAgentsHandler,\n  getEvalsByAgentIdHandler: () => getEvalsByAgentIdHandler,\n  getLiveEvalsByAgentIdHandler: () => getLiveEvalsByAgentIdHandler,\n  getSerializedAgentTools: () => getSerializedAgentTools,\n  streamGenerateHandler: () => streamGenerateHandler,\n  streamGenerateLegacyHandler: () => streamGenerateLegacyHandler,\n  streamVNextGenerateHandler: () => streamVNextGenerateHandler,\n  streamVNextUIMessageHandler: () => streamVNextUIMessageHandler,\n  updateAgentModelHandler: () => updateAgentModelHandler\n});\n\n// ../../node_modules/.pnpm/@ai-sdk+provider@1.1.3/node_modules/@ai-sdk/provider/dist/index.mjs\nvar marker = \"vercel.ai.error\";\nvar symbol = Symbol.for(marker);\nvar _a;\nvar _AISDKError = class _AISDKError2 extends Error {\n  /**\n   * Creates an AI SDK Error.\n   *\n   * @param {Object} params - The parameters for creating the error.\n   * @param {string} params.name - The name of the error.\n   * @param {string} params.message - The error message.\n   * @param {unknown} [params.cause] - The underlying cause of the error.\n   */\n  constructor({\n    name: name142,\n    message,\n    cause\n  }) {\n    super(message);\n    this[_a] = true;\n    this.name = name142;\n    this.cause = cause;\n  }\n  /**\n   * Checks if the given error is an AI SDK Error.\n   * @param {unknown} error - The error to check.\n   * @returns {boolean} True if the error is an AI SDK Error, false otherwise.\n   */\n  static isInstance(error) {\n    return _AISDKError2.hasMarker(error, marker);\n  }\n  static hasMarker(error, marker152) {\n    const markerSymbol = Symbol.for(marker152);\n    return error != null && typeof error === \"object\" && markerSymbol in error && typeof error[markerSymbol] === \"boolean\" && error[markerSymbol] === true;\n  }\n};\n_a = symbol;\nvar AISDKError = _AISDKError;\nvar name = \"AI_APICallError\";\nvar marker2 = `vercel.ai.error.${name}`;\nvar symbol2 = Symbol.for(marker2);\nvar _a2;\nvar APICallError = class extends AISDKError {\n  constructor({\n    message,\n    url,\n    requestBodyValues,\n    statusCode,\n    responseHeaders,\n    responseBody,\n    cause,\n    isRetryable = statusCode != null && (statusCode === 408 || // request timeout\n    statusCode === 409 || // conflict\n    statusCode === 429 || // too many requests\n    statusCode >= 500),\n    // server error\n    data\n  }) {\n    super({ name, message, cause });\n    this[_a2] = true;\n    this.url = url;\n    this.requestBodyValues = requestBodyValues;\n    this.statusCode = statusCode;\n    this.responseHeaders = responseHeaders;\n    this.responseBody = responseBody;\n    this.isRetryable = isRetryable;\n    this.data = data;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker2);\n  }\n};\n_a2 = symbol2;\nvar name2 = \"AI_EmptyResponseBodyError\";\nvar marker3 = `vercel.ai.error.${name2}`;\nvar symbol3 = Symbol.for(marker3);\nvar _a3;\nvar EmptyResponseBodyError = class extends AISDKError {\n  // used in isInstance\n  constructor({ message = \"Empty response body\" } = {}) {\n    super({ name: name2, message });\n    this[_a3] = true;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker3);\n  }\n};\n_a3 = symbol3;\nfunction getErrorMessage(error) {\n  if (error == null) {\n    return \"unknown error\";\n  }\n  if (typeof error === \"string\") {\n    return error;\n  }\n  if (error instanceof Error) {\n    return error.message;\n  }\n  return JSON.stringify(error);\n}\nvar name3 = \"AI_InvalidArgumentError\";\nvar marker4 = `vercel.ai.error.${name3}`;\nvar symbol4 = Symbol.for(marker4);\nvar _a4;\nvar InvalidArgumentError = class extends AISDKError {\n  constructor({\n    message,\n    cause,\n    argument\n  }) {\n    super({ name: name3, message, cause });\n    this[_a4] = true;\n    this.argument = argument;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker4);\n  }\n};\n_a4 = symbol4;\nvar name4 = \"AI_InvalidPromptError\";\nvar marker5 = `vercel.ai.error.${name4}`;\nvar symbol5 = Symbol.for(marker5);\nvar _a5;\nvar InvalidPromptError = class extends AISDKError {\n  constructor({\n    prompt,\n    message,\n    cause\n  }) {\n    super({ name: name4, message: `Invalid prompt: ${message}`, cause });\n    this[_a5] = true;\n    this.prompt = prompt;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker5);\n  }\n};\n_a5 = symbol5;\nvar name5 = \"AI_InvalidResponseDataError\";\nvar marker6 = `vercel.ai.error.${name5}`;\nvar symbol6 = Symbol.for(marker6);\nvar _a6;\nvar InvalidResponseDataError = class extends AISDKError {\n  constructor({\n    data,\n    message = `Invalid response data: ${JSON.stringify(data)}.`\n  }) {\n    super({ name: name5, message });\n    this[_a6] = true;\n    this.data = data;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker6);\n  }\n};\n_a6 = symbol6;\nvar name6 = \"AI_JSONParseError\";\nvar marker7 = `vercel.ai.error.${name6}`;\nvar symbol7 = Symbol.for(marker7);\nvar _a7;\nvar JSONParseError = class extends AISDKError {\n  constructor({ text, cause }) {\n    super({\n      name: name6,\n      message: `JSON parsing failed: Text: ${text}.\nError message: ${getErrorMessage(cause)}`,\n      cause\n    });\n    this[_a7] = true;\n    this.text = text;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker7);\n  }\n};\n_a7 = symbol7;\nvar name7 = \"AI_LoadAPIKeyError\";\nvar marker8 = `vercel.ai.error.${name7}`;\nvar symbol8 = Symbol.for(marker8);\nvar _a8;\nvar LoadAPIKeyError = class extends AISDKError {\n  // used in isInstance\n  constructor({ message }) {\n    super({ name: name7, message });\n    this[_a8] = true;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker8);\n  }\n};\n_a8 = symbol8;\nvar name10 = \"AI_NoSuchModelError\";\nvar marker11 = `vercel.ai.error.${name10}`;\nvar symbol11 = Symbol.for(marker11);\nvar _a11;\nvar NoSuchModelError = class extends AISDKError {\n  constructor({\n    errorName = name10,\n    modelId,\n    modelType,\n    message = `No such ${modelType}: ${modelId}`\n  }) {\n    super({ name: errorName, message });\n    this[_a11] = true;\n    this.modelId = modelId;\n    this.modelType = modelType;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker11);\n  }\n};\n_a11 = symbol11;\nvar name11 = \"AI_TooManyEmbeddingValuesForCallError\";\nvar marker12 = `vercel.ai.error.${name11}`;\nvar symbol12 = Symbol.for(marker12);\nvar _a12;\nvar TooManyEmbeddingValuesForCallError = class extends AISDKError {\n  constructor(options) {\n    super({\n      name: name11,\n      message: `Too many values for a single embedding call. The ${options.provider} model \"${options.modelId}\" can only embed up to ${options.maxEmbeddingsPerCall} values per call, but ${options.values.length} values were provided.`\n    });\n    this[_a12] = true;\n    this.provider = options.provider;\n    this.modelId = options.modelId;\n    this.maxEmbeddingsPerCall = options.maxEmbeddingsPerCall;\n    this.values = options.values;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker12);\n  }\n};\n_a12 = symbol12;\nvar name12 = \"AI_TypeValidationError\";\nvar marker13 = `vercel.ai.error.${name12}`;\nvar symbol13 = Symbol.for(marker13);\nvar _a13;\nvar _TypeValidationError = class _TypeValidationError2 extends AISDKError {\n  constructor({ value, cause }) {\n    super({\n      name: name12,\n      message: `Type validation failed: Value: ${JSON.stringify(value)}.\nError message: ${getErrorMessage(cause)}`,\n      cause\n    });\n    this[_a13] = true;\n    this.value = value;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker13);\n  }\n  /**\n   * Wraps an error into a TypeValidationError.\n   * If the cause is already a TypeValidationError with the same value, it returns the cause.\n   * Otherwise, it creates a new TypeValidationError.\n   *\n   * @param {Object} params - The parameters for wrapping the error.\n   * @param {unknown} params.value - The value that failed validation.\n   * @param {unknown} params.cause - The original error or cause of the validation failure.\n   * @returns {TypeValidationError} A TypeValidationError instance.\n   */\n  static wrap({\n    value,\n    cause\n  }) {\n    return _TypeValidationError2.isInstance(cause) && cause.value === value ? cause : new _TypeValidationError2({ value, cause });\n  }\n};\n_a13 = symbol13;\nvar TypeValidationError = _TypeValidationError;\nvar name13 = \"AI_UnsupportedFunctionalityError\";\nvar marker14 = `vercel.ai.error.${name13}`;\nvar symbol14 = Symbol.for(marker14);\nvar _a14;\nvar UnsupportedFunctionalityError = class extends AISDKError {\n  constructor({\n    functionality,\n    message = `'${functionality}' functionality not supported.`\n  }) {\n    super({ name: name13, message });\n    this[_a14] = true;\n    this.functionality = functionality;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker14);\n  }\n};\n_a14 = symbol14;\n\n// ../../node_modules/.pnpm/nanoid@3.3.11/node_modules/nanoid/non-secure/index.js\nvar customAlphabet = (alphabet, defaultSize = 21) => {\n  return (size = defaultSize) => {\n    let id = \"\";\n    let i = size | 0;\n    while (i--) {\n      id += alphabet[Math.random() * alphabet.length | 0];\n    }\n    return id;\n  };\n};\n\n// ../../node_modules/.pnpm/@ai-sdk+provider-utils@2.2.8_zod@3.25.76/node_modules/@ai-sdk/provider-utils/dist/index.mjs\nvar import_secure_json_parse = __toESM(require_secure_json_parse(), 1);\nfunction combineHeaders(...headers) {\n  return headers.reduce(\n    (combinedHeaders, currentHeaders) => ({\n      ...combinedHeaders,\n      ...currentHeaders != null ? currentHeaders : {}\n    }),\n    {}\n  );\n}\nfunction createEventSourceParserStream() {\n  let buffer = \"\";\n  let event = void 0;\n  let data = [];\n  let lastEventId = void 0;\n  let retry = void 0;\n  function parseLine(line, controller) {\n    if (line === \"\") {\n      dispatchEvent(controller);\n      return;\n    }\n    if (line.startsWith(\":\")) {\n      return;\n    }\n    const colonIndex = line.indexOf(\":\");\n    if (colonIndex === -1) {\n      handleField(line, \"\");\n      return;\n    }\n    const field = line.slice(0, colonIndex);\n    const valueStart = colonIndex + 1;\n    const value = valueStart < line.length && line[valueStart] === \" \" ? line.slice(valueStart + 1) : line.slice(valueStart);\n    handleField(field, value);\n  }\n  function dispatchEvent(controller) {\n    if (data.length > 0) {\n      controller.enqueue({\n        event,\n        data: data.join(\"\\n\"),\n        id: lastEventId,\n        retry\n      });\n      data = [];\n      event = void 0;\n      retry = void 0;\n    }\n  }\n  function handleField(field, value) {\n    switch (field) {\n      case \"event\":\n        event = value;\n        break;\n      case \"data\":\n        data.push(value);\n        break;\n      case \"id\":\n        lastEventId = value;\n        break;\n      case \"retry\":\n        const parsedRetry = parseInt(value, 10);\n        if (!isNaN(parsedRetry)) {\n          retry = parsedRetry;\n        }\n        break;\n    }\n  }\n  return new TransformStream({\n    transform(chunk, controller) {\n      const { lines, incompleteLine } = splitLines(buffer, chunk);\n      buffer = incompleteLine;\n      for (let i = 0; i < lines.length; i++) {\n        parseLine(lines[i], controller);\n      }\n    },\n    flush(controller) {\n      parseLine(buffer, controller);\n      dispatchEvent(controller);\n    }\n  });\n}\nfunction splitLines(buffer, chunk) {\n  const lines = [];\n  let currentLine = buffer;\n  for (let i = 0; i < chunk.length; ) {\n    const char = chunk[i++];\n    if (char === \"\\n\") {\n      lines.push(currentLine);\n      currentLine = \"\";\n    } else if (char === \"\\r\") {\n      lines.push(currentLine);\n      currentLine = \"\";\n      if (chunk[i] === \"\\n\") {\n        i++;\n      }\n    } else {\n      currentLine += char;\n    }\n  }\n  return { lines, incompleteLine: currentLine };\n}\nfunction extractResponseHeaders(response) {\n  const headers = {};\n  response.headers.forEach((value, key) => {\n    headers[key] = value;\n  });\n  return headers;\n}\nvar createIdGenerator = ({\n  prefix,\n  size: defaultSize = 16,\n  alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\",\n  separator = \"-\"\n} = {}) => {\n  const generator = customAlphabet(alphabet, defaultSize);\n  if (prefix == null) {\n    return generator;\n  }\n  if (alphabet.includes(separator)) {\n    throw new InvalidArgumentError({\n      argument: \"separator\",\n      message: `The separator \"${separator}\" must not be part of the alphabet \"${alphabet}\".`\n    });\n  }\n  return (size) => `${prefix}${separator}${generator(size)}`;\n};\nvar generateId = createIdGenerator();\nfunction removeUndefinedEntries(record) {\n  return Object.fromEntries(\n    Object.entries(record).filter(([_key, value]) => value != null)\n  );\n}\nfunction isAbortError(error) {\n  return error instanceof Error && (error.name === \"AbortError\" || error.name === \"TimeoutError\");\n}\nfunction loadApiKey({\n  apiKey,\n  environmentVariableName,\n  apiKeyParameterName = \"apiKey\",\n  description\n}) {\n  if (typeof apiKey === \"string\") {\n    return apiKey;\n  }\n  if (apiKey != null) {\n    throw new LoadAPIKeyError({\n      message: `${description} API key must be a string.`\n    });\n  }\n  if (typeof process === \"undefined\") {\n    throw new LoadAPIKeyError({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter. Environment variables is not supported in this environment.`\n    });\n  }\n  apiKey = process.env[environmentVariableName];\n  if (apiKey == null) {\n    throw new LoadAPIKeyError({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter or the ${environmentVariableName} environment variable.`\n    });\n  }\n  if (typeof apiKey !== \"string\") {\n    throw new LoadAPIKeyError({\n      message: `${description} API key must be a string. The value of the ${environmentVariableName} environment variable is not a string.`\n    });\n  }\n  return apiKey;\n}\nvar validatorSymbol = Symbol.for(\"vercel.ai.validator\");\nfunction validator(validate) {\n  return { [validatorSymbol]: true, validate };\n}\nfunction isValidator(value) {\n  return typeof value === \"object\" && value !== null && validatorSymbol in value && value[validatorSymbol] === true && \"validate\" in value;\n}\nfunction asValidator(value) {\n  return isValidator(value) ? value : zodValidator(value);\n}\nfunction zodValidator(zodSchema) {\n  return validator((value) => {\n    const result = zodSchema.safeParse(value);\n    return result.success ? { success: true, value: result.data } : { success: false, error: result.error };\n  });\n}\nfunction validateTypes({\n  value,\n  schema: inputSchema\n}) {\n  const result = safeValidateTypes({ value, schema: inputSchema });\n  if (!result.success) {\n    throw TypeValidationError.wrap({ value, cause: result.error });\n  }\n  return result.value;\n}\nfunction safeValidateTypes({\n  value,\n  schema\n}) {\n  const validator22 = asValidator(schema);\n  try {\n    if (validator22.validate == null) {\n      return { success: true, value };\n    }\n    const result = validator22.validate(value);\n    if (result.success) {\n      return result;\n    }\n    return {\n      success: false,\n      error: TypeValidationError.wrap({ value, cause: result.error })\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error: TypeValidationError.wrap({ value, cause: error })\n    };\n  }\n}\nfunction parseJSON({\n  text,\n  schema\n}) {\n  try {\n    const value = import_secure_json_parse.default.parse(text);\n    if (schema == null) {\n      return value;\n    }\n    return validateTypes({ value, schema });\n  } catch (error) {\n    if (JSONParseError.isInstance(error) || TypeValidationError.isInstance(error)) {\n      throw error;\n    }\n    throw new JSONParseError({ text, cause: error });\n  }\n}\nfunction safeParseJSON({\n  text,\n  schema\n}) {\n  try {\n    const value = import_secure_json_parse.default.parse(text);\n    if (schema == null) {\n      return { success: true, value, rawValue: value };\n    }\n    const validationResult = safeValidateTypes({ value, schema });\n    return validationResult.success ? { ...validationResult, rawValue: value } : validationResult;\n  } catch (error) {\n    return {\n      success: false,\n      error: JSONParseError.isInstance(error) ? error : new JSONParseError({ text, cause: error })\n    };\n  }\n}\nfunction isParsableJson(input) {\n  try {\n    import_secure_json_parse.default.parse(input);\n    return true;\n  } catch (e) {\n    return false;\n  }\n}\nfunction parseProviderOptions({\n  provider,\n  providerOptions,\n  schema\n}) {\n  if ((providerOptions == null ? void 0 : providerOptions[provider]) == null) {\n    return void 0;\n  }\n  const parsedProviderOptions = safeValidateTypes({\n    value: providerOptions[provider],\n    schema\n  });\n  if (!parsedProviderOptions.success) {\n    throw new InvalidArgumentError({\n      argument: \"providerOptions\",\n      message: `invalid ${provider} provider options`,\n      cause: parsedProviderOptions.error\n    });\n  }\n  return parsedProviderOptions.value;\n}\nvar getOriginalFetch2 = () => globalThis.fetch;\nvar postJsonToApi = async ({\n  url,\n  headers,\n  body,\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n}) => postToApi({\n  url,\n  headers: {\n    \"Content-Type\": \"application/json\",\n    ...headers\n  },\n  body: {\n    content: JSON.stringify(body),\n    values: body\n  },\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n});\nvar postFormDataToApi = async ({\n  url,\n  headers,\n  formData,\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n}) => postToApi({\n  url,\n  headers,\n  body: {\n    content: formData,\n    values: Object.fromEntries(formData.entries())\n  },\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n});\nvar postToApi = async ({\n  url,\n  headers = {},\n  body,\n  successfulResponseHandler,\n  failedResponseHandler,\n  abortSignal,\n  fetch = getOriginalFetch2()\n}) => {\n  try {\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers: removeUndefinedEntries(headers),\n      body: body.content,\n      signal: abortSignal\n    });\n    const responseHeaders = extractResponseHeaders(response);\n    if (!response.ok) {\n      let errorInformation;\n      try {\n        errorInformation = await failedResponseHandler({\n          response,\n          url,\n          requestBodyValues: body.values\n        });\n      } catch (error) {\n        if (isAbortError(error) || APICallError.isInstance(error)) {\n          throw error;\n        }\n        throw new APICallError({\n          message: \"Failed to process error response\",\n          cause: error,\n          statusCode: response.status,\n          url,\n          responseHeaders,\n          requestBodyValues: body.values\n        });\n      }\n      throw errorInformation.value;\n    }\n    try {\n      return await successfulResponseHandler({\n        response,\n        url,\n        requestBodyValues: body.values\n      });\n    } catch (error) {\n      if (error instanceof Error) {\n        if (isAbortError(error) || APICallError.isInstance(error)) {\n          throw error;\n        }\n      }\n      throw new APICallError({\n        message: \"Failed to process successful response\",\n        cause: error,\n        statusCode: response.status,\n        url,\n        responseHeaders,\n        requestBodyValues: body.values\n      });\n    }\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error;\n    }\n    if (error instanceof TypeError && error.message === \"fetch failed\") {\n      const cause = error.cause;\n      if (cause != null) {\n        throw new APICallError({\n          message: `Cannot connect to API: ${cause.message}`,\n          cause,\n          url,\n          requestBodyValues: body.values,\n          isRetryable: true\n          // retry when network error\n        });\n      }\n    }\n    throw error;\n  }\n};\nasync function resolve(value) {\n  if (typeof value === \"function\") {\n    value = value();\n  }\n  return Promise.resolve(value);\n}\nvar createJsonErrorResponseHandler = ({\n  errorSchema,\n  errorToMessage,\n  isRetryable\n}) => async ({ response, url, requestBodyValues }) => {\n  const responseBody = await response.text();\n  const responseHeaders = extractResponseHeaders(response);\n  if (responseBody.trim() === \"\") {\n    return {\n      responseHeaders,\n      value: new APICallError({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response)\n      })\n    };\n  }\n  try {\n    const parsedError = parseJSON({\n      text: responseBody,\n      schema: errorSchema\n    });\n    return {\n      responseHeaders,\n      value: new APICallError({\n        message: errorToMessage(parsedError),\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        data: parsedError,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response, parsedError)\n      })\n    };\n  } catch (parseError) {\n    return {\n      responseHeaders,\n      value: new APICallError({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response)\n      })\n    };\n  }\n};\nvar createEventSourceResponseHandler = (chunkSchema3) => async ({ response }) => {\n  const responseHeaders = extractResponseHeaders(response);\n  if (response.body == null) {\n    throw new EmptyResponseBodyError({});\n  }\n  return {\n    responseHeaders,\n    value: response.body.pipeThrough(new TextDecoderStream()).pipeThrough(createEventSourceParserStream()).pipeThrough(\n      new TransformStream({\n        transform({ data }, controller) {\n          if (data === \"[DONE]\") {\n            return;\n          }\n          controller.enqueue(\n            safeParseJSON({\n              text: data,\n              schema: chunkSchema3\n            })\n          );\n        }\n      })\n    )\n  };\n};\nvar createJsonResponseHandler = (responseSchema3) => async ({ response, url, requestBodyValues }) => {\n  const responseBody = await response.text();\n  const parsedResult = safeParseJSON({\n    text: responseBody,\n    schema: responseSchema3\n  });\n  const responseHeaders = extractResponseHeaders(response);\n  if (!parsedResult.success) {\n    throw new APICallError({\n      message: \"Invalid JSON response\",\n      cause: parsedResult.error,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody,\n      url,\n      requestBodyValues\n    });\n  }\n  return {\n    responseHeaders,\n    value: parsedResult.value,\n    rawValue: parsedResult.rawValue\n  };\n};\nvar createBinaryResponseHandler = () => async ({ response, url, requestBodyValues }) => {\n  const responseHeaders = extractResponseHeaders(response);\n  if (!response.body) {\n    throw new APICallError({\n      message: \"Response body is empty\",\n      url,\n      requestBodyValues,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody: void 0\n    });\n  }\n  try {\n    const buffer = await response.arrayBuffer();\n    return {\n      responseHeaders,\n      value: new Uint8Array(buffer)\n    };\n  } catch (error) {\n    throw new APICallError({\n      message: \"Failed to read response as array buffer\",\n      url,\n      requestBodyValues,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody: void 0,\n      cause: error\n    });\n  }\n};\nvar { btoa, atob } = globalThis;\nfunction convertBase64ToUint8Array(base64String) {\n  const base64Url = base64String.replace(/-/g, \"+\").replace(/_/g, \"/\");\n  const latin1string = atob(base64Url);\n  return Uint8Array.from(latin1string, (byte) => byte.codePointAt(0));\n}\nfunction convertUint8ArrayToBase64(array) {\n  let latin1string = \"\";\n  for (let i = 0; i < array.length; i++) {\n    latin1string += String.fromCodePoint(array[i]);\n  }\n  return btoa(latin1string);\n}\nfunction withoutTrailingSlash(url) {\n  return url == null ? void 0 : url.replace(/\\/$/, \"\");\n}\nvar anthropicErrorDataSchema = z.object({\n  type: z.literal(\"error\"),\n  error: z.object({\n    type: z.string(),\n    message: z.string()\n  })\n});\nvar anthropicFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: anthropicErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\nfunction prepareTools(mode) {\n  var _a16;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  const betas = /* @__PURE__ */ new Set();\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings, betas };\n  }\n  const anthropicTools22 = [];\n  for (const tool2 of tools) {\n    switch (tool2.type) {\n      case \"function\":\n        anthropicTools22.push({\n          name: tool2.name,\n          description: tool2.description,\n          input_schema: tool2.parameters\n        });\n        break;\n      case \"provider-defined\":\n        switch (tool2.id) {\n          case \"anthropic.computer_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"computer_20250124\",\n              display_width_px: tool2.args.displayWidthPx,\n              display_height_px: tool2.args.displayHeightPx,\n              display_number: tool2.args.displayNumber\n            });\n            break;\n          case \"anthropic.computer_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"computer_20241022\",\n              display_width_px: tool2.args.displayWidthPx,\n              display_height_px: tool2.args.displayHeightPx,\n              display_number: tool2.args.displayNumber\n            });\n            break;\n          case \"anthropic.text_editor_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"text_editor_20250124\"\n            });\n            break;\n          case \"anthropic.text_editor_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"text_editor_20241022\"\n            });\n            break;\n          case \"anthropic.bash_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"bash_20250124\"\n            });\n            break;\n          case \"anthropic.bash_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: tool2.name,\n              type: \"bash_20241022\"\n            });\n            break;\n          default:\n            toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  const toolChoice = mode.toolChoice;\n  if (toolChoice == null) {\n    return {\n      tools: anthropicTools22,\n      tool_choice: void 0,\n      toolWarnings,\n      betas\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n      return {\n        tools: anthropicTools22,\n        tool_choice: { type: \"auto\" },\n        toolWarnings,\n        betas\n      };\n    case \"required\":\n      return {\n        tools: anthropicTools22,\n        tool_choice: { type: \"any\" },\n        toolWarnings,\n        betas\n      };\n    case \"none\":\n      return { tools: void 0, tool_choice: void 0, toolWarnings, betas };\n    case \"tool\":\n      return {\n        tools: anthropicTools22,\n        tool_choice: { type: \"tool\", name: toolChoice.toolName },\n        toolWarnings,\n        betas\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nfunction convertToAnthropicMessagesPrompt({\n  prompt,\n  sendReasoning,\n  warnings\n}) {\n  var _a16, _b, _c, _d;\n  const betas = /* @__PURE__ */ new Set();\n  const blocks = groupIntoBlocks(prompt);\n  let system = void 0;\n  const messages = [];\n  function getCacheControl2(providerMetadata) {\n    var _a23;\n    const anthropic22 = providerMetadata == null ? void 0 : providerMetadata.anthropic;\n    const cacheControlValue = (_a23 = anthropic22 == null ? void 0 : anthropic22.cacheControl) != null ? _a23 : anthropic22 == null ? void 0 : anthropic22.cache_control;\n    return cacheControlValue;\n  }\n  for (let i = 0; i < blocks.length; i++) {\n    const block = blocks[i];\n    const isLastBlock = i === blocks.length - 1;\n    const type = block.type;\n    switch (type) {\n      case \"system\": {\n        if (system != null) {\n          throw new UnsupportedFunctionalityError({\n            functionality: \"Multiple system messages that are separated by user/assistant messages\"\n          });\n        }\n        system = block.messages.map(({ content, providerMetadata }) => ({\n          type: \"text\",\n          text: content,\n          cache_control: getCacheControl2(providerMetadata)\n        }));\n        break;\n      }\n      case \"user\": {\n        const anthropicContent = [];\n        for (const message of block.messages) {\n          const { role, content } = message;\n          switch (role) {\n            case \"user\": {\n              for (let j = 0; j < content.length; j++) {\n                const part = content[j];\n                const isLastPart = j === content.length - 1;\n                const cacheControl = (_a16 = getCacheControl2(part.providerMetadata)) != null ? _a16 : isLastPart ? getCacheControl2(message.providerMetadata) : void 0;\n                switch (part.type) {\n                  case \"text\": {\n                    anthropicContent.push({\n                      type: \"text\",\n                      text: part.text,\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                  case \"image\": {\n                    anthropicContent.push({\n                      type: \"image\",\n                      source: part.image instanceof URL ? {\n                        type: \"url\",\n                        url: part.image.toString()\n                      } : {\n                        type: \"base64\",\n                        media_type: (_b = part.mimeType) != null ? _b : \"image/jpeg\",\n                        data: convertUint8ArrayToBase64(part.image)\n                      },\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                  case \"file\": {\n                    if (part.mimeType !== \"application/pdf\") {\n                      throw new UnsupportedFunctionalityError({\n                        functionality: \"Non-PDF files in user messages\"\n                      });\n                    }\n                    betas.add(\"pdfs-2024-09-25\");\n                    anthropicContent.push({\n                      type: \"document\",\n                      source: part.data instanceof URL ? {\n                        type: \"url\",\n                        url: part.data.toString()\n                      } : {\n                        type: \"base64\",\n                        media_type: \"application/pdf\",\n                        data: part.data\n                      },\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                }\n              }\n              break;\n            }\n            case \"tool\": {\n              for (let i2 = 0; i2 < content.length; i2++) {\n                const part = content[i2];\n                const isLastPart = i2 === content.length - 1;\n                const cacheControl = (_c = getCacheControl2(part.providerMetadata)) != null ? _c : isLastPart ? getCacheControl2(message.providerMetadata) : void 0;\n                const toolResultContent = part.content != null ? part.content.map((part2) => {\n                  var _a23;\n                  switch (part2.type) {\n                    case \"text\":\n                      return {\n                        type: \"text\",\n                        text: part2.text,\n                        cache_control: void 0\n                      };\n                    case \"image\":\n                      return {\n                        type: \"image\",\n                        source: {\n                          type: \"base64\",\n                          media_type: (_a23 = part2.mimeType) != null ? _a23 : \"image/jpeg\",\n                          data: part2.data\n                        },\n                        cache_control: void 0\n                      };\n                  }\n                }) : JSON.stringify(part.result);\n                anthropicContent.push({\n                  type: \"tool_result\",\n                  tool_use_id: part.toolCallId,\n                  content: toolResultContent,\n                  is_error: part.isError,\n                  cache_control: cacheControl\n                });\n              }\n              break;\n            }\n            default: {\n              const _exhaustiveCheck = role;\n              throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n            }\n          }\n        }\n        messages.push({ role: \"user\", content: anthropicContent });\n        break;\n      }\n      case \"assistant\": {\n        const anthropicContent = [];\n        for (let j = 0; j < block.messages.length; j++) {\n          const message = block.messages[j];\n          const isLastMessage = j === block.messages.length - 1;\n          const { content } = message;\n          for (let k = 0; k < content.length; k++) {\n            const part = content[k];\n            const isLastContentPart = k === content.length - 1;\n            const cacheControl = (_d = getCacheControl2(part.providerMetadata)) != null ? _d : isLastContentPart ? getCacheControl2(message.providerMetadata) : void 0;\n            switch (part.type) {\n              case \"text\": {\n                anthropicContent.push({\n                  type: \"text\",\n                  text: (\n                    // trim the last text part if it's the last message in the block\n                    // because Anthropic does not allow trailing whitespace\n                    // in pre-filled assistant responses\n                    isLastBlock && isLastMessage && isLastContentPart ? part.text.trim() : part.text\n                  ),\n                  cache_control: cacheControl\n                });\n                break;\n              }\n              case \"reasoning\": {\n                if (sendReasoning) {\n                  anthropicContent.push({\n                    type: \"thinking\",\n                    thinking: part.text,\n                    signature: part.signature,\n                    cache_control: cacheControl\n                  });\n                } else {\n                  warnings.push({\n                    type: \"other\",\n                    message: \"sending reasoning content is disabled for this model\"\n                  });\n                }\n                break;\n              }\n              case \"redacted-reasoning\": {\n                anthropicContent.push({\n                  type: \"redacted_thinking\",\n                  data: part.data,\n                  cache_control: cacheControl\n                });\n                break;\n              }\n              case \"tool-call\": {\n                anthropicContent.push({\n                  type: \"tool_use\",\n                  id: part.toolCallId,\n                  name: part.toolName,\n                  input: part.args,\n                  cache_control: cacheControl\n                });\n                break;\n              }\n            }\n          }\n        }\n        messages.push({ role: \"assistant\", content: anthropicContent });\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return {\n    prompt: { system, messages },\n    betas\n  };\n}\nfunction groupIntoBlocks(prompt) {\n  const blocks = [];\n  let currentBlock = void 0;\n  for (const message of prompt) {\n    const { role } = message;\n    switch (role) {\n      case \"system\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"system\") {\n          currentBlock = { type: \"system\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"assistant\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"assistant\") {\n          currentBlock = { type: \"assistant\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"user\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"user\") {\n          currentBlock = { type: \"user\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"tool\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"user\") {\n          currentBlock = { type: \"user\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return blocks;\n}\nfunction mapAnthropicStopReason(finishReason) {\n  switch (finishReason) {\n    case \"end_turn\":\n    case \"stop_sequence\":\n      return \"stop\";\n    case \"tool_use\":\n      return \"tool-calls\";\n    case \"max_tokens\":\n      return \"length\";\n    default:\n      return \"unknown\";\n  }\n}\nvar AnthropicMessagesLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = \"tool\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  supportsUrl(url) {\n    return url.protocol === \"https:\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportsImageUrls() {\n    return this.config.supportsImageUrls;\n  }\n  async getArgs({\n    mode,\n    prompt,\n    maxTokens = 4096,\n    // 4096: max model output tokens TODO update default in v5\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    providerMetadata: providerOptions\n  }) {\n    var _a16, _b, _c;\n    const type = mode.type;\n    const warnings = [];\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const { prompt: messagesPrompt, betas: messagesBetas } = convertToAnthropicMessagesPrompt({\n      prompt,\n      sendReasoning: (_a16 = this.settings.sendReasoning) != null ? _a16 : true,\n      warnings\n    });\n    const anthropicOptions = parseProviderOptions({\n      provider: \"anthropic\",\n      providerOptions,\n      schema: anthropicProviderOptionsSchema\n    });\n    const isThinking = ((_b = anthropicOptions == null ? void 0 : anthropicOptions.thinking) == null ? void 0 : _b.type) === \"enabled\";\n    const thinkingBudget = (_c = anthropicOptions == null ? void 0 : anthropicOptions.thinking) == null ? void 0 : _c.budgetTokens;\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_k: topK,\n      top_p: topP,\n      stop_sequences: stopSequences,\n      // provider specific settings:\n      ...isThinking && {\n        thinking: { type: \"enabled\", budget_tokens: thinkingBudget }\n      },\n      // prompt:\n      system: messagesPrompt.system,\n      messages: messagesPrompt.messages\n    };\n    if (isThinking) {\n      if (thinkingBudget == null) {\n        throw new UnsupportedFunctionalityError({\n          functionality: \"thinking requires a budget\"\n        });\n      }\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported when thinking is enabled\"\n        });\n      }\n      if (topK != null) {\n        baseArgs.top_k = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topK\",\n          details: \"topK is not supported when thinking is enabled\"\n        });\n      }\n      if (topP != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported when thinking is enabled\"\n        });\n      }\n      baseArgs.max_tokens = maxTokens + thinkingBudget;\n    }\n    switch (type) {\n      case \"regular\": {\n        const {\n          tools,\n          tool_choice,\n          toolWarnings,\n          betas: toolsBetas\n        } = prepareTools(mode);\n        return {\n          args: { ...baseArgs, tools, tool_choice },\n          warnings: [...warnings, ...toolWarnings],\n          betas: /* @__PURE__ */ new Set([...messagesBetas, ...toolsBetas])\n        };\n      }\n      case \"object-json\": {\n        throw new UnsupportedFunctionalityError({\n          functionality: \"json-mode object generation\"\n        });\n      }\n      case \"object-tool\": {\n        const { name: name15, description, parameters } = mode.tool;\n        return {\n          args: {\n            ...baseArgs,\n            tools: [{ name: name15, description, input_schema: parameters }],\n            tool_choice: { type: \"tool\", name: name15 }\n          },\n          warnings,\n          betas: messagesBetas\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async getHeaders({\n    betas,\n    headers\n  }) {\n    return combineHeaders(\n      await resolve(this.config.headers),\n      betas.size > 0 ? { \"anthropic-beta\": Array.from(betas).join(\",\") } : {},\n      headers\n    );\n  }\n  buildRequestUrl(isStreaming) {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).buildRequestUrl) == null ? void 0 : _b.call(_a16, this.config.baseURL, isStreaming)) != null ? _c : `${this.config.baseURL}/messages`;\n  }\n  transformRequestBody(args) {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).transformRequestBody) == null ? void 0 : _b.call(_a16, args)) != null ? _c : args;\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d;\n    const { args, warnings, betas } = await this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.buildRequestUrl(false),\n      headers: await this.getHeaders({ betas, headers: options.headers }),\n      body: this.transformRequestBody(args),\n      failedResponseHandler: anthropicFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        anthropicMessagesResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    let text = \"\";\n    for (const content of response.content) {\n      if (content.type === \"text\") {\n        text += content.text;\n      }\n    }\n    let toolCalls = void 0;\n    if (response.content.some((content) => content.type === \"tool_use\")) {\n      toolCalls = [];\n      for (const content of response.content) {\n        if (content.type === \"tool_use\") {\n          toolCalls.push({\n            toolCallType: \"function\",\n            toolCallId: content.id,\n            toolName: content.name,\n            args: JSON.stringify(content.input)\n          });\n        }\n      }\n    }\n    const reasoning = response.content.filter(\n      (content) => content.type === \"redacted_thinking\" || content.type === \"thinking\"\n    ).map(\n      (content) => content.type === \"thinking\" ? {\n        type: \"text\",\n        text: content.thinking,\n        signature: content.signature\n      } : {\n        type: \"redacted\",\n        data: content.data\n      }\n    );\n    return {\n      text,\n      reasoning: reasoning.length > 0 ? reasoning : void 0,\n      toolCalls,\n      finishReason: mapAnthropicStopReason(response.stop_reason),\n      usage: {\n        promptTokens: response.usage.input_tokens,\n        completionTokens: response.usage.output_tokens\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      response: {\n        id: (_a16 = response.id) != null ? _a16 : void 0,\n        modelId: (_b = response.model) != null ? _b : void 0\n      },\n      warnings,\n      providerMetadata: {\n        anthropic: {\n          cacheCreationInputTokens: (_c = response.usage.cache_creation_input_tokens) != null ? _c : null,\n          cacheReadInputTokens: (_d = response.usage.cache_read_input_tokens) != null ? _d : null\n        }\n      },\n      request: { body: JSON.stringify(args) }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings, betas } = await this.getArgs(options);\n    const body = { ...args, stream: true };\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.buildRequestUrl(true),\n      headers: await this.getHeaders({ betas, headers: options.headers }),\n      body: this.transformRequestBody(body),\n      failedResponseHandler: anthropicFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        anthropicMessagesChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    let finishReason = \"unknown\";\n    const usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    const toolCallContentBlocks = {};\n    let providerMetadata = void 0;\n    let blockType = void 0;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d;\n            if (!chunk.success) {\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            switch (value.type) {\n              case \"ping\": {\n                return;\n              }\n              case \"content_block_start\": {\n                const contentBlockType = value.content_block.type;\n                blockType = contentBlockType;\n                switch (contentBlockType) {\n                  case \"text\":\n                  case \"thinking\": {\n                    return;\n                  }\n                  case \"redacted_thinking\": {\n                    controller.enqueue({\n                      type: \"redacted-reasoning\",\n                      data: value.content_block.data\n                    });\n                    return;\n                  }\n                  case \"tool_use\": {\n                    toolCallContentBlocks[value.index] = {\n                      toolCallId: value.content_block.id,\n                      toolName: value.content_block.name,\n                      jsonText: \"\"\n                    };\n                    return;\n                  }\n                  default: {\n                    const _exhaustiveCheck = contentBlockType;\n                    throw new Error(\n                      `Unsupported content block type: ${_exhaustiveCheck}`\n                    );\n                  }\n                }\n              }\n              case \"content_block_stop\": {\n                if (toolCallContentBlocks[value.index] != null) {\n                  const contentBlock = toolCallContentBlocks[value.index];\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: contentBlock.toolCallId,\n                    toolName: contentBlock.toolName,\n                    args: contentBlock.jsonText\n                  });\n                  delete toolCallContentBlocks[value.index];\n                }\n                blockType = void 0;\n                return;\n              }\n              case \"content_block_delta\": {\n                const deltaType = value.delta.type;\n                switch (deltaType) {\n                  case \"text_delta\": {\n                    controller.enqueue({\n                      type: \"text-delta\",\n                      textDelta: value.delta.text\n                    });\n                    return;\n                  }\n                  case \"thinking_delta\": {\n                    controller.enqueue({\n                      type: \"reasoning\",\n                      textDelta: value.delta.thinking\n                    });\n                    return;\n                  }\n                  case \"signature_delta\": {\n                    if (blockType === \"thinking\") {\n                      controller.enqueue({\n                        type: \"reasoning-signature\",\n                        signature: value.delta.signature\n                      });\n                    }\n                    return;\n                  }\n                  case \"input_json_delta\": {\n                    const contentBlock = toolCallContentBlocks[value.index];\n                    controller.enqueue({\n                      type: \"tool-call-delta\",\n                      toolCallType: \"function\",\n                      toolCallId: contentBlock.toolCallId,\n                      toolName: contentBlock.toolName,\n                      argsTextDelta: value.delta.partial_json\n                    });\n                    contentBlock.jsonText += value.delta.partial_json;\n                    return;\n                  }\n                  default: {\n                    const _exhaustiveCheck = deltaType;\n                    throw new Error(\n                      `Unsupported delta type: ${_exhaustiveCheck}`\n                    );\n                  }\n                }\n              }\n              case \"message_start\": {\n                usage.promptTokens = value.message.usage.input_tokens;\n                usage.completionTokens = value.message.usage.output_tokens;\n                providerMetadata = {\n                  anthropic: {\n                    cacheCreationInputTokens: (_a16 = value.message.usage.cache_creation_input_tokens) != null ? _a16 : null,\n                    cacheReadInputTokens: (_b = value.message.usage.cache_read_input_tokens) != null ? _b : null\n                  }\n                };\n                controller.enqueue({\n                  type: \"response-metadata\",\n                  id: (_c = value.message.id) != null ? _c : void 0,\n                  modelId: (_d = value.message.model) != null ? _d : void 0\n                });\n                return;\n              }\n              case \"message_delta\": {\n                usage.completionTokens = value.usage.output_tokens;\n                finishReason = mapAnthropicStopReason(value.delta.stop_reason);\n                return;\n              }\n              case \"message_stop\": {\n                controller.enqueue({\n                  type: \"finish\",\n                  finishReason,\n                  usage,\n                  providerMetadata\n                });\n                return;\n              }\n              case \"error\": {\n                controller.enqueue({ type: \"error\", error: value.error });\n                return;\n              }\n              default: {\n                const _exhaustiveCheck = value;\n                throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n              }\n            }\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) }\n    };\n  }\n};\nvar anthropicMessagesResponseSchema = z.object({\n  type: z.literal(\"message\"),\n  id: z.string().nullish(),\n  model: z.string().nullish(),\n  content: z.array(\n    z.discriminatedUnion(\"type\", [\n      z.object({\n        type: z.literal(\"text\"),\n        text: z.string()\n      }),\n      z.object({\n        type: z.literal(\"thinking\"),\n        thinking: z.string(),\n        signature: z.string()\n      }),\n      z.object({\n        type: z.literal(\"redacted_thinking\"),\n        data: z.string()\n      }),\n      z.object({\n        type: z.literal(\"tool_use\"),\n        id: z.string(),\n        name: z.string(),\n        input: z.unknown()\n      })\n    ])\n  ),\n  stop_reason: z.string().nullish(),\n  usage: z.object({\n    input_tokens: z.number(),\n    output_tokens: z.number(),\n    cache_creation_input_tokens: z.number().nullish(),\n    cache_read_input_tokens: z.number().nullish()\n  })\n});\nvar anthropicMessagesChunkSchema = z.discriminatedUnion(\"type\", [\n  z.object({\n    type: z.literal(\"message_start\"),\n    message: z.object({\n      id: z.string().nullish(),\n      model: z.string().nullish(),\n      usage: z.object({\n        input_tokens: z.number(),\n        output_tokens: z.number(),\n        cache_creation_input_tokens: z.number().nullish(),\n        cache_read_input_tokens: z.number().nullish()\n      })\n    })\n  }),\n  z.object({\n    type: z.literal(\"content_block_start\"),\n    index: z.number(),\n    content_block: z.discriminatedUnion(\"type\", [\n      z.object({\n        type: z.literal(\"text\"),\n        text: z.string()\n      }),\n      z.object({\n        type: z.literal(\"thinking\"),\n        thinking: z.string()\n      }),\n      z.object({\n        type: z.literal(\"tool_use\"),\n        id: z.string(),\n        name: z.string()\n      }),\n      z.object({\n        type: z.literal(\"redacted_thinking\"),\n        data: z.string()\n      })\n    ])\n  }),\n  z.object({\n    type: z.literal(\"content_block_delta\"),\n    index: z.number(),\n    delta: z.discriminatedUnion(\"type\", [\n      z.object({\n        type: z.literal(\"input_json_delta\"),\n        partial_json: z.string()\n      }),\n      z.object({\n        type: z.literal(\"text_delta\"),\n        text: z.string()\n      }),\n      z.object({\n        type: z.literal(\"thinking_delta\"),\n        thinking: z.string()\n      }),\n      z.object({\n        type: z.literal(\"signature_delta\"),\n        signature: z.string()\n      })\n    ])\n  }),\n  z.object({\n    type: z.literal(\"content_block_stop\"),\n    index: z.number()\n  }),\n  z.object({\n    type: z.literal(\"error\"),\n    error: z.object({\n      type: z.string(),\n      message: z.string()\n    })\n  }),\n  z.object({\n    type: z.literal(\"message_delta\"),\n    delta: z.object({ stop_reason: z.string().nullish() }),\n    usage: z.object({ output_tokens: z.number() })\n  }),\n  z.object({\n    type: z.literal(\"message_stop\")\n  }),\n  z.object({\n    type: z.literal(\"ping\")\n  })\n]);\nvar anthropicProviderOptionsSchema = z.object({\n  thinking: z.object({\n    type: z.union([z.literal(\"enabled\"), z.literal(\"disabled\")]),\n    budgetTokens: z.number().optional()\n  }).optional()\n});\nvar Bash20241022Parameters = z.object({\n  command: z.string(),\n  restart: z.boolean().optional()\n});\nfunction bashTool_20241022(options = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.bash_20241022\",\n    args: {},\n    parameters: Bash20241022Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar Bash20250124Parameters = z.object({\n  command: z.string(),\n  restart: z.boolean().optional()\n});\nfunction bashTool_20250124(options = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.bash_20250124\",\n    args: {},\n    parameters: Bash20250124Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar TextEditor20241022Parameters = z.object({\n  command: z.enum([\"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"]),\n  path: z.string(),\n  file_text: z.string().optional(),\n  insert_line: z.number().int().optional(),\n  new_str: z.string().optional(),\n  old_str: z.string().optional(),\n  view_range: z.array(z.number().int()).optional()\n});\nfunction textEditorTool_20241022(options = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.text_editor_20241022\",\n    args: {},\n    parameters: TextEditor20241022Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar TextEditor20250124Parameters = z.object({\n  command: z.enum([\"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"]),\n  path: z.string(),\n  file_text: z.string().optional(),\n  insert_line: z.number().int().optional(),\n  new_str: z.string().optional(),\n  old_str: z.string().optional(),\n  view_range: z.array(z.number().int()).optional()\n});\nfunction textEditorTool_20250124(options = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.text_editor_20250124\",\n    args: {},\n    parameters: TextEditor20250124Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar Computer20241022Parameters = z.object({\n  action: z.enum([\n    \"key\",\n    \"type\",\n    \"mouse_move\",\n    \"left_click\",\n    \"left_click_drag\",\n    \"right_click\",\n    \"middle_click\",\n    \"double_click\",\n    \"screenshot\",\n    \"cursor_position\"\n  ]),\n  coordinate: z.array(z.number().int()).optional(),\n  text: z.string().optional()\n});\nfunction computerTool_20241022(options) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.computer_20241022\",\n    args: {\n      displayWidthPx: options.displayWidthPx,\n      displayHeightPx: options.displayHeightPx,\n      displayNumber: options.displayNumber\n    },\n    parameters: Computer20241022Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar Computer20250124Parameters = z.object({\n  action: z.enum([\n    \"key\",\n    \"hold_key\",\n    \"type\",\n    \"cursor_position\",\n    \"mouse_move\",\n    \"left_mouse_down\",\n    \"left_mouse_up\",\n    \"left_click\",\n    \"left_click_drag\",\n    \"right_click\",\n    \"middle_click\",\n    \"double_click\",\n    \"triple_click\",\n    \"scroll\",\n    \"wait\",\n    \"screenshot\"\n  ]),\n  coordinate: z.tuple([z.number().int(), z.number().int()]).optional(),\n  duration: z.number().optional(),\n  scroll_amount: z.number().optional(),\n  scroll_direction: z.enum([\"up\", \"down\", \"left\", \"right\"]).optional(),\n  start_coordinate: z.tuple([z.number().int(), z.number().int()]).optional(),\n  text: z.string().optional()\n});\nfunction computerTool_20250124(options) {\n  return {\n    type: \"provider-defined\",\n    id: \"anthropic.computer_20250124\",\n    args: {\n      displayWidthPx: options.displayWidthPx,\n      displayHeightPx: options.displayHeightPx,\n      displayNumber: options.displayNumber\n    },\n    parameters: Computer20250124Parameters,\n    execute: options.execute,\n    experimental_toToolResultContent: options.experimental_toToolResultContent\n  };\n}\nvar anthropicTools = {\n  bash_20241022: bashTool_20241022,\n  bash_20250124: bashTool_20250124,\n  textEditor_20241022: textEditorTool_20241022,\n  textEditor_20250124: textEditorTool_20250124,\n  computer_20241022: computerTool_20241022,\n  computer_20250124: computerTool_20250124\n};\nfunction createAnthropic(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash(options.baseURL)) != null ? _a16 : \"https://api.anthropic.com/v1\";\n  const getHeaders = () => ({\n    \"anthropic-version\": \"2023-06-01\",\n    \"x-api-key\": loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"ANTHROPIC_API_KEY\",\n      description: \"Anthropic\"\n    }),\n    ...options.headers\n  });\n  const createChatModel = (modelId, settings = {}) => new AnthropicMessagesLanguageModel(modelId, settings, {\n    provider: \"anthropic.messages\",\n    baseURL,\n    headers: getHeaders,\n    fetch: options.fetch,\n    supportsImageUrls: true\n  });\n  const provider = function(modelId, settings) {\n    if (new.target) {\n      throw new Error(\n        \"The Anthropic model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId, settings);\n  };\n  provider.languageModel = createChatModel;\n  provider.chat = createChatModel;\n  provider.messages = createChatModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.tools = anthropicTools;\n  return provider;\n}\nvar anthropic = createAnthropic();\n\n// ../../node_modules/.pnpm/@ai-sdk+provider@2.0.0/node_modules/@ai-sdk/provider/dist/index.mjs\nvar marker15 = \"vercel.ai.error\";\nvar symbol15 = Symbol.for(marker15);\nvar _a15;\nvar _AISDKError3 = class _AISDKError4 extends Error {\n  /**\n   * Creates an AI SDK Error.\n   *\n   * @param {Object} params - The parameters for creating the error.\n   * @param {string} params.name - The name of the error.\n   * @param {string} params.message - The error message.\n   * @param {unknown} [params.cause] - The underlying cause of the error.\n   */\n  constructor({\n    name: name142,\n    message,\n    cause\n  }) {\n    super(message);\n    this[_a15] = true;\n    this.name = name142;\n    this.cause = cause;\n  }\n  /**\n   * Checks if the given error is an AI SDK Error.\n   * @param {unknown} error - The error to check.\n   * @returns {boolean} True if the error is an AI SDK Error, false otherwise.\n   */\n  static isInstance(error) {\n    return _AISDKError4.hasMarker(error, marker15);\n  }\n  static hasMarker(error, marker152) {\n    const markerSymbol = Symbol.for(marker152);\n    return error != null && typeof error === \"object\" && markerSymbol in error && typeof error[markerSymbol] === \"boolean\" && error[markerSymbol] === true;\n  }\n};\n_a15 = symbol15;\nvar AISDKError2 = _AISDKError3;\nvar name14 = \"AI_APICallError\";\nvar marker22 = `vercel.ai.error.${name14}`;\nvar symbol22 = Symbol.for(marker22);\nvar _a22;\nvar APICallError2 = class extends AISDKError2 {\n  constructor({\n    message,\n    url,\n    requestBodyValues,\n    statusCode,\n    responseHeaders,\n    responseBody,\n    cause,\n    isRetryable = statusCode != null && (statusCode === 408 || // request timeout\n    statusCode === 409 || // conflict\n    statusCode === 429 || // too many requests\n    statusCode >= 500),\n    // server error\n    data\n  }) {\n    super({ name: name14, message, cause });\n    this[_a22] = true;\n    this.url = url;\n    this.requestBodyValues = requestBodyValues;\n    this.statusCode = statusCode;\n    this.responseHeaders = responseHeaders;\n    this.responseBody = responseBody;\n    this.isRetryable = isRetryable;\n    this.data = data;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker22);\n  }\n};\n_a22 = symbol22;\nvar name22 = \"AI_EmptyResponseBodyError\";\nvar marker32 = `vercel.ai.error.${name22}`;\nvar symbol32 = Symbol.for(marker32);\nvar _a32;\nvar EmptyResponseBodyError2 = class extends AISDKError2 {\n  // used in isInstance\n  constructor({ message = \"Empty response body\" } = {}) {\n    super({ name: name22, message });\n    this[_a32] = true;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker32);\n  }\n};\n_a32 = symbol32;\nfunction getErrorMessage2(error) {\n  if (error == null) {\n    return \"unknown error\";\n  }\n  if (typeof error === \"string\") {\n    return error;\n  }\n  if (error instanceof Error) {\n    return error.message;\n  }\n  return JSON.stringify(error);\n}\nvar name32 = \"AI_InvalidArgumentError\";\nvar marker42 = `vercel.ai.error.${name32}`;\nvar symbol42 = Symbol.for(marker42);\nvar _a42;\nvar InvalidArgumentError2 = class extends AISDKError2 {\n  constructor({\n    message,\n    cause,\n    argument\n  }) {\n    super({ name: name32, message, cause });\n    this[_a42] = true;\n    this.argument = argument;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker42);\n  }\n};\n_a42 = symbol42;\nvar name42 = \"AI_InvalidPromptError\";\nvar marker52 = `vercel.ai.error.${name42}`;\nvar symbol52 = Symbol.for(marker52);\nvar _a52;\nvar InvalidPromptError2 = class extends AISDKError2 {\n  constructor({\n    prompt,\n    message,\n    cause\n  }) {\n    super({ name: name42, message: `Invalid prompt: ${message}`, cause });\n    this[_a52] = true;\n    this.prompt = prompt;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker52);\n  }\n};\n_a52 = symbol52;\nvar name52 = \"AI_InvalidResponseDataError\";\nvar marker62 = `vercel.ai.error.${name52}`;\nvar symbol62 = Symbol.for(marker62);\nvar _a62;\nvar InvalidResponseDataError2 = class extends AISDKError2 {\n  constructor({\n    data,\n    message = `Invalid response data: ${JSON.stringify(data)}.`\n  }) {\n    super({ name: name52, message });\n    this[_a62] = true;\n    this.data = data;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker62);\n  }\n};\n_a62 = symbol62;\nvar name62 = \"AI_JSONParseError\";\nvar marker72 = `vercel.ai.error.${name62}`;\nvar symbol72 = Symbol.for(marker72);\nvar _a72;\nvar JSONParseError2 = class extends AISDKError2 {\n  constructor({ text, cause }) {\n    super({\n      name: name62,\n      message: `JSON parsing failed: Text: ${text}.\nError message: ${getErrorMessage2(cause)}`,\n      cause\n    });\n    this[_a72] = true;\n    this.text = text;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker72);\n  }\n};\n_a72 = symbol72;\nvar name72 = \"AI_LoadAPIKeyError\";\nvar marker82 = `vercel.ai.error.${name72}`;\nvar symbol82 = Symbol.for(marker82);\nvar _a82;\nvar LoadAPIKeyError2 = class extends AISDKError2 {\n  // used in isInstance\n  constructor({ message }) {\n    super({ name: name72, message });\n    this[_a82] = true;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker82);\n  }\n};\n_a82 = symbol82;\nvar name102 = \"AI_NoSuchModelError\";\nvar marker112 = `vercel.ai.error.${name102}`;\nvar symbol112 = Symbol.for(marker112);\nvar _a112;\nvar NoSuchModelError2 = class extends AISDKError2 {\n  constructor({\n    errorName = name102,\n    modelId,\n    modelType,\n    message = `No such ${modelType}: ${modelId}`\n  }) {\n    super({ name: errorName, message });\n    this[_a112] = true;\n    this.modelId = modelId;\n    this.modelType = modelType;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker112);\n  }\n};\n_a112 = symbol112;\nvar name112 = \"AI_TooManyEmbeddingValuesForCallError\";\nvar marker122 = `vercel.ai.error.${name112}`;\nvar symbol122 = Symbol.for(marker122);\nvar _a122;\nvar TooManyEmbeddingValuesForCallError2 = class extends AISDKError2 {\n  constructor(options) {\n    super({\n      name: name112,\n      message: `Too many values for a single embedding call. The ${options.provider} model \"${options.modelId}\" can only embed up to ${options.maxEmbeddingsPerCall} values per call, but ${options.values.length} values were provided.`\n    });\n    this[_a122] = true;\n    this.provider = options.provider;\n    this.modelId = options.modelId;\n    this.maxEmbeddingsPerCall = options.maxEmbeddingsPerCall;\n    this.values = options.values;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker122);\n  }\n};\n_a122 = symbol122;\nvar name122 = \"AI_TypeValidationError\";\nvar marker132 = `vercel.ai.error.${name122}`;\nvar symbol132 = Symbol.for(marker132);\nvar _a132;\nvar _TypeValidationError3 = class _TypeValidationError4 extends AISDKError2 {\n  constructor({ value, cause }) {\n    super({\n      name: name122,\n      message: `Type validation failed: Value: ${JSON.stringify(value)}.\nError message: ${getErrorMessage2(cause)}`,\n      cause\n    });\n    this[_a132] = true;\n    this.value = value;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker132);\n  }\n  /**\n   * Wraps an error into a TypeValidationError.\n   * If the cause is already a TypeValidationError with the same value, it returns the cause.\n   * Otherwise, it creates a new TypeValidationError.\n   *\n   * @param {Object} params - The parameters for wrapping the error.\n   * @param {unknown} params.value - The value that failed validation.\n   * @param {unknown} params.cause - The original error or cause of the validation failure.\n   * @returns {TypeValidationError} A TypeValidationError instance.\n   */\n  static wrap({\n    value,\n    cause\n  }) {\n    return _TypeValidationError4.isInstance(cause) && cause.value === value ? cause : new _TypeValidationError4({ value, cause });\n  }\n};\n_a132 = symbol132;\nvar TypeValidationError2 = _TypeValidationError3;\nvar name132 = \"AI_UnsupportedFunctionalityError\";\nvar marker142 = `vercel.ai.error.${name132}`;\nvar symbol142 = Symbol.for(marker142);\nvar _a142;\nvar UnsupportedFunctionalityError2 = class extends AISDKError2 {\n  constructor({\n    functionality,\n    message = `'${functionality}' functionality not supported.`\n  }) {\n    super({ name: name132, message });\n    this[_a142] = true;\n    this.functionality = functionality;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker142);\n  }\n};\n_a142 = symbol142;\n\n// ../../node_modules/.pnpm/eventsource-parser@3.0.6/node_modules/eventsource-parser/dist/index.js\nvar ParseError = class extends Error {\n  constructor(message, options) {\n    super(message), this.name = \"ParseError\", this.type = options.type, this.field = options.field, this.value = options.value, this.line = options.line;\n  }\n};\nfunction noop(_arg) {\n}\nfunction createParser(callbacks) {\n  if (typeof callbacks == \"function\")\n    throw new TypeError(\n      \"`callbacks` must be an object, got a function instead. Did you mean `{onEvent: fn}`?\"\n    );\n  const { onEvent = noop, onError = noop, onRetry = noop, onComment } = callbacks;\n  let incompleteLine = \"\", isFirstChunk = true, id, data = \"\", eventType = \"\";\n  function feed(newChunk) {\n    const chunk = isFirstChunk ? newChunk.replace(/^\\xEF\\xBB\\xBF/, \"\") : newChunk, [complete, incomplete] = splitLines2(`${incompleteLine}${chunk}`);\n    for (const line of complete)\n      parseLine(line);\n    incompleteLine = incomplete, isFirstChunk = false;\n  }\n  function parseLine(line) {\n    if (line === \"\") {\n      dispatchEvent();\n      return;\n    }\n    if (line.startsWith(\":\")) {\n      onComment && onComment(line.slice(line.startsWith(\": \") ? 2 : 1));\n      return;\n    }\n    const fieldSeparatorIndex = line.indexOf(\":\");\n    if (fieldSeparatorIndex !== -1) {\n      const field = line.slice(0, fieldSeparatorIndex), offset = line[fieldSeparatorIndex + 1] === \" \" ? 2 : 1, value = line.slice(fieldSeparatorIndex + offset);\n      processField(field, value, line);\n      return;\n    }\n    processField(line, \"\", line);\n  }\n  function processField(field, value, line) {\n    switch (field) {\n      case \"event\":\n        eventType = value;\n        break;\n      case \"data\":\n        data = `${data}${value}\n`;\n        break;\n      case \"id\":\n        id = value.includes(\"\\0\") ? void 0 : value;\n        break;\n      case \"retry\":\n        /^\\d+$/.test(value) ? onRetry(parseInt(value, 10)) : onError(\n          new ParseError(`Invalid \\`retry\\` value: \"${value}\"`, {\n            type: \"invalid-retry\",\n            value,\n            line\n          })\n        );\n        break;\n      default:\n        onError(\n          new ParseError(\n            `Unknown field \"${field.length > 20 ? `${field.slice(0, 20)}\\u2026` : field}\"`,\n            { type: \"unknown-field\", field, value, line }\n          )\n        );\n        break;\n    }\n  }\n  function dispatchEvent() {\n    data.length > 0 && onEvent({\n      id,\n      event: eventType || void 0,\n      // If the data buffer's last character is a U+000A LINE FEED (LF) character,\n      // then remove the last character from the data buffer.\n      data: data.endsWith(`\n`) ? data.slice(0, -1) : data\n    }), id = void 0, data = \"\", eventType = \"\";\n  }\n  function reset(options = {}) {\n    incompleteLine && options.consume && parseLine(incompleteLine), isFirstChunk = true, id = void 0, data = \"\", eventType = \"\", incompleteLine = \"\";\n  }\n  return { feed, reset };\n}\nfunction splitLines2(chunk) {\n  const lines = [];\n  let incompleteLine = \"\", searchIndex = 0;\n  for (; searchIndex < chunk.length; ) {\n    const crIndex = chunk.indexOf(\"\\r\", searchIndex), lfIndex = chunk.indexOf(`\n`, searchIndex);\n    let lineEnd = -1;\n    if (crIndex !== -1 && lfIndex !== -1 ? lineEnd = Math.min(crIndex, lfIndex) : crIndex !== -1 ? crIndex === chunk.length - 1 ? lineEnd = -1 : lineEnd = crIndex : lfIndex !== -1 && (lineEnd = lfIndex), lineEnd === -1) {\n      incompleteLine = chunk.slice(searchIndex);\n      break;\n    } else {\n      const line = chunk.slice(searchIndex, lineEnd);\n      lines.push(line), searchIndex = lineEnd + 1, chunk[searchIndex - 1] === \"\\r\" && chunk[searchIndex] === `\n` && searchIndex++;\n    }\n  }\n  return [lines, incompleteLine];\n}\n\n// ../../node_modules/.pnpm/eventsource-parser@3.0.6/node_modules/eventsource-parser/dist/stream.js\nvar EventSourceParserStream = class extends TransformStream {\n  constructor({ onError, onRetry, onComment } = {}) {\n    let parser;\n    super({\n      start(controller) {\n        parser = createParser({\n          onEvent: (event) => {\n            controller.enqueue(event);\n          },\n          onError(error) {\n            onError === \"terminate\" ? controller.error(error) : typeof onError == \"function\" && onError(error);\n          },\n          onRetry,\n          onComment\n        });\n      },\n      transform(chunk) {\n        parser.feed(chunk);\n      }\n    });\n  }\n};\n\n// ../../node_modules/.pnpm/zod-to-json-schema@3.24.6_zod@3.25.76/node_modules/zod-to-json-schema/dist/esm/parsers/string.js\nnew Set(\"ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijklmnopqrstuvxyz0123456789\");\n\n// ../../node_modules/.pnpm/@ai-sdk+provider-utils@3.0.3_zod@3.25.76/node_modules/@ai-sdk/provider-utils/dist/index.mjs\nfunction combineHeaders2(...headers) {\n  return headers.reduce(\n    (combinedHeaders, currentHeaders) => ({\n      ...combinedHeaders,\n      ...currentHeaders != null ? currentHeaders : {}\n    }),\n    {}\n  );\n}\nfunction extractResponseHeaders2(response) {\n  return Object.fromEntries([...response.headers]);\n}\nvar createIdGenerator2 = ({\n  prefix,\n  size = 16,\n  alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\",\n  separator = \"-\"\n} = {}) => {\n  const generator = () => {\n    const alphabetLength = alphabet.length;\n    const chars = new Array(size);\n    for (let i = 0; i < size; i++) {\n      chars[i] = alphabet[Math.random() * alphabetLength | 0];\n    }\n    return chars.join(\"\");\n  };\n  if (prefix == null) {\n    return generator;\n  }\n  if (alphabet.includes(separator)) {\n    throw new InvalidArgumentError2({\n      argument: \"separator\",\n      message: `The separator \"${separator}\" must not be part of the alphabet \"${alphabet}\".`\n    });\n  }\n  return () => `${prefix}${separator}${generator()}`;\n};\nvar generateId2 = createIdGenerator2();\nfunction isAbortError2(error) {\n  return (error instanceof Error || error instanceof DOMException) && (error.name === \"AbortError\" || error.name === \"ResponseAborted\" || // Next.js\n  error.name === \"TimeoutError\");\n}\nvar FETCH_FAILED_ERROR_MESSAGES = [\"fetch failed\", \"failed to fetch\"];\nfunction handleFetchError({\n  error,\n  url,\n  requestBodyValues\n}) {\n  if (isAbortError2(error)) {\n    return error;\n  }\n  if (error instanceof TypeError && FETCH_FAILED_ERROR_MESSAGES.includes(error.message.toLowerCase())) {\n    const cause = error.cause;\n    if (cause != null) {\n      return new APICallError2({\n        message: `Cannot connect to API: ${cause.message}`,\n        cause,\n        url,\n        requestBodyValues,\n        isRetryable: true\n        // retry when network error\n      });\n    }\n  }\n  return error;\n}\nfunction removeUndefinedEntries2(record) {\n  return Object.fromEntries(\n    Object.entries(record).filter(([_key, value]) => value != null)\n  );\n}\nfunction loadApiKey2({\n  apiKey,\n  environmentVariableName,\n  apiKeyParameterName = \"apiKey\",\n  description\n}) {\n  if (typeof apiKey === \"string\") {\n    return apiKey;\n  }\n  if (apiKey != null) {\n    throw new LoadAPIKeyError2({\n      message: `${description} API key must be a string.`\n    });\n  }\n  if (typeof process === \"undefined\") {\n    throw new LoadAPIKeyError2({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter. Environment variables is not supported in this environment.`\n    });\n  }\n  apiKey = process.env[environmentVariableName];\n  if (apiKey == null) {\n    throw new LoadAPIKeyError2({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter or the ${environmentVariableName} environment variable.`\n    });\n  }\n  if (typeof apiKey !== \"string\") {\n    throw new LoadAPIKeyError2({\n      message: `${description} API key must be a string. The value of the ${environmentVariableName} environment variable is not a string.`\n    });\n  }\n  return apiKey;\n}\nvar suspectProtoRx = /\"__proto__\"\\s*:/;\nvar suspectConstructorRx = /\"constructor\"\\s*:/;\nfunction _parse(text) {\n  const obj = JSON.parse(text);\n  if (obj === null || typeof obj !== \"object\") {\n    return obj;\n  }\n  if (suspectProtoRx.test(text) === false && suspectConstructorRx.test(text) === false) {\n    return obj;\n  }\n  return filter(obj);\n}\nfunction filter(obj) {\n  let next = [obj];\n  while (next.length) {\n    const nodes = next;\n    next = [];\n    for (const node of nodes) {\n      if (Object.prototype.hasOwnProperty.call(node, \"__proto__\")) {\n        throw new SyntaxError(\"Object contains forbidden prototype property\");\n      }\n      if (Object.prototype.hasOwnProperty.call(node, \"constructor\") && Object.prototype.hasOwnProperty.call(node.constructor, \"prototype\")) {\n        throw new SyntaxError(\"Object contains forbidden prototype property\");\n      }\n      for (const key in node) {\n        const value = node[key];\n        if (value && typeof value === \"object\") {\n          next.push(value);\n        }\n      }\n    }\n  }\n  return obj;\n}\nfunction secureJsonParse(text) {\n  const { stackTraceLimit } = Error;\n  Error.stackTraceLimit = 0;\n  try {\n    return _parse(text);\n  } finally {\n    Error.stackTraceLimit = stackTraceLimit;\n  }\n}\nvar validatorSymbol2 = Symbol.for(\"vercel.ai.validator\");\nfunction validator2(validate) {\n  return { [validatorSymbol2]: true, validate };\n}\nfunction isValidator2(value) {\n  return typeof value === \"object\" && value !== null && validatorSymbol2 in value && value[validatorSymbol2] === true && \"validate\" in value;\n}\nfunction asValidator2(value) {\n  return isValidator2(value) ? value : standardSchemaValidator(value);\n}\nfunction standardSchemaValidator(standardSchema) {\n  return validator2(async (value) => {\n    const result = await standardSchema[\"~standard\"].validate(value);\n    return result.issues == null ? { success: true, value: result.value } : {\n      success: false,\n      error: new TypeValidationError2({\n        value,\n        cause: result.issues\n      })\n    };\n  });\n}\nasync function validateTypes2({\n  value,\n  schema\n}) {\n  const result = await safeValidateTypes2({ value, schema });\n  if (!result.success) {\n    throw TypeValidationError2.wrap({ value, cause: result.error });\n  }\n  return result.value;\n}\nasync function safeValidateTypes2({\n  value,\n  schema\n}) {\n  const validator22 = asValidator2(schema);\n  try {\n    if (validator22.validate == null) {\n      return { success: true, value, rawValue: value };\n    }\n    const result = await validator22.validate(value);\n    if (result.success) {\n      return { success: true, value: result.value, rawValue: value };\n    }\n    return {\n      success: false,\n      error: TypeValidationError2.wrap({ value, cause: result.error }),\n      rawValue: value\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error: TypeValidationError2.wrap({ value, cause: error }),\n      rawValue: value\n    };\n  }\n}\nasync function parseJSON2({\n  text,\n  schema\n}) {\n  try {\n    const value = secureJsonParse(text);\n    if (schema == null) {\n      return value;\n    }\n    return validateTypes2({ value, schema });\n  } catch (error) {\n    if (JSONParseError2.isInstance(error) || TypeValidationError2.isInstance(error)) {\n      throw error;\n    }\n    throw new JSONParseError2({ text, cause: error });\n  }\n}\nasync function safeParseJSON2({\n  text,\n  schema\n}) {\n  try {\n    const value = secureJsonParse(text);\n    if (schema == null) {\n      return { success: true, value, rawValue: value };\n    }\n    return await safeValidateTypes2({ value, schema });\n  } catch (error) {\n    return {\n      success: false,\n      error: JSONParseError2.isInstance(error) ? error : new JSONParseError2({ text, cause: error }),\n      rawValue: void 0\n    };\n  }\n}\nfunction isParsableJson2(input) {\n  try {\n    secureJsonParse(input);\n    return true;\n  } catch (e) {\n    return false;\n  }\n}\nfunction parseJsonEventStream({\n  stream,\n  schema\n}) {\n  return stream.pipeThrough(new TextDecoderStream()).pipeThrough(new EventSourceParserStream()).pipeThrough(\n    new TransformStream({\n      async transform({ data }, controller) {\n        if (data === \"[DONE]\") {\n          return;\n        }\n        controller.enqueue(await safeParseJSON2({ text: data, schema }));\n      }\n    })\n  );\n}\nasync function parseProviderOptions2({\n  provider,\n  providerOptions,\n  schema\n}) {\n  if ((providerOptions == null ? void 0 : providerOptions[provider]) == null) {\n    return void 0;\n  }\n  const parsedProviderOptions = await safeValidateTypes2({\n    value: providerOptions[provider],\n    schema\n  });\n  if (!parsedProviderOptions.success) {\n    throw new InvalidArgumentError2({\n      argument: \"providerOptions\",\n      message: `invalid ${provider} provider options`,\n      cause: parsedProviderOptions.error\n    });\n  }\n  return parsedProviderOptions.value;\n}\nvar getOriginalFetch22 = () => globalThis.fetch;\nvar postJsonToApi2 = async ({\n  url,\n  headers,\n  body,\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n}) => postToApi2({\n  url,\n  headers: {\n    \"Content-Type\": \"application/json\",\n    ...headers\n  },\n  body: {\n    content: JSON.stringify(body),\n    values: body\n  },\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n});\nvar postFormDataToApi2 = async ({\n  url,\n  headers,\n  formData,\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n}) => postToApi2({\n  url,\n  headers,\n  body: {\n    content: formData,\n    values: Object.fromEntries(formData.entries())\n  },\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n  fetch\n});\nvar postToApi2 = async ({\n  url,\n  headers = {},\n  body,\n  successfulResponseHandler,\n  failedResponseHandler,\n  abortSignal,\n  fetch = getOriginalFetch22()\n}) => {\n  try {\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers: removeUndefinedEntries2(headers),\n      body: body.content,\n      signal: abortSignal\n    });\n    const responseHeaders = extractResponseHeaders2(response);\n    if (!response.ok) {\n      let errorInformation;\n      try {\n        errorInformation = await failedResponseHandler({\n          response,\n          url,\n          requestBodyValues: body.values\n        });\n      } catch (error) {\n        if (isAbortError2(error) || APICallError2.isInstance(error)) {\n          throw error;\n        }\n        throw new APICallError2({\n          message: \"Failed to process error response\",\n          cause: error,\n          statusCode: response.status,\n          url,\n          responseHeaders,\n          requestBodyValues: body.values\n        });\n      }\n      throw errorInformation.value;\n    }\n    try {\n      return await successfulResponseHandler({\n        response,\n        url,\n        requestBodyValues: body.values\n      });\n    } catch (error) {\n      if (error instanceof Error) {\n        if (isAbortError2(error) || APICallError2.isInstance(error)) {\n          throw error;\n        }\n      }\n      throw new APICallError2({\n        message: \"Failed to process successful response\",\n        cause: error,\n        statusCode: response.status,\n        url,\n        responseHeaders,\n        requestBodyValues: body.values\n      });\n    }\n  } catch (error) {\n    throw handleFetchError({ error, url, requestBodyValues: body.values });\n  }\n};\nfunction tool(tool2) {\n  return tool2;\n}\nfunction createProviderDefinedToolFactory({\n  id,\n  name: name15,\n  inputSchema\n}) {\n  return ({\n    execute,\n    outputSchema,\n    toModelOutput,\n    onInputStart,\n    onInputDelta,\n    onInputAvailable,\n    ...args\n  }) => tool({\n    type: \"provider-defined\",\n    id,\n    name: name15,\n    args,\n    inputSchema,\n    outputSchema,\n    execute,\n    toModelOutput,\n    onInputStart,\n    onInputDelta,\n    onInputAvailable\n  });\n}\nfunction createProviderDefinedToolFactoryWithOutputSchema({\n  id,\n  name: name15,\n  inputSchema,\n  outputSchema\n}) {\n  return ({\n    execute,\n    toModelOutput,\n    onInputStart,\n    onInputDelta,\n    onInputAvailable,\n    ...args\n  }) => tool({\n    type: \"provider-defined\",\n    id,\n    name: name15,\n    args,\n    inputSchema,\n    outputSchema,\n    execute,\n    toModelOutput,\n    onInputStart,\n    onInputDelta,\n    onInputAvailable\n  });\n}\nasync function resolve2(value) {\n  if (typeof value === \"function\") {\n    value = value();\n  }\n  return Promise.resolve(value);\n}\nvar createJsonErrorResponseHandler2 = ({\n  errorSchema,\n  errorToMessage,\n  isRetryable\n}) => async ({ response, url, requestBodyValues }) => {\n  const responseBody = await response.text();\n  const responseHeaders = extractResponseHeaders2(response);\n  if (responseBody.trim() === \"\") {\n    return {\n      responseHeaders,\n      value: new APICallError2({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response)\n      })\n    };\n  }\n  try {\n    const parsedError = await parseJSON2({\n      text: responseBody,\n      schema: errorSchema\n    });\n    return {\n      responseHeaders,\n      value: new APICallError2({\n        message: errorToMessage(parsedError),\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        data: parsedError,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response, parsedError)\n      })\n    };\n  } catch (parseError) {\n    return {\n      responseHeaders,\n      value: new APICallError2({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseHeaders,\n        responseBody,\n        isRetryable: isRetryable == null ? void 0 : isRetryable(response)\n      })\n    };\n  }\n};\nvar createEventSourceResponseHandler2 = (chunkSchema3) => async ({ response }) => {\n  const responseHeaders = extractResponseHeaders2(response);\n  if (response.body == null) {\n    throw new EmptyResponseBodyError2({});\n  }\n  return {\n    responseHeaders,\n    value: parseJsonEventStream({\n      stream: response.body,\n      schema: chunkSchema3\n    })\n  };\n};\nvar createJsonResponseHandler2 = (responseSchema3) => async ({ response, url, requestBodyValues }) => {\n  const responseBody = await response.text();\n  const parsedResult = await safeParseJSON2({\n    text: responseBody,\n    schema: responseSchema3\n  });\n  const responseHeaders = extractResponseHeaders2(response);\n  if (!parsedResult.success) {\n    throw new APICallError2({\n      message: \"Invalid JSON response\",\n      cause: parsedResult.error,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody,\n      url,\n      requestBodyValues\n    });\n  }\n  return {\n    responseHeaders,\n    value: parsedResult.value,\n    rawValue: parsedResult.rawValue\n  };\n};\nvar createBinaryResponseHandler2 = () => async ({ response, url, requestBodyValues }) => {\n  const responseHeaders = extractResponseHeaders2(response);\n  if (!response.body) {\n    throw new APICallError2({\n      message: \"Response body is empty\",\n      url,\n      requestBodyValues,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody: void 0\n    });\n  }\n  try {\n    const buffer = await response.arrayBuffer();\n    return {\n      responseHeaders,\n      value: new Uint8Array(buffer)\n    };\n  } catch (error) {\n    throw new APICallError2({\n      message: \"Failed to read response as array buffer\",\n      url,\n      requestBodyValues,\n      statusCode: response.status,\n      responseHeaders,\n      responseBody: void 0,\n      cause: error\n    });\n  }\n};\nvar { btoa: btoa2, atob: atob2 } = globalThis;\nfunction convertBase64ToUint8Array2(base64String) {\n  const base64Url = base64String.replace(/-/g, \"+\").replace(/_/g, \"/\");\n  const latin1string = atob2(base64Url);\n  return Uint8Array.from(latin1string, (byte) => byte.codePointAt(0));\n}\nfunction convertUint8ArrayToBase642(array) {\n  let latin1string = \"\";\n  for (let i = 0; i < array.length; i++) {\n    latin1string += String.fromCodePoint(array[i]);\n  }\n  return btoa2(latin1string);\n}\nfunction convertToBase64(value) {\n  return value instanceof Uint8Array ? convertUint8ArrayToBase642(value) : value;\n}\nfunction withoutTrailingSlash2(url) {\n  return url == null ? void 0 : url.replace(/\\/$/, \"\");\n}\nvar anthropicErrorDataSchema2 = z$1.object({\n  type: z$1.literal(\"error\"),\n  error: z$1.object({\n    type: z$1.string(),\n    message: z$1.string()\n  })\n});\nvar anthropicFailedResponseHandler2 = createJsonErrorResponseHandler2({\n  errorSchema: anthropicErrorDataSchema2,\n  errorToMessage: (data) => data.error.message\n});\nvar anthropicFilePartProviderOptions = z$1.object({\n  /**\n   * Citation configuration for this document.\n   * When enabled, this document will generate citations in the response.\n   */\n  citations: z$1.object({\n    /**\n     * Enable citations for this document\n     */\n    enabled: z$1.boolean()\n  }).optional(),\n  /**\n   * Custom title for the document.\n   * If not provided, the filename will be used.\n   */\n  title: z$1.string().optional(),\n  /**\n   * Context about the document that will be passed to the model\n   * but not used towards cited content.\n   * Useful for storing document metadata as text or stringified JSON.\n   */\n  context: z$1.string().optional()\n});\nvar anthropicProviderOptions = z$1.object({\n  sendReasoning: z$1.boolean().optional(),\n  thinking: z$1.object({\n    type: z$1.union([z$1.literal(\"enabled\"), z$1.literal(\"disabled\")]),\n    budgetTokens: z$1.number().optional()\n  }).optional(),\n  /**\n   * Whether to disable parallel function calling during tool use. Default is false.\n   * When set to true, Claude will use at most one tool per response.\n   */\n  disableParallelToolUse: z$1.boolean().optional()\n});\nfunction getCacheControl(providerMetadata) {\n  var _a16;\n  const anthropic22 = providerMetadata == null ? void 0 : providerMetadata.anthropic;\n  const cacheControlValue = (_a16 = anthropic22 == null ? void 0 : anthropic22.cacheControl) != null ? _a16 : anthropic22 == null ? void 0 : anthropic22.cache_control;\n  return cacheControlValue;\n}\nvar webSearch_20250305ArgsSchema = z$1.object({\n  /**\n   * Maximum number of web searches Claude can perform during the conversation.\n   */\n  maxUses: z$1.number().optional(),\n  /**\n   * Optional list of domains that Claude is allowed to search.\n   */\n  allowedDomains: z$1.array(z$1.string()).optional(),\n  /**\n   * Optional list of domains that Claude should avoid when searching.\n   */\n  blockedDomains: z$1.array(z$1.string()).optional(),\n  /**\n   * Optional user location information to provide geographically relevant search results.\n   */\n  userLocation: z$1.object({\n    type: z$1.literal(\"approximate\"),\n    city: z$1.string().optional(),\n    region: z$1.string().optional(),\n    country: z$1.string().optional(),\n    timezone: z$1.string().optional()\n  }).optional()\n});\nvar webSearch_20250305OutputSchema = z$1.array(\n  z$1.object({\n    url: z$1.string(),\n    title: z$1.string(),\n    pageAge: z$1.string().nullable(),\n    encryptedContent: z$1.string(),\n    type: z$1.string()\n  })\n);\nvar factory = createProviderDefinedToolFactoryWithOutputSchema({\n  id: \"anthropic.web_search_20250305\",\n  name: \"web_search\",\n  inputSchema: z$1.object({\n    query: z$1.string()\n  }),\n  outputSchema: webSearch_20250305OutputSchema\n});\nvar webSearch_20250305 = (args = {}) => {\n  return factory(args);\n};\nfunction isWebSearchTool(tool2) {\n  return typeof tool2 === \"object\" && tool2 !== null && \"type\" in tool2 && tool2.type === \"web_search_20250305\";\n}\nfunction prepareTools2({\n  tools,\n  toolChoice,\n  disableParallelToolUse\n}) {\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  const betas = /* @__PURE__ */ new Set();\n  if (tools == null) {\n    return { tools: void 0, toolChoice: void 0, toolWarnings, betas };\n  }\n  const anthropicTools22 = [];\n  for (const tool2 of tools) {\n    if (isWebSearchTool(tool2)) {\n      anthropicTools22.push(tool2);\n      continue;\n    }\n    switch (tool2.type) {\n      case \"function\":\n        const cacheControl = getCacheControl(tool2.providerOptions);\n        anthropicTools22.push({\n          name: tool2.name,\n          description: tool2.description,\n          input_schema: tool2.inputSchema,\n          cache_control: cacheControl\n        });\n        break;\n      case \"provider-defined\":\n        switch (tool2.id) {\n          case \"anthropic.computer_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: \"computer\",\n              type: \"computer_20250124\",\n              display_width_px: tool2.args.displayWidthPx,\n              display_height_px: tool2.args.displayHeightPx,\n              display_number: tool2.args.displayNumber\n            });\n            break;\n          case \"anthropic.computer_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: \"computer\",\n              type: \"computer_20241022\",\n              display_width_px: tool2.args.displayWidthPx,\n              display_height_px: tool2.args.displayHeightPx,\n              display_number: tool2.args.displayNumber\n            });\n            break;\n          case \"anthropic.text_editor_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: \"str_replace_editor\",\n              type: \"text_editor_20250124\"\n            });\n            break;\n          case \"anthropic.text_editor_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: \"str_replace_editor\",\n              type: \"text_editor_20241022\"\n            });\n            break;\n          case \"anthropic.text_editor_20250429\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: \"str_replace_based_edit_tool\",\n              type: \"text_editor_20250429\"\n            });\n            break;\n          case \"anthropic.bash_20250124\":\n            betas.add(\"computer-use-2025-01-24\");\n            anthropicTools22.push({\n              name: \"bash\",\n              type: \"bash_20250124\"\n            });\n            break;\n          case \"anthropic.bash_20241022\":\n            betas.add(\"computer-use-2024-10-22\");\n            anthropicTools22.push({\n              name: \"bash\",\n              type: \"bash_20241022\"\n            });\n            break;\n          case \"anthropic.web_search_20250305\": {\n            const args = webSearch_20250305ArgsSchema.parse(tool2.args);\n            anthropicTools22.push({\n              type: \"web_search_20250305\",\n              name: \"web_search\",\n              max_uses: args.maxUses,\n              allowed_domains: args.allowedDomains,\n              blocked_domains: args.blockedDomains,\n              user_location: args.userLocation\n            });\n            break;\n          }\n          case \"anthropic.code_execution_20250522\": {\n            betas.add(\"code-execution-2025-05-22\");\n            anthropicTools22.push({\n              type: \"code_execution_20250522\",\n              name: \"code_execution\"\n            });\n            break;\n          }\n          default:\n            toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return {\n      tools: anthropicTools22,\n      toolChoice: disableParallelToolUse ? { type: \"auto\", disable_parallel_tool_use: disableParallelToolUse } : void 0,\n      toolWarnings,\n      betas\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n      return {\n        tools: anthropicTools22,\n        toolChoice: {\n          type: \"auto\",\n          disable_parallel_tool_use: disableParallelToolUse\n        },\n        toolWarnings,\n        betas\n      };\n    case \"required\":\n      return {\n        tools: anthropicTools22,\n        toolChoice: {\n          type: \"any\",\n          disable_parallel_tool_use: disableParallelToolUse\n        },\n        toolWarnings,\n        betas\n      };\n    case \"none\":\n      return { tools: void 0, toolChoice: void 0, toolWarnings, betas };\n    case \"tool\":\n      return {\n        tools: anthropicTools22,\n        toolChoice: {\n          type: \"tool\",\n          name: toolChoice.toolName,\n          disable_parallel_tool_use: disableParallelToolUse\n        },\n        toolWarnings,\n        betas\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar codeExecution_20250522OutputSchema = z$1.object({\n  type: z$1.literal(\"code_execution_result\"),\n  stdout: z$1.string(),\n  stderr: z$1.string(),\n  return_code: z$1.number()\n});\nvar factory2 = createProviderDefinedToolFactoryWithOutputSchema({\n  id: \"anthropic.code_execution_20250522\",\n  name: \"code_execution\",\n  inputSchema: z$1.object({\n    code: z$1.string()\n  }),\n  outputSchema: codeExecution_20250522OutputSchema\n});\nvar codeExecution_20250522 = (args = {}) => {\n  return factory2(args);\n};\nfunction convertToString(data) {\n  if (typeof data === \"string\") {\n    return Buffer.from(data, \"base64\").toString(\"utf-8\");\n  }\n  if (data instanceof Uint8Array) {\n    return new TextDecoder().decode(data);\n  }\n  if (data instanceof URL) {\n    throw new UnsupportedFunctionalityError2({\n      functionality: \"URL-based text documents are not supported for citations\"\n    });\n  }\n  throw new UnsupportedFunctionalityError2({\n    functionality: `unsupported data type for text documents: ${typeof data}`\n  });\n}\nasync function convertToAnthropicMessagesPrompt2({\n  prompt,\n  sendReasoning,\n  warnings\n}) {\n  var _a16, _b, _c, _d, _e;\n  const betas = /* @__PURE__ */ new Set();\n  const blocks = groupIntoBlocks2(prompt);\n  let system = void 0;\n  const messages = [];\n  async function shouldEnableCitations(providerMetadata) {\n    var _a23, _b2;\n    const anthropicOptions = await parseProviderOptions2({\n      provider: \"anthropic\",\n      providerOptions: providerMetadata,\n      schema: anthropicFilePartProviderOptions\n    });\n    return (_b2 = (_a23 = anthropicOptions == null ? void 0 : anthropicOptions.citations) == null ? void 0 : _a23.enabled) != null ? _b2 : false;\n  }\n  async function getDocumentMetadata(providerMetadata) {\n    const anthropicOptions = await parseProviderOptions2({\n      provider: \"anthropic\",\n      providerOptions: providerMetadata,\n      schema: anthropicFilePartProviderOptions\n    });\n    return {\n      title: anthropicOptions == null ? void 0 : anthropicOptions.title,\n      context: anthropicOptions == null ? void 0 : anthropicOptions.context\n    };\n  }\n  for (let i = 0; i < blocks.length; i++) {\n    const block = blocks[i];\n    const isLastBlock = i === blocks.length - 1;\n    const type = block.type;\n    switch (type) {\n      case \"system\": {\n        if (system != null) {\n          throw new UnsupportedFunctionalityError2({\n            functionality: \"Multiple system messages that are separated by user/assistant messages\"\n          });\n        }\n        system = block.messages.map(({ content, providerOptions }) => ({\n          type: \"text\",\n          text: content,\n          cache_control: getCacheControl(providerOptions)\n        }));\n        break;\n      }\n      case \"user\": {\n        const anthropicContent = [];\n        for (const message of block.messages) {\n          const { role, content } = message;\n          switch (role) {\n            case \"user\": {\n              for (let j = 0; j < content.length; j++) {\n                const part = content[j];\n                const isLastPart = j === content.length - 1;\n                const cacheControl = (_a16 = getCacheControl(part.providerOptions)) != null ? _a16 : isLastPart ? getCacheControl(message.providerOptions) : void 0;\n                switch (part.type) {\n                  case \"text\": {\n                    anthropicContent.push({\n                      type: \"text\",\n                      text: part.text,\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                  case \"file\": {\n                    if (part.mediaType.startsWith(\"image/\")) {\n                      anthropicContent.push({\n                        type: \"image\",\n                        source: part.data instanceof URL ? {\n                          type: \"url\",\n                          url: part.data.toString()\n                        } : {\n                          type: \"base64\",\n                          media_type: part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType,\n                          data: convertToBase64(part.data)\n                        },\n                        cache_control: cacheControl\n                      });\n                    } else if (part.mediaType === \"application/pdf\") {\n                      betas.add(\"pdfs-2024-09-25\");\n                      const enableCitations = await shouldEnableCitations(\n                        part.providerOptions\n                      );\n                      const metadata = await getDocumentMetadata(\n                        part.providerOptions\n                      );\n                      anthropicContent.push({\n                        type: \"document\",\n                        source: part.data instanceof URL ? {\n                          type: \"url\",\n                          url: part.data.toString()\n                        } : {\n                          type: \"base64\",\n                          media_type: \"application/pdf\",\n                          data: convertToBase64(part.data)\n                        },\n                        title: (_b = metadata.title) != null ? _b : part.filename,\n                        ...metadata.context && { context: metadata.context },\n                        ...enableCitations && {\n                          citations: { enabled: true }\n                        },\n                        cache_control: cacheControl\n                      });\n                    } else if (part.mediaType === \"text/plain\") {\n                      const enableCitations = await shouldEnableCitations(\n                        part.providerOptions\n                      );\n                      const metadata = await getDocumentMetadata(\n                        part.providerOptions\n                      );\n                      anthropicContent.push({\n                        type: \"document\",\n                        source: part.data instanceof URL ? {\n                          type: \"url\",\n                          url: part.data.toString()\n                        } : {\n                          type: \"text\",\n                          media_type: \"text/plain\",\n                          data: convertToString(part.data)\n                        },\n                        title: (_c = metadata.title) != null ? _c : part.filename,\n                        ...metadata.context && { context: metadata.context },\n                        ...enableCitations && {\n                          citations: { enabled: true }\n                        },\n                        cache_control: cacheControl\n                      });\n                    } else {\n                      throw new UnsupportedFunctionalityError2({\n                        functionality: `media type: ${part.mediaType}`\n                      });\n                    }\n                    break;\n                  }\n                }\n              }\n              break;\n            }\n            case \"tool\": {\n              for (let i2 = 0; i2 < content.length; i2++) {\n                const part = content[i2];\n                const isLastPart = i2 === content.length - 1;\n                const cacheControl = (_d = getCacheControl(part.providerOptions)) != null ? _d : isLastPart ? getCacheControl(message.providerOptions) : void 0;\n                const output = part.output;\n                let contentValue;\n                switch (output.type) {\n                  case \"content\":\n                    contentValue = output.value.map((contentPart) => {\n                      switch (contentPart.type) {\n                        case \"text\":\n                          return {\n                            type: \"text\",\n                            text: contentPart.text,\n                            cache_control: void 0\n                          };\n                        case \"media\": {\n                          if (contentPart.mediaType.startsWith(\"image/\")) {\n                            return {\n                              type: \"image\",\n                              source: {\n                                type: \"base64\",\n                                media_type: contentPart.mediaType,\n                                data: contentPart.data\n                              },\n                              cache_control: void 0\n                            };\n                          }\n                          throw new UnsupportedFunctionalityError2({\n                            functionality: `media type: ${contentPart.mediaType}`\n                          });\n                        }\n                      }\n                    });\n                    break;\n                  case \"text\":\n                  case \"error-text\":\n                    contentValue = output.value;\n                    break;\n                  case \"json\":\n                  case \"error-json\":\n                  default:\n                    contentValue = JSON.stringify(output.value);\n                    break;\n                }\n                anthropicContent.push({\n                  type: \"tool_result\",\n                  tool_use_id: part.toolCallId,\n                  content: contentValue,\n                  is_error: output.type === \"error-text\" || output.type === \"error-json\" ? true : void 0,\n                  cache_control: cacheControl\n                });\n              }\n              break;\n            }\n            default: {\n              const _exhaustiveCheck = role;\n              throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n            }\n          }\n        }\n        messages.push({ role: \"user\", content: anthropicContent });\n        break;\n      }\n      case \"assistant\": {\n        const anthropicContent = [];\n        for (let j = 0; j < block.messages.length; j++) {\n          const message = block.messages[j];\n          const isLastMessage = j === block.messages.length - 1;\n          const { content } = message;\n          for (let k = 0; k < content.length; k++) {\n            const part = content[k];\n            const isLastContentPart = k === content.length - 1;\n            const cacheControl = (_e = getCacheControl(part.providerOptions)) != null ? _e : isLastContentPart ? getCacheControl(message.providerOptions) : void 0;\n            switch (part.type) {\n              case \"text\": {\n                anthropicContent.push({\n                  type: \"text\",\n                  text: (\n                    // trim the last text part if it's the last message in the block\n                    // because Anthropic does not allow trailing whitespace\n                    // in pre-filled assistant responses\n                    isLastBlock && isLastMessage && isLastContentPart ? part.text.trim() : part.text\n                  ),\n                  cache_control: cacheControl\n                });\n                break;\n              }\n              case \"reasoning\": {\n                if (sendReasoning) {\n                  const reasoningMetadata = await parseProviderOptions2({\n                    provider: \"anthropic\",\n                    providerOptions: part.providerOptions,\n                    schema: anthropicReasoningMetadataSchema\n                  });\n                  if (reasoningMetadata != null) {\n                    if (reasoningMetadata.signature != null) {\n                      anthropicContent.push({\n                        type: \"thinking\",\n                        thinking: part.text,\n                        signature: reasoningMetadata.signature,\n                        cache_control: cacheControl\n                      });\n                    } else if (reasoningMetadata.redactedData != null) {\n                      anthropicContent.push({\n                        type: \"redacted_thinking\",\n                        data: reasoningMetadata.redactedData,\n                        cache_control: cacheControl\n                      });\n                    } else {\n                      warnings.push({\n                        type: \"other\",\n                        message: \"unsupported reasoning metadata\"\n                      });\n                    }\n                  } else {\n                    warnings.push({\n                      type: \"other\",\n                      message: \"unsupported reasoning metadata\"\n                    });\n                  }\n                } else {\n                  warnings.push({\n                    type: \"other\",\n                    message: \"sending reasoning content is disabled for this model\"\n                  });\n                }\n                break;\n              }\n              case \"tool-call\": {\n                if (part.providerExecuted) {\n                  if (part.toolName === \"web_search\") {\n                    anthropicContent.push({\n                      type: \"server_tool_use\",\n                      id: part.toolCallId,\n                      name: \"web_search\",\n                      input: part.input,\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                  if (part.toolName === \"code_execution\") {\n                    anthropicContent.push({\n                      type: \"server_tool_use\",\n                      id: part.toolCallId,\n                      name: \"code_execution\",\n                      input: part.input,\n                      cache_control: cacheControl\n                    });\n                    break;\n                  }\n                  warnings.push({\n                    type: \"other\",\n                    message: `provider executed tool call for tool ${part.toolName} is not supported`\n                  });\n                  break;\n                }\n                anthropicContent.push({\n                  type: \"tool_use\",\n                  id: part.toolCallId,\n                  name: part.toolName,\n                  input: part.input,\n                  cache_control: cacheControl\n                });\n                break;\n              }\n              case \"tool-result\": {\n                if (part.toolName === \"web_search\") {\n                  const output = part.output;\n                  if (output.type !== \"json\") {\n                    warnings.push({\n                      type: \"other\",\n                      message: `provider executed tool result output type ${output.type} for tool ${part.toolName} is not supported`\n                    });\n                    break;\n                  }\n                  const webSearchOutput = webSearch_20250305OutputSchema.parse(\n                    output.value\n                  );\n                  anthropicContent.push({\n                    type: \"web_search_tool_result\",\n                    tool_use_id: part.toolCallId,\n                    content: webSearchOutput.map((result) => ({\n                      url: result.url,\n                      title: result.title,\n                      page_age: result.pageAge,\n                      encrypted_content: result.encryptedContent,\n                      type: result.type\n                    })),\n                    cache_control: cacheControl\n                  });\n                  break;\n                }\n                if (part.toolName === \"code_execution\") {\n                  const output = part.output;\n                  if (output.type !== \"json\") {\n                    warnings.push({\n                      type: \"other\",\n                      message: `provider executed tool result output type ${output.type} for tool ${part.toolName} is not supported`\n                    });\n                    break;\n                  }\n                  const codeExecutionOutput = codeExecution_20250522OutputSchema.parse(output.value);\n                  anthropicContent.push({\n                    type: \"code_execution_tool_result\",\n                    tool_use_id: part.toolCallId,\n                    content: {\n                      type: codeExecutionOutput.type,\n                      stdout: codeExecutionOutput.stdout,\n                      stderr: codeExecutionOutput.stderr,\n                      return_code: codeExecutionOutput.return_code\n                    },\n                    cache_control: cacheControl\n                  });\n                  break;\n                }\n                warnings.push({\n                  type: \"other\",\n                  message: `provider executed tool result for tool ${part.toolName} is not supported`\n                });\n                break;\n              }\n            }\n          }\n        }\n        messages.push({ role: \"assistant\", content: anthropicContent });\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`content type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return {\n    prompt: { system, messages },\n    betas\n  };\n}\nfunction groupIntoBlocks2(prompt) {\n  const blocks = [];\n  let currentBlock = void 0;\n  for (const message of prompt) {\n    const { role } = message;\n    switch (role) {\n      case \"system\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"system\") {\n          currentBlock = { type: \"system\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"assistant\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"assistant\") {\n          currentBlock = { type: \"assistant\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"user\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"user\") {\n          currentBlock = { type: \"user\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      case \"tool\": {\n        if ((currentBlock == null ? void 0 : currentBlock.type) !== \"user\") {\n          currentBlock = { type: \"user\", messages: [] };\n          blocks.push(currentBlock);\n        }\n        currentBlock.messages.push(message);\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return blocks;\n}\nfunction mapAnthropicStopReason2({\n  finishReason,\n  isJsonResponseFromTool\n}) {\n  switch (finishReason) {\n    case \"end_turn\":\n    case \"stop_sequence\":\n      return \"stop\";\n    case \"tool_use\":\n      return isJsonResponseFromTool ? \"stop\" : \"tool-calls\";\n    case \"max_tokens\":\n      return \"length\";\n    default:\n      return \"unknown\";\n  }\n}\nvar citationSchemas = {\n  webSearchResult: z$1.object({\n    type: z$1.literal(\"web_search_result_location\"),\n    cited_text: z$1.string(),\n    url: z$1.string(),\n    title: z$1.string(),\n    encrypted_index: z$1.string()\n  }),\n  pageLocation: z$1.object({\n    type: z$1.literal(\"page_location\"),\n    cited_text: z$1.string(),\n    document_index: z$1.number(),\n    document_title: z$1.string().nullable(),\n    start_page_number: z$1.number(),\n    end_page_number: z$1.number()\n  }),\n  charLocation: z$1.object({\n    type: z$1.literal(\"char_location\"),\n    cited_text: z$1.string(),\n    document_index: z$1.number(),\n    document_title: z$1.string().nullable(),\n    start_char_index: z$1.number(),\n    end_char_index: z$1.number()\n  })\n};\nvar citationSchema = z$1.discriminatedUnion(\"type\", [\n  citationSchemas.webSearchResult,\n  citationSchemas.pageLocation,\n  citationSchemas.charLocation\n]);\nz$1.discriminatedUnion(\"type\", [\n  citationSchemas.pageLocation,\n  citationSchemas.charLocation\n]);\nfunction processCitation(citation, citationDocuments, generateId3, onSource) {\n  if (citation.type === \"page_location\" || citation.type === \"char_location\") {\n    const source = createCitationSource(\n      citation,\n      citationDocuments,\n      generateId3\n    );\n    if (source) {\n      onSource(source);\n    }\n  }\n}\nfunction createCitationSource(citation, citationDocuments, generateId3) {\n  var _a16;\n  const documentInfo = citationDocuments[citation.document_index];\n  if (!documentInfo) {\n    return null;\n  }\n  const providerMetadata = citation.type === \"page_location\" ? {\n    citedText: citation.cited_text,\n    startPageNumber: citation.start_page_number,\n    endPageNumber: citation.end_page_number\n  } : {\n    citedText: citation.cited_text,\n    startCharIndex: citation.start_char_index,\n    endCharIndex: citation.end_char_index\n  };\n  return {\n    type: \"source\",\n    sourceType: \"document\",\n    id: generateId3(),\n    mediaType: documentInfo.mediaType,\n    title: (_a16 = citation.document_title) != null ? _a16 : documentInfo.title,\n    filename: documentInfo.filename,\n    providerMetadata: {\n      anthropic: providerMetadata\n    }\n  };\n}\nvar AnthropicMessagesLanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    var _a16;\n    this.modelId = modelId;\n    this.config = config;\n    this.generateId = (_a16 = config.generateId) != null ? _a16 : generateId2;\n  }\n  supportsUrl(url) {\n    return url.protocol === \"https:\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportedUrls() {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).supportedUrls) == null ? void 0 : _b.call(_a16)) != null ? _c : {};\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens = 4096,\n    // 4096: max model output tokens TODO update default in v5\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    tools,\n    toolChoice,\n    providerOptions\n  }) {\n    var _a16, _b, _c;\n    const warnings = [];\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\") {\n      if (responseFormat.schema == null) {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"responseFormat\",\n          details: \"JSON response format requires a schema. The response format is ignored.\"\n        });\n      } else if (tools != null) {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"tools\",\n          details: \"JSON response format does not support tools. The provided tools are ignored.\"\n        });\n      }\n    }\n    const jsonResponseTool = (responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null ? {\n      type: \"function\",\n      name: \"json\",\n      description: \"Respond with a JSON object.\",\n      inputSchema: responseFormat.schema\n    } : void 0;\n    const anthropicOptions = await parseProviderOptions2({\n      provider: \"anthropic\",\n      providerOptions,\n      schema: anthropicProviderOptions\n    });\n    const { prompt: messagesPrompt, betas: messagesBetas } = await convertToAnthropicMessagesPrompt2({\n      prompt,\n      sendReasoning: (_a16 = anthropicOptions == null ? void 0 : anthropicOptions.sendReasoning) != null ? _a16 : true,\n      warnings\n    });\n    const isThinking = ((_b = anthropicOptions == null ? void 0 : anthropicOptions.thinking) == null ? void 0 : _b.type) === \"enabled\";\n    const thinkingBudget = (_c = anthropicOptions == null ? void 0 : anthropicOptions.thinking) == null ? void 0 : _c.budgetTokens;\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // standardized settings:\n      max_tokens: maxOutputTokens,\n      temperature,\n      top_k: topK,\n      top_p: topP,\n      stop_sequences: stopSequences,\n      // provider specific settings:\n      ...isThinking && {\n        thinking: { type: \"enabled\", budget_tokens: thinkingBudget }\n      },\n      // prompt:\n      system: messagesPrompt.system,\n      messages: messagesPrompt.messages\n    };\n    if (isThinking) {\n      if (thinkingBudget == null) {\n        throw new UnsupportedFunctionalityError2({\n          functionality: \"thinking requires a budget\"\n        });\n      }\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported when thinking is enabled\"\n        });\n      }\n      if (topK != null) {\n        baseArgs.top_k = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topK\",\n          details: \"topK is not supported when thinking is enabled\"\n        });\n      }\n      if (topP != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported when thinking is enabled\"\n        });\n      }\n      baseArgs.max_tokens = maxOutputTokens + thinkingBudget;\n    }\n    const {\n      tools: anthropicTools22,\n      toolChoice: anthropicToolChoice,\n      toolWarnings,\n      betas: toolsBetas\n    } = prepareTools2(\n      jsonResponseTool != null ? {\n        tools: [jsonResponseTool],\n        toolChoice: { type: \"tool\", toolName: jsonResponseTool.name },\n        disableParallelToolUse: anthropicOptions == null ? void 0 : anthropicOptions.disableParallelToolUse\n      } : {\n        tools: tools != null ? tools : [],\n        toolChoice,\n        disableParallelToolUse: anthropicOptions == null ? void 0 : anthropicOptions.disableParallelToolUse\n      }\n    );\n    return {\n      args: {\n        ...baseArgs,\n        tools: anthropicTools22,\n        tool_choice: anthropicToolChoice\n      },\n      warnings: [...warnings, ...toolWarnings],\n      betas: /* @__PURE__ */ new Set([...messagesBetas, ...toolsBetas]),\n      usesJsonResponseTool: jsonResponseTool != null\n    };\n  }\n  async getHeaders({\n    betas,\n    headers\n  }) {\n    return combineHeaders2(\n      await resolve2(this.config.headers),\n      betas.size > 0 ? { \"anthropic-beta\": Array.from(betas).join(\",\") } : {},\n      headers\n    );\n  }\n  buildRequestUrl(isStreaming) {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).buildRequestUrl) == null ? void 0 : _b.call(_a16, this.config.baseURL, isStreaming)) != null ? _c : `${this.config.baseURL}/messages`;\n  }\n  transformRequestBody(args) {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).transformRequestBody) == null ? void 0 : _b.call(_a16, args)) != null ? _c : args;\n  }\n  extractCitationDocuments(prompt) {\n    const isCitationPart = (part) => {\n      var _a16, _b;\n      if (part.type !== \"file\") {\n        return false;\n      }\n      if (part.mediaType !== \"application/pdf\" && part.mediaType !== \"text/plain\") {\n        return false;\n      }\n      const anthropic22 = (_a16 = part.providerOptions) == null ? void 0 : _a16.anthropic;\n      const citationsConfig = anthropic22 == null ? void 0 : anthropic22.citations;\n      return (_b = citationsConfig == null ? void 0 : citationsConfig.enabled) != null ? _b : false;\n    };\n    return prompt.filter((message) => message.role === \"user\").flatMap((message) => message.content).filter(isCitationPart).map((part) => {\n      var _a16;\n      const filePart = part;\n      return {\n        title: (_a16 = filePart.filename) != null ? _a16 : \"Untitled Document\",\n        filename: filePart.filename,\n        mediaType: filePart.mediaType\n      };\n    });\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e;\n    const { args, warnings, betas, usesJsonResponseTool } = await this.getArgs(options);\n    const citationDocuments = this.extractCitationDocuments(options.prompt);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.buildRequestUrl(false),\n      headers: await this.getHeaders({ betas, headers: options.headers }),\n      body: this.transformRequestBody(args),\n      failedResponseHandler: anthropicFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        anthropicMessagesResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const content = [];\n    for (const part of response.content) {\n      switch (part.type) {\n        case \"text\": {\n          if (!usesJsonResponseTool) {\n            content.push({ type: \"text\", text: part.text });\n            if (part.citations) {\n              for (const citation of part.citations) {\n                processCitation(\n                  citation,\n                  citationDocuments,\n                  this.generateId,\n                  (source) => content.push(source)\n                );\n              }\n            }\n          }\n          break;\n        }\n        case \"thinking\": {\n          content.push({\n            type: \"reasoning\",\n            text: part.thinking,\n            providerMetadata: {\n              anthropic: {\n                signature: part.signature\n              }\n            }\n          });\n          break;\n        }\n        case \"redacted_thinking\": {\n          content.push({\n            type: \"reasoning\",\n            text: \"\",\n            providerMetadata: {\n              anthropic: {\n                redactedData: part.data\n              }\n            }\n          });\n          break;\n        }\n        case \"tool_use\": {\n          content.push(\n            // when a json response tool is used, the tool call becomes the text:\n            usesJsonResponseTool ? {\n              type: \"text\",\n              text: JSON.stringify(part.input)\n            } : {\n              type: \"tool-call\",\n              toolCallId: part.id,\n              toolName: part.name,\n              input: JSON.stringify(part.input)\n            }\n          );\n          break;\n        }\n        case \"server_tool_use\": {\n          if (part.name === \"web_search\" || part.name === \"code_execution\") {\n            content.push({\n              type: \"tool-call\",\n              toolCallId: part.id,\n              toolName: part.name,\n              input: JSON.stringify(part.input),\n              providerExecuted: true\n            });\n          }\n          break;\n        }\n        case \"web_search_tool_result\": {\n          if (Array.isArray(part.content)) {\n            content.push({\n              type: \"tool-result\",\n              toolCallId: part.tool_use_id,\n              toolName: \"web_search\",\n              result: part.content.map((result) => {\n                var _a23;\n                return {\n                  url: result.url,\n                  title: result.title,\n                  pageAge: (_a23 = result.page_age) != null ? _a23 : null,\n                  encryptedContent: result.encrypted_content,\n                  type: result.type\n                };\n              }),\n              providerExecuted: true\n            });\n            for (const result of part.content) {\n              content.push({\n                type: \"source\",\n                sourceType: \"url\",\n                id: this.generateId(),\n                url: result.url,\n                title: result.title,\n                providerMetadata: {\n                  anthropic: {\n                    pageAge: (_a16 = result.page_age) != null ? _a16 : null\n                  }\n                }\n              });\n            }\n          } else {\n            content.push({\n              type: \"tool-result\",\n              toolCallId: part.tool_use_id,\n              toolName: \"web_search\",\n              isError: true,\n              result: {\n                type: \"web_search_tool_result_error\",\n                errorCode: part.content.error_code\n              },\n              providerExecuted: true\n            });\n          }\n          break;\n        }\n        case \"code_execution_tool_result\": {\n          if (part.content.type === \"code_execution_result\") {\n            content.push({\n              type: \"tool-result\",\n              toolCallId: part.tool_use_id,\n              toolName: \"code_execution\",\n              result: {\n                type: part.content.type,\n                stdout: part.content.stdout,\n                stderr: part.content.stderr,\n                return_code: part.content.return_code\n              },\n              providerExecuted: true\n            });\n          } else if (part.content.type === \"code_execution_tool_result_error\") {\n            content.push({\n              type: \"tool-result\",\n              toolCallId: part.tool_use_id,\n              toolName: \"code_execution\",\n              isError: true,\n              result: {\n                type: \"code_execution_tool_result_error\",\n                errorCode: part.content.error_code\n              },\n              providerExecuted: true\n            });\n          }\n          break;\n        }\n      }\n    }\n    return {\n      content,\n      finishReason: mapAnthropicStopReason2({\n        finishReason: response.stop_reason,\n        isJsonResponseFromTool: usesJsonResponseTool\n      }),\n      usage: {\n        inputTokens: response.usage.input_tokens,\n        outputTokens: response.usage.output_tokens,\n        totalTokens: response.usage.input_tokens + response.usage.output_tokens,\n        cachedInputTokens: (_b = response.usage.cache_read_input_tokens) != null ? _b : void 0\n      },\n      request: { body: args },\n      response: {\n        id: (_c = response.id) != null ? _c : void 0,\n        modelId: (_d = response.model) != null ? _d : void 0,\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      warnings,\n      providerMetadata: {\n        anthropic: {\n          usage: response.usage,\n          cacheCreationInputTokens: (_e = response.usage.cache_creation_input_tokens) != null ? _e : null\n        }\n      }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings, betas, usesJsonResponseTool } = await this.getArgs(options);\n    const citationDocuments = this.extractCitationDocuments(options.prompt);\n    const body = { ...args, stream: true };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.buildRequestUrl(true),\n      headers: await this.getHeaders({ betas, headers: options.headers }),\n      body: this.transformRequestBody(body),\n      failedResponseHandler: anthropicFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(\n        anthropicMessagesChunkSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    const contentBlocks = {};\n    let providerMetadata = void 0;\n    let blockType = void 0;\n    const generateId3 = this.generateId;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            switch (value.type) {\n              case \"ping\": {\n                return;\n              }\n              case \"content_block_start\": {\n                const contentBlockType = value.content_block.type;\n                blockType = contentBlockType;\n                switch (contentBlockType) {\n                  case \"text\": {\n                    contentBlocks[value.index] = { type: \"text\" };\n                    controller.enqueue({\n                      type: \"text-start\",\n                      id: String(value.index)\n                    });\n                    return;\n                  }\n                  case \"thinking\": {\n                    contentBlocks[value.index] = { type: \"reasoning\" };\n                    controller.enqueue({\n                      type: \"reasoning-start\",\n                      id: String(value.index)\n                    });\n                    return;\n                  }\n                  case \"redacted_thinking\": {\n                    contentBlocks[value.index] = { type: \"reasoning\" };\n                    controller.enqueue({\n                      type: \"reasoning-start\",\n                      id: String(value.index),\n                      providerMetadata: {\n                        anthropic: {\n                          redactedData: value.content_block.data\n                        }\n                      }\n                    });\n                    return;\n                  }\n                  case \"tool_use\": {\n                    contentBlocks[value.index] = usesJsonResponseTool ? { type: \"text\" } : {\n                      type: \"tool-call\",\n                      toolCallId: value.content_block.id,\n                      toolName: value.content_block.name,\n                      input: \"\"\n                    };\n                    controller.enqueue(\n                      usesJsonResponseTool ? { type: \"text-start\", id: String(value.index) } : {\n                        type: \"tool-input-start\",\n                        id: value.content_block.id,\n                        toolName: value.content_block.name\n                      }\n                    );\n                    return;\n                  }\n                  case \"server_tool_use\": {\n                    if (value.content_block.name === \"web_search\" || value.content_block.name === \"code_execution\") {\n                      contentBlocks[value.index] = {\n                        type: \"tool-call\",\n                        toolCallId: value.content_block.id,\n                        toolName: value.content_block.name,\n                        input: \"\",\n                        providerExecuted: true\n                      };\n                      controller.enqueue({\n                        type: \"tool-input-start\",\n                        id: value.content_block.id,\n                        toolName: value.content_block.name,\n                        providerExecuted: true\n                      });\n                    }\n                    return;\n                  }\n                  case \"web_search_tool_result\": {\n                    const part = value.content_block;\n                    if (Array.isArray(part.content)) {\n                      controller.enqueue({\n                        type: \"tool-result\",\n                        toolCallId: part.tool_use_id,\n                        toolName: \"web_search\",\n                        result: part.content.map((result) => {\n                          var _a23;\n                          return {\n                            url: result.url,\n                            title: result.title,\n                            pageAge: (_a23 = result.page_age) != null ? _a23 : null,\n                            encryptedContent: result.encrypted_content,\n                            type: result.type\n                          };\n                        }),\n                        providerExecuted: true\n                      });\n                      for (const result of part.content) {\n                        controller.enqueue({\n                          type: \"source\",\n                          sourceType: \"url\",\n                          id: generateId3(),\n                          url: result.url,\n                          title: result.title,\n                          providerMetadata: {\n                            anthropic: {\n                              pageAge: (_a16 = result.page_age) != null ? _a16 : null\n                            }\n                          }\n                        });\n                      }\n                    } else {\n                      controller.enqueue({\n                        type: \"tool-result\",\n                        toolCallId: part.tool_use_id,\n                        toolName: \"web_search\",\n                        isError: true,\n                        result: {\n                          type: \"web_search_tool_result_error\",\n                          errorCode: part.content.error_code\n                        },\n                        providerExecuted: true\n                      });\n                    }\n                    return;\n                  }\n                  case \"code_execution_tool_result\": {\n                    const part = value.content_block;\n                    if (part.content.type === \"code_execution_result\") {\n                      controller.enqueue({\n                        type: \"tool-result\",\n                        toolCallId: part.tool_use_id,\n                        toolName: \"code_execution\",\n                        result: {\n                          type: part.content.type,\n                          stdout: part.content.stdout,\n                          stderr: part.content.stderr,\n                          return_code: part.content.return_code\n                        },\n                        providerExecuted: true\n                      });\n                    } else if (part.content.type === \"code_execution_tool_result_error\") {\n                      controller.enqueue({\n                        type: \"tool-result\",\n                        toolCallId: part.tool_use_id,\n                        toolName: \"code_execution\",\n                        isError: true,\n                        result: {\n                          type: \"code_execution_tool_result_error\",\n                          errorCode: part.content.error_code\n                        },\n                        providerExecuted: true\n                      });\n                    }\n                    return;\n                  }\n                  default: {\n                    const _exhaustiveCheck = contentBlockType;\n                    throw new Error(\n                      `Unsupported content block type: ${_exhaustiveCheck}`\n                    );\n                  }\n                }\n              }\n              case \"content_block_stop\": {\n                if (contentBlocks[value.index] != null) {\n                  const contentBlock = contentBlocks[value.index];\n                  switch (contentBlock.type) {\n                    case \"text\": {\n                      controller.enqueue({\n                        type: \"text-end\",\n                        id: String(value.index)\n                      });\n                      break;\n                    }\n                    case \"reasoning\": {\n                      controller.enqueue({\n                        type: \"reasoning-end\",\n                        id: String(value.index)\n                      });\n                      break;\n                    }\n                    case \"tool-call\":\n                      if (!usesJsonResponseTool) {\n                        controller.enqueue({\n                          type: \"tool-input-end\",\n                          id: contentBlock.toolCallId\n                        });\n                        controller.enqueue(contentBlock);\n                      }\n                      break;\n                  }\n                  delete contentBlocks[value.index];\n                }\n                blockType = void 0;\n                return;\n              }\n              case \"content_block_delta\": {\n                const deltaType = value.delta.type;\n                switch (deltaType) {\n                  case \"text_delta\": {\n                    if (usesJsonResponseTool) {\n                      return;\n                    }\n                    controller.enqueue({\n                      type: \"text-delta\",\n                      id: String(value.index),\n                      delta: value.delta.text\n                    });\n                    return;\n                  }\n                  case \"thinking_delta\": {\n                    controller.enqueue({\n                      type: \"reasoning-delta\",\n                      id: String(value.index),\n                      delta: value.delta.thinking\n                    });\n                    return;\n                  }\n                  case \"signature_delta\": {\n                    if (blockType === \"thinking\") {\n                      controller.enqueue({\n                        type: \"reasoning-delta\",\n                        id: String(value.index),\n                        delta: \"\",\n                        providerMetadata: {\n                          anthropic: {\n                            signature: value.delta.signature\n                          }\n                        }\n                      });\n                    }\n                    return;\n                  }\n                  case \"input_json_delta\": {\n                    const contentBlock = contentBlocks[value.index];\n                    const delta = value.delta.partial_json;\n                    if (usesJsonResponseTool) {\n                      if ((contentBlock == null ? void 0 : contentBlock.type) !== \"text\") {\n                        return;\n                      }\n                      controller.enqueue({\n                        type: \"text-delta\",\n                        id: String(value.index),\n                        delta\n                      });\n                    } else {\n                      if ((contentBlock == null ? void 0 : contentBlock.type) !== \"tool-call\") {\n                        return;\n                      }\n                      controller.enqueue({\n                        type: \"tool-input-delta\",\n                        id: contentBlock.toolCallId,\n                        delta\n                      });\n                      contentBlock.input += delta;\n                    }\n                    return;\n                  }\n                  case \"citations_delta\": {\n                    const citation = value.delta.citation;\n                    processCitation(\n                      citation,\n                      citationDocuments,\n                      generateId3,\n                      (source) => controller.enqueue(source)\n                    );\n                    return;\n                  }\n                  default: {\n                    const _exhaustiveCheck = deltaType;\n                    throw new Error(\n                      `Unsupported delta type: ${_exhaustiveCheck}`\n                    );\n                  }\n                }\n              }\n              case \"message_start\": {\n                usage.inputTokens = value.message.usage.input_tokens;\n                usage.cachedInputTokens = (_b = value.message.usage.cache_read_input_tokens) != null ? _b : void 0;\n                providerMetadata = {\n                  anthropic: {\n                    usage: value.message.usage,\n                    cacheCreationInputTokens: (_c = value.message.usage.cache_creation_input_tokens) != null ? _c : null\n                  }\n                };\n                controller.enqueue({\n                  type: \"response-metadata\",\n                  id: (_d = value.message.id) != null ? _d : void 0,\n                  modelId: (_e = value.message.model) != null ? _e : void 0\n                });\n                return;\n              }\n              case \"message_delta\": {\n                usage.outputTokens = value.usage.output_tokens;\n                usage.totalTokens = ((_f = usage.inputTokens) != null ? _f : 0) + ((_g = value.usage.output_tokens) != null ? _g : 0);\n                finishReason = mapAnthropicStopReason2({\n                  finishReason: value.delta.stop_reason,\n                  isJsonResponseFromTool: usesJsonResponseTool\n                });\n                return;\n              }\n              case \"message_stop\": {\n                controller.enqueue({\n                  type: \"finish\",\n                  finishReason,\n                  usage,\n                  providerMetadata\n                });\n                return;\n              }\n              case \"error\": {\n                controller.enqueue({ type: \"error\", error: value.error });\n                return;\n              }\n              default: {\n                const _exhaustiveCheck = value;\n                throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n              }\n            }\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar anthropicMessagesResponseSchema2 = z$1.object({\n  type: z$1.literal(\"message\"),\n  id: z$1.string().nullish(),\n  model: z$1.string().nullish(),\n  content: z$1.array(\n    z$1.discriminatedUnion(\"type\", [\n      z$1.object({\n        type: z$1.literal(\"text\"),\n        text: z$1.string(),\n        citations: z$1.array(citationSchema).optional()\n      }),\n      z$1.object({\n        type: z$1.literal(\"thinking\"),\n        thinking: z$1.string(),\n        signature: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"redacted_thinking\"),\n        data: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"tool_use\"),\n        id: z$1.string(),\n        name: z$1.string(),\n        input: z$1.unknown()\n      }),\n      z$1.object({\n        type: z$1.literal(\"server_tool_use\"),\n        id: z$1.string(),\n        name: z$1.string(),\n        input: z$1.record(z$1.string(), z$1.unknown()).nullish()\n      }),\n      z$1.object({\n        type: z$1.literal(\"web_search_tool_result\"),\n        tool_use_id: z$1.string(),\n        content: z$1.union([\n          z$1.array(\n            z$1.object({\n              type: z$1.literal(\"web_search_result\"),\n              url: z$1.string(),\n              title: z$1.string(),\n              encrypted_content: z$1.string(),\n              page_age: z$1.string().nullish()\n            })\n          ),\n          z$1.object({\n            type: z$1.literal(\"web_search_tool_result_error\"),\n            error_code: z$1.string()\n          })\n        ])\n      }),\n      z$1.object({\n        type: z$1.literal(\"code_execution_tool_result\"),\n        tool_use_id: z$1.string(),\n        content: z$1.union([\n          z$1.object({\n            type: z$1.literal(\"code_execution_result\"),\n            stdout: z$1.string(),\n            stderr: z$1.string(),\n            return_code: z$1.number()\n          }),\n          z$1.object({\n            type: z$1.literal(\"code_execution_tool_result_error\"),\n            error_code: z$1.string()\n          })\n        ])\n      })\n    ])\n  ),\n  stop_reason: z$1.string().nullish(),\n  usage: z$1.looseObject({\n    input_tokens: z$1.number(),\n    output_tokens: z$1.number(),\n    cache_creation_input_tokens: z$1.number().nullish(),\n    cache_read_input_tokens: z$1.number().nullish()\n  })\n});\nvar anthropicMessagesChunkSchema2 = z$1.discriminatedUnion(\"type\", [\n  z$1.object({\n    type: z$1.literal(\"message_start\"),\n    message: z$1.object({\n      id: z$1.string().nullish(),\n      model: z$1.string().nullish(),\n      usage: z$1.looseObject({\n        input_tokens: z$1.number(),\n        output_tokens: z$1.number(),\n        cache_creation_input_tokens: z$1.number().nullish(),\n        cache_read_input_tokens: z$1.number().nullish()\n      })\n    })\n  }),\n  z$1.object({\n    type: z$1.literal(\"content_block_start\"),\n    index: z$1.number(),\n    content_block: z$1.discriminatedUnion(\"type\", [\n      z$1.object({\n        type: z$1.literal(\"text\"),\n        text: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"thinking\"),\n        thinking: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"tool_use\"),\n        id: z$1.string(),\n        name: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"redacted_thinking\"),\n        data: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"server_tool_use\"),\n        id: z$1.string(),\n        name: z$1.string(),\n        input: z$1.record(z$1.string(), z$1.unknown()).nullish()\n      }),\n      z$1.object({\n        type: z$1.literal(\"web_search_tool_result\"),\n        tool_use_id: z$1.string(),\n        content: z$1.union([\n          z$1.array(\n            z$1.object({\n              type: z$1.literal(\"web_search_result\"),\n              url: z$1.string(),\n              title: z$1.string(),\n              encrypted_content: z$1.string(),\n              page_age: z$1.string().nullish()\n            })\n          ),\n          z$1.object({\n            type: z$1.literal(\"web_search_tool_result_error\"),\n            error_code: z$1.string()\n          })\n        ])\n      }),\n      z$1.object({\n        type: z$1.literal(\"code_execution_tool_result\"),\n        tool_use_id: z$1.string(),\n        content: z$1.union([\n          z$1.object({\n            type: z$1.literal(\"code_execution_result\"),\n            stdout: z$1.string(),\n            stderr: z$1.string(),\n            return_code: z$1.number()\n          }),\n          z$1.object({\n            type: z$1.literal(\"code_execution_tool_result_error\"),\n            error_code: z$1.string()\n          })\n        ])\n      })\n    ])\n  }),\n  z$1.object({\n    type: z$1.literal(\"content_block_delta\"),\n    index: z$1.number(),\n    delta: z$1.discriminatedUnion(\"type\", [\n      z$1.object({\n        type: z$1.literal(\"input_json_delta\"),\n        partial_json: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"text_delta\"),\n        text: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"thinking_delta\"),\n        thinking: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"signature_delta\"),\n        signature: z$1.string()\n      }),\n      z$1.object({\n        type: z$1.literal(\"citations_delta\"),\n        citation: citationSchema\n      })\n    ])\n  }),\n  z$1.object({\n    type: z$1.literal(\"content_block_stop\"),\n    index: z$1.number()\n  }),\n  z$1.object({\n    type: z$1.literal(\"error\"),\n    error: z$1.object({\n      type: z$1.string(),\n      message: z$1.string()\n    })\n  }),\n  z$1.object({\n    type: z$1.literal(\"message_delta\"),\n    delta: z$1.object({ stop_reason: z$1.string().nullish() }),\n    usage: z$1.object({ output_tokens: z$1.number() })\n  }),\n  z$1.object({\n    type: z$1.literal(\"message_stop\")\n  }),\n  z$1.object({\n    type: z$1.literal(\"ping\")\n  })\n]);\nvar anthropicReasoningMetadataSchema = z$1.object({\n  signature: z$1.string().optional(),\n  redactedData: z$1.string().optional()\n});\nvar bash_20241022 = createProviderDefinedToolFactory({\n  id: \"anthropic.bash_20241022\",\n  name: \"bash\",\n  inputSchema: z62.object({\n    command: z62.string(),\n    restart: z62.boolean().optional()\n  })\n});\nvar bash_20250124 = createProviderDefinedToolFactory({\n  id: \"anthropic.bash_20250124\",\n  name: \"bash\",\n  inputSchema: z62.object({\n    command: z62.string(),\n    restart: z62.boolean().optional()\n  })\n});\nvar computer_20241022 = createProviderDefinedToolFactory({\n  id: \"anthropic.computer_20241022\",\n  name: \"computer\",\n  inputSchema: z$1.object({\n    action: z$1.enum([\n      \"key\",\n      \"type\",\n      \"mouse_move\",\n      \"left_click\",\n      \"left_click_drag\",\n      \"right_click\",\n      \"middle_click\",\n      \"double_click\",\n      \"screenshot\",\n      \"cursor_position\"\n    ]),\n    coordinate: z$1.array(z$1.number().int()).optional(),\n    text: z$1.string().optional()\n  })\n});\nvar computer_20250124 = createProviderDefinedToolFactory({\n  id: \"anthropic.computer_20250124\",\n  name: \"computer\",\n  inputSchema: z$1.object({\n    action: z$1.enum([\n      \"key\",\n      \"hold_key\",\n      \"type\",\n      \"cursor_position\",\n      \"mouse_move\",\n      \"left_mouse_down\",\n      \"left_mouse_up\",\n      \"left_click\",\n      \"left_click_drag\",\n      \"right_click\",\n      \"middle_click\",\n      \"double_click\",\n      \"triple_click\",\n      \"scroll\",\n      \"wait\",\n      \"screenshot\"\n    ]),\n    coordinate: z$1.tuple([z$1.number().int(), z$1.number().int()]).optional(),\n    duration: z$1.number().optional(),\n    scroll_amount: z$1.number().optional(),\n    scroll_direction: z$1.enum([\"up\", \"down\", \"left\", \"right\"]).optional(),\n    start_coordinate: z$1.tuple([z$1.number().int(), z$1.number().int()]).optional(),\n    text: z$1.string().optional()\n  })\n});\nvar textEditor_20241022 = createProviderDefinedToolFactory({\n  id: \"anthropic.text_editor_20241022\",\n  name: \"str_replace_editor\",\n  inputSchema: z$1.object({\n    command: z$1.enum([\"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"]),\n    path: z$1.string(),\n    file_text: z$1.string().optional(),\n    insert_line: z$1.number().int().optional(),\n    new_str: z$1.string().optional(),\n    old_str: z$1.string().optional(),\n    view_range: z$1.array(z$1.number().int()).optional()\n  })\n});\nvar textEditor_20250124 = createProviderDefinedToolFactory({\n  id: \"anthropic.text_editor_20250124\",\n  name: \"str_replace_editor\",\n  inputSchema: z$1.object({\n    command: z$1.enum([\"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"]),\n    path: z$1.string(),\n    file_text: z$1.string().optional(),\n    insert_line: z$1.number().int().optional(),\n    new_str: z$1.string().optional(),\n    old_str: z$1.string().optional(),\n    view_range: z$1.array(z$1.number().int()).optional()\n  })\n});\nvar textEditor_20250429 = createProviderDefinedToolFactory({\n  id: \"anthropic.text_editor_20250429\",\n  name: \"str_replace_based_edit_tool\",\n  inputSchema: z$1.object({\n    command: z$1.enum([\"view\", \"create\", \"str_replace\", \"insert\"]),\n    path: z$1.string(),\n    file_text: z$1.string().optional(),\n    insert_line: z$1.number().int().optional(),\n    new_str: z$1.string().optional(),\n    old_str: z$1.string().optional(),\n    view_range: z$1.array(z$1.number().int()).optional()\n  })\n});\nvar anthropicTools2 = {\n  /**\n   * Creates a tool for running a bash command. Must have name \"bash\".\n   *\n   * Image results are supported.\n   *\n   * @param execute - The function to execute the tool. Optional.\n   */\n  bash_20241022,\n  /**\n   * Creates a tool for running a bash command. Must have name \"bash\".\n   *\n   * Image results are supported.\n   *\n   * @param execute - The function to execute the tool. Optional.\n   */\n  bash_20250124,\n  /**\n   * Creates a tool for editing text. Must have name \"str_replace_editor\".\n   */\n  textEditor_20241022,\n  /**\n   * Creates a tool for editing text. Must have name \"str_replace_editor\".\n   */\n  textEditor_20250124,\n  /**\n   * Creates a tool for editing text. Must have name \"str_replace_based_edit_tool\".\n   * Note: This version does not support the \"undo_edit\" command.\n   */\n  textEditor_20250429,\n  /**\n   * Creates a tool for executing actions on a computer. Must have name \"computer\".\n   *\n   * Image results are supported.\n   *\n   * @param displayWidthPx - The width of the display being controlled by the model in pixels.\n   * @param displayHeightPx - The height of the display being controlled by the model in pixels.\n   * @param displayNumber - The display number to control (only relevant for X11 environments). If specified, the tool will be provided a display number in the tool definition.\n   */\n  computer_20241022,\n  /**\n   * Creates a tool for executing actions on a computer. Must have name \"computer\".\n   *\n   * Image results are supported.\n   *\n   * @param displayWidthPx - The width of the display being controlled by the model in pixels.\n   * @param displayHeightPx - The height of the display being controlled by the model in pixels.\n   * @param displayNumber - The display number to control (only relevant for X11 environments). If specified, the tool will be provided a display number in the tool definition.\n   * @param execute - The function to execute the tool. Optional.\n   */\n  computer_20250124,\n  /**\n   * Creates a web search tool that gives Claude direct access to real-time web content.\n   * Must have name \"web_search\".\n   *\n   * @param maxUses - Maximum number of web searches Claude can perform during the conversation.\n   * @param allowedDomains - Optional list of domains that Claude is allowed to search.\n   * @param blockedDomains - Optional list of domains that Claude should avoid when searching.\n   * @param userLocation - Optional user location information to provide geographically relevant search results.\n   */\n  webSearch_20250305,\n  /**\n   * Creates a tool for executing Python code. Must have name \"code_execution\".\n   */\n  codeExecution_20250522\n};\nfunction createAnthropic2(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash2(options.baseURL)) != null ? _a16 : \"https://api.anthropic.com/v1\";\n  const getHeaders = () => ({\n    \"anthropic-version\": \"2023-06-01\",\n    \"x-api-key\": loadApiKey2({\n      apiKey: options.apiKey,\n      environmentVariableName: \"ANTHROPIC_API_KEY\",\n      description: \"Anthropic\"\n    }),\n    ...options.headers\n  });\n  const createChatModel = (modelId) => {\n    var _a23;\n    return new AnthropicMessagesLanguageModel2(modelId, {\n      provider: \"anthropic.messages\",\n      baseURL,\n      headers: getHeaders,\n      fetch: options.fetch,\n      generateId: (_a23 = options.generateId) != null ? _a23 : generateId2,\n      supportedUrls: () => ({\n        \"image/*\": [/^https?:\\/\\/.*$/]\n      })\n    });\n  };\n  const provider = function(modelId) {\n    if (new.target) {\n      throw new Error(\n        \"The Anthropic model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId);\n  };\n  provider.languageModel = createChatModel;\n  provider.chat = createChatModel;\n  provider.messages = createChatModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError2({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.imageModel = (modelId) => {\n    throw new NoSuchModelError2({ modelId, modelType: \"imageModel\" });\n  };\n  provider.tools = anthropicTools2;\n  return provider;\n}\nvar anthropic2 = createAnthropic2();\nfunction convertJSONSchemaToOpenAPISchema(jsonSchema) {\n  if (isEmptyObjectSchema(jsonSchema)) {\n    return void 0;\n  }\n  if (typeof jsonSchema === \"boolean\") {\n    return { type: \"boolean\", properties: {} };\n  }\n  const {\n    type,\n    description,\n    required,\n    properties,\n    items,\n    allOf,\n    anyOf,\n    oneOf,\n    format,\n    const: constValue,\n    minLength,\n    enum: enumValues\n  } = jsonSchema;\n  const result = {};\n  if (description)\n    result.description = description;\n  if (required)\n    result.required = required;\n  if (format)\n    result.format = format;\n  if (constValue !== void 0) {\n    result.enum = [constValue];\n  }\n  if (type) {\n    if (Array.isArray(type)) {\n      if (type.includes(\"null\")) {\n        result.type = type.filter((t) => t !== \"null\")[0];\n        result.nullable = true;\n      } else {\n        result.type = type;\n      }\n    } else if (type === \"null\") {\n      result.type = \"null\";\n    } else {\n      result.type = type;\n    }\n  }\n  if (enumValues !== void 0) {\n    result.enum = enumValues;\n  }\n  if (properties != null) {\n    result.properties = Object.entries(properties).reduce(\n      (acc, [key, value]) => {\n        acc[key] = convertJSONSchemaToOpenAPISchema(value);\n        return acc;\n      },\n      {}\n    );\n  }\n  if (items) {\n    result.items = Array.isArray(items) ? items.map(convertJSONSchemaToOpenAPISchema) : convertJSONSchemaToOpenAPISchema(items);\n  }\n  if (allOf) {\n    result.allOf = allOf.map(convertJSONSchemaToOpenAPISchema);\n  }\n  if (anyOf) {\n    if (anyOf.some(\n      (schema) => typeof schema === \"object\" && (schema == null ? void 0 : schema.type) === \"null\"\n    )) {\n      const nonNullSchemas = anyOf.filter(\n        (schema) => !(typeof schema === \"object\" && (schema == null ? void 0 : schema.type) === \"null\")\n      );\n      if (nonNullSchemas.length === 1) {\n        const converted = convertJSONSchemaToOpenAPISchema(nonNullSchemas[0]);\n        if (typeof converted === \"object\") {\n          result.nullable = true;\n          Object.assign(result, converted);\n        }\n      } else {\n        result.anyOf = nonNullSchemas.map(convertJSONSchemaToOpenAPISchema);\n        result.nullable = true;\n      }\n    } else {\n      result.anyOf = anyOf.map(convertJSONSchemaToOpenAPISchema);\n    }\n  }\n  if (oneOf) {\n    result.oneOf = oneOf.map(convertJSONSchemaToOpenAPISchema);\n  }\n  if (minLength !== void 0) {\n    result.minLength = minLength;\n  }\n  return result;\n}\nfunction isEmptyObjectSchema(jsonSchema) {\n  return jsonSchema != null && typeof jsonSchema === \"object\" && jsonSchema.type === \"object\" && (jsonSchema.properties == null || Object.keys(jsonSchema.properties).length === 0) && !jsonSchema.additionalProperties;\n}\nfunction convertToGoogleGenerativeAIMessages(prompt) {\n  var _a16, _b;\n  const systemInstructionParts = [];\n  const contents = [];\n  let systemMessagesAllowed = true;\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        if (!systemMessagesAllowed) {\n          throw new UnsupportedFunctionalityError({\n            functionality: \"system messages are only supported at the beginning of the conversation\"\n          });\n        }\n        systemInstructionParts.push({ text: content });\n        break;\n      }\n      case \"user\": {\n        systemMessagesAllowed = false;\n        const parts = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              parts.push({ text: part.text });\n              break;\n            }\n            case \"image\": {\n              parts.push(\n                part.image instanceof URL ? {\n                  fileData: {\n                    mimeType: (_a16 = part.mimeType) != null ? _a16 : \"image/jpeg\",\n                    fileUri: part.image.toString()\n                  }\n                } : {\n                  inlineData: {\n                    mimeType: (_b = part.mimeType) != null ? _b : \"image/jpeg\",\n                    data: convertUint8ArrayToBase64(part.image)\n                  }\n                }\n              );\n              break;\n            }\n            case \"file\": {\n              parts.push(\n                part.data instanceof URL ? {\n                  fileData: {\n                    mimeType: part.mimeType,\n                    fileUri: part.data.toString()\n                  }\n                } : {\n                  inlineData: {\n                    mimeType: part.mimeType,\n                    data: part.data\n                  }\n                }\n              );\n              break;\n            }\n          }\n        }\n        contents.push({ role: \"user\", parts });\n        break;\n      }\n      case \"assistant\": {\n        systemMessagesAllowed = false;\n        contents.push({\n          role: \"model\",\n          parts: content.map((part) => {\n            switch (part.type) {\n              case \"text\": {\n                return part.text.length === 0 ? void 0 : { text: part.text };\n              }\n              case \"file\": {\n                if (part.mimeType !== \"image/png\") {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: \"Only PNG images are supported in assistant messages\"\n                  });\n                }\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: \"File data URLs in assistant messages are not supported\"\n                  });\n                }\n                return {\n                  inlineData: {\n                    mimeType: part.mimeType,\n                    data: part.data\n                  }\n                };\n              }\n              case \"tool-call\": {\n                return {\n                  functionCall: {\n                    name: part.toolName,\n                    args: part.args\n                  }\n                };\n              }\n            }\n          }).filter((part) => part !== void 0)\n        });\n        break;\n      }\n      case \"tool\": {\n        systemMessagesAllowed = false;\n        contents.push({\n          role: \"user\",\n          parts: content.map((part) => ({\n            functionResponse: {\n              name: part.toolName,\n              response: {\n                name: part.toolName,\n                content: part.result\n              }\n            }\n          }))\n        });\n        break;\n      }\n    }\n  }\n  return {\n    systemInstruction: systemInstructionParts.length > 0 ? { parts: systemInstructionParts } : void 0,\n    contents\n  };\n}\nfunction getModelPath(modelId) {\n  return modelId.includes(\"/\") ? modelId : `models/${modelId}`;\n}\nvar googleErrorDataSchema = z.object({\n  error: z.object({\n    code: z.number().nullable(),\n    message: z.string(),\n    status: z.string()\n  })\n});\nvar googleFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: googleErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\nfunction prepareTools3(mode, useSearchGrounding, dynamicRetrievalConfig, modelId) {\n  var _a16, _b;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  const isGemini2 = modelId.includes(\"gemini-2\");\n  const supportsDynamicRetrieval = modelId.includes(\"gemini-1.5-flash\") && !modelId.includes(\"-8b\");\n  if (useSearchGrounding) {\n    return {\n      tools: isGemini2 ? { googleSearch: {} } : {\n        googleSearchRetrieval: !supportsDynamicRetrieval || !dynamicRetrievalConfig ? {} : { dynamicRetrievalConfig }\n      },\n      toolConfig: void 0,\n      toolWarnings\n    };\n  }\n  if (tools == null) {\n    return { tools: void 0, toolConfig: void 0, toolWarnings };\n  }\n  const functionDeclarations = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n    } else {\n      functionDeclarations.push({\n        name: tool2.name,\n        description: (_b = tool2.description) != null ? _b : \"\",\n        parameters: convertJSONSchemaToOpenAPISchema(tool2.parameters)\n      });\n    }\n  }\n  const toolChoice = mode.toolChoice;\n  if (toolChoice == null) {\n    return {\n      tools: { functionDeclarations },\n      toolConfig: void 0,\n      toolWarnings\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"AUTO\" } },\n        toolWarnings\n      };\n    case \"none\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"NONE\" } },\n        toolWarnings\n      };\n    case \"required\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"ANY\" } },\n        toolWarnings\n      };\n    case \"tool\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: {\n          functionCallingConfig: {\n            mode: \"ANY\",\n            allowedFunctionNames: [toolChoice.toolName]\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nfunction mapGoogleGenerativeAIFinishReason({\n  finishReason,\n  hasToolCalls\n}) {\n  switch (finishReason) {\n    case \"STOP\":\n      return hasToolCalls ? \"tool-calls\" : \"stop\";\n    case \"MAX_TOKENS\":\n      return \"length\";\n    case \"IMAGE_SAFETY\":\n    case \"RECITATION\":\n    case \"SAFETY\":\n    case \"BLOCKLIST\":\n    case \"PROHIBITED_CONTENT\":\n    case \"SPII\":\n      return \"content-filter\";\n    case \"FINISH_REASON_UNSPECIFIED\":\n    case \"OTHER\":\n      return \"other\";\n    case \"MALFORMED_FUNCTION_CALL\":\n      return \"error\";\n    default:\n      return \"unknown\";\n  }\n}\nvar GoogleGenerativeAILanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = \"json\";\n    this.supportsImageUrls = false;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get supportsStructuredOutputs() {\n    var _a16;\n    return (_a16 = this.settings.structuredOutputs) != null ? _a16 : true;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    providerMetadata\n  }) {\n    var _a16, _b, _c;\n    const type = mode.type;\n    const warnings = [];\n    const googleOptions = parseProviderOptions({\n      provider: \"google\",\n      providerOptions: providerMetadata,\n      schema: googleGenerativeAIProviderOptionsSchema\n    });\n    if (((_a16 = googleOptions == null ? void 0 : googleOptions.thinkingConfig) == null ? void 0 : _a16.includeThoughts) === true && !this.config.provider.startsWith(\"google.vertex.\")) {\n      warnings.push({\n        type: \"other\",\n        message: `The 'includeThoughts' option is only supported with the Google Vertex provider and might not be supported or could behave unexpectedly with the current Google provider (${this.config.provider}).`\n      });\n    }\n    const generationConfig = {\n      // standardized settings:\n      maxOutputTokens: maxTokens,\n      temperature,\n      topK,\n      topP,\n      frequencyPenalty,\n      presencePenalty,\n      stopSequences,\n      seed,\n      // response format:\n      responseMimeType: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? \"application/json\" : void 0,\n      responseSchema: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && // Google GenAI does not support all OpenAPI Schema features,\n      // so this is needed as an escape hatch:\n      this.supportsStructuredOutputs ? convertJSONSchemaToOpenAPISchema(responseFormat.schema) : void 0,\n      ...this.settings.audioTimestamp && {\n        audioTimestamp: this.settings.audioTimestamp\n      },\n      // provider options:\n      responseModalities: googleOptions == null ? void 0 : googleOptions.responseModalities,\n      thinkingConfig: googleOptions == null ? void 0 : googleOptions.thinkingConfig\n    };\n    const { contents, systemInstruction } = convertToGoogleGenerativeAIMessages(prompt);\n    switch (type) {\n      case \"regular\": {\n        const { tools, toolConfig, toolWarnings } = prepareTools3(\n          mode,\n          (_b = this.settings.useSearchGrounding) != null ? _b : false,\n          this.settings.dynamicRetrievalConfig,\n          this.modelId\n        );\n        return {\n          args: {\n            generationConfig,\n            contents,\n            systemInstruction,\n            safetySettings: this.settings.safetySettings,\n            tools,\n            toolConfig,\n            cachedContent: this.settings.cachedContent\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            generationConfig: {\n              ...generationConfig,\n              responseMimeType: \"application/json\",\n              responseSchema: mode.schema != null && // Google GenAI does not support all OpenAPI Schema features,\n              // so this is needed as an escape hatch:\n              this.supportsStructuredOutputs ? convertJSONSchemaToOpenAPISchema(mode.schema) : void 0\n            },\n            contents,\n            systemInstruction,\n            safetySettings: this.settings.safetySettings,\n            cachedContent: this.settings.cachedContent\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: {\n            generationConfig,\n            contents,\n            systemInstruction,\n            tools: {\n              functionDeclarations: [\n                {\n                  name: mode.tool.name,\n                  description: (_c = mode.tool.description) != null ? _c : \"\",\n                  parameters: convertJSONSchemaToOpenAPISchema(\n                    mode.tool.parameters\n                  )\n                }\n              ]\n            },\n            toolConfig: { functionCallingConfig: { mode: \"ANY\" } },\n            safetySettings: this.settings.safetySettings,\n            cachedContent: this.settings.cachedContent\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  supportsUrl(url) {\n    return this.config.isSupportedUrl(url);\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e;\n    const { args, warnings } = await this.getArgs(options);\n    const body = JSON.stringify(args);\n    const mergedHeaders = combineHeaders(\n      await resolve(this.config.headers),\n      options.headers\n    );\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: `${this.config.baseURL}/${getModelPath(\n        this.modelId\n      )}:generateContent`,\n      headers: mergedHeaders,\n      body: args,\n      failedResponseHandler: googleFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(responseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { contents: rawPrompt, ...rawSettings } = args;\n    const candidate = response.candidates[0];\n    const parts = candidate.content == null || typeof candidate.content !== \"object\" || !(\"parts\" in candidate.content) ? [] : candidate.content.parts;\n    const toolCalls = getToolCallsFromParts({\n      parts,\n      // Use candidateParts\n      generateId: this.config.generateId\n    });\n    const usageMetadata = response.usageMetadata;\n    return {\n      text: getTextFromParts(parts),\n      reasoning: getReasoningDetailsFromParts(parts),\n      files: (_a16 = getInlineDataParts(parts)) == null ? void 0 : _a16.map((part) => ({\n        data: part.inlineData.data,\n        mimeType: part.inlineData.mimeType\n      })),\n      toolCalls,\n      finishReason: mapGoogleGenerativeAIFinishReason({\n        finishReason: candidate.finishReason,\n        hasToolCalls: toolCalls != null && toolCalls.length > 0\n      }),\n      usage: {\n        promptTokens: (_b = usageMetadata == null ? void 0 : usageMetadata.promptTokenCount) != null ? _b : NaN,\n        completionTokens: (_c = usageMetadata == null ? void 0 : usageMetadata.candidatesTokenCount) != null ? _c : NaN\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      warnings,\n      providerMetadata: {\n        google: {\n          groundingMetadata: (_d = candidate.groundingMetadata) != null ? _d : null,\n          safetyRatings: (_e = candidate.safetyRatings) != null ? _e : null\n        }\n      },\n      sources: extractSources({\n        groundingMetadata: candidate.groundingMetadata,\n        generateId: this.config.generateId\n      }),\n      request: { body }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = await this.getArgs(options);\n    const body = JSON.stringify(args);\n    const headers = combineHeaders(\n      await resolve(this.config.headers),\n      options.headers\n    );\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: `${this.config.baseURL}/${getModelPath(\n        this.modelId\n      )}:streamGenerateContent?alt=sse`,\n      headers,\n      body: args,\n      failedResponseHandler: googleFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(chunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { contents: rawPrompt, ...rawSettings } = args;\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    let providerMetadata = void 0;\n    const generateId22 = this.config.generateId;\n    let hasToolCalls = false;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f;\n            if (!chunk.success) {\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            const usageMetadata = value.usageMetadata;\n            if (usageMetadata != null) {\n              usage = {\n                promptTokens: (_a16 = usageMetadata.promptTokenCount) != null ? _a16 : NaN,\n                completionTokens: (_b = usageMetadata.candidatesTokenCount) != null ? _b : NaN\n              };\n            }\n            const candidate = (_c = value.candidates) == null ? void 0 : _c[0];\n            if (candidate == null) {\n              return;\n            }\n            const content = candidate.content;\n            if (content != null) {\n              const deltaText = getTextFromParts(content.parts);\n              if (deltaText != null) {\n                controller.enqueue({\n                  type: \"text-delta\",\n                  textDelta: deltaText\n                });\n              }\n              const reasoningDeltaText = getReasoningDetailsFromParts(\n                content.parts\n              );\n              if (reasoningDeltaText != null) {\n                for (const part of reasoningDeltaText) {\n                  controller.enqueue({\n                    type: \"reasoning\",\n                    textDelta: part.text\n                  });\n                }\n              }\n              const inlineDataParts = getInlineDataParts(content.parts);\n              if (inlineDataParts != null) {\n                for (const part of inlineDataParts) {\n                  controller.enqueue({\n                    type: \"file\",\n                    mimeType: part.inlineData.mimeType,\n                    data: part.inlineData.data\n                  });\n                }\n              }\n              const toolCallDeltas = getToolCallsFromParts({\n                parts: content.parts,\n                generateId: generateId22\n              });\n              if (toolCallDeltas != null) {\n                for (const toolCall of toolCallDeltas) {\n                  controller.enqueue({\n                    type: \"tool-call-delta\",\n                    toolCallType: \"function\",\n                    toolCallId: toolCall.toolCallId,\n                    toolName: toolCall.toolName,\n                    argsTextDelta: toolCall.args\n                  });\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: toolCall.toolCallId,\n                    toolName: toolCall.toolName,\n                    args: toolCall.args\n                  });\n                  hasToolCalls = true;\n                }\n              }\n            }\n            if (candidate.finishReason != null) {\n              finishReason = mapGoogleGenerativeAIFinishReason({\n                finishReason: candidate.finishReason,\n                hasToolCalls\n              });\n              const sources = (_d = extractSources({\n                groundingMetadata: candidate.groundingMetadata,\n                generateId: generateId22\n              })) != null ? _d : [];\n              for (const source of sources) {\n                controller.enqueue({ type: \"source\", source });\n              }\n              providerMetadata = {\n                google: {\n                  groundingMetadata: (_e = candidate.groundingMetadata) != null ? _e : null,\n                  safetyRatings: (_f = candidate.safetyRatings) != null ? _f : null\n                }\n              };\n            }\n          },\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n              providerMetadata\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body }\n    };\n  }\n};\nfunction getToolCallsFromParts({\n  parts,\n  generateId: generateId22\n}) {\n  const functionCallParts = parts == null ? void 0 : parts.filter(\n    (part) => \"functionCall\" in part\n  );\n  return functionCallParts == null || functionCallParts.length === 0 ? void 0 : functionCallParts.map((part) => ({\n    toolCallType: \"function\",\n    toolCallId: generateId22(),\n    toolName: part.functionCall.name,\n    args: JSON.stringify(part.functionCall.args)\n  }));\n}\nfunction getTextFromParts(parts) {\n  const textParts = parts == null ? void 0 : parts.filter(\n    (part) => \"text\" in part && part.thought !== true\n  );\n  return textParts == null || textParts.length === 0 ? void 0 : textParts.map((part) => part.text).join(\"\");\n}\nfunction getReasoningDetailsFromParts(parts) {\n  const reasoningParts = parts == null ? void 0 : parts.filter(\n    (part) => \"text\" in part && part.thought === true && part.text != null\n  );\n  return reasoningParts == null || reasoningParts.length === 0 ? void 0 : reasoningParts.map((part) => ({ type: \"text\", text: part.text }));\n}\nfunction getInlineDataParts(parts) {\n  return parts == null ? void 0 : parts.filter(\n    (part) => \"inlineData\" in part\n  );\n}\nfunction extractSources({\n  groundingMetadata,\n  generateId: generateId22\n}) {\n  var _a16;\n  return (_a16 = groundingMetadata == null ? void 0 : groundingMetadata.groundingChunks) == null ? void 0 : _a16.filter(\n    (chunk) => chunk.web != null\n  ).map((chunk) => ({\n    sourceType: \"url\",\n    id: generateId22(),\n    url: chunk.web.uri,\n    title: chunk.web.title\n  }));\n}\nvar contentSchema = z.object({\n  parts: z.array(\n    z.union([\n      // note: order matters since text can be fully empty\n      z.object({\n        functionCall: z.object({\n          name: z.string(),\n          args: z.unknown()\n        })\n      }),\n      z.object({\n        inlineData: z.object({\n          mimeType: z.string(),\n          data: z.string()\n        })\n      }),\n      z.object({\n        text: z.string().nullish(),\n        thought: z.boolean().nullish()\n      })\n    ])\n  ).nullish()\n});\nvar groundingChunkSchema = z.object({\n  web: z.object({ uri: z.string(), title: z.string() }).nullish(),\n  retrievedContext: z.object({ uri: z.string(), title: z.string() }).nullish()\n});\nvar groundingMetadataSchema = z.object({\n  webSearchQueries: z.array(z.string()).nullish(),\n  retrievalQueries: z.array(z.string()).nullish(),\n  searchEntryPoint: z.object({ renderedContent: z.string() }).nullish(),\n  groundingChunks: z.array(groundingChunkSchema).nullish(),\n  groundingSupports: z.array(\n    z.object({\n      segment: z.object({\n        startIndex: z.number().nullish(),\n        endIndex: z.number().nullish(),\n        text: z.string().nullish()\n      }),\n      segment_text: z.string().nullish(),\n      groundingChunkIndices: z.array(z.number()).nullish(),\n      supportChunkIndices: z.array(z.number()).nullish(),\n      confidenceScores: z.array(z.number()).nullish(),\n      confidenceScore: z.array(z.number()).nullish()\n    })\n  ).nullish(),\n  retrievalMetadata: z.union([\n    z.object({\n      webDynamicRetrievalScore: z.number()\n    }),\n    z.object({})\n  ]).nullish()\n});\nvar safetyRatingSchema = z.object({\n  category: z.string().nullish(),\n  probability: z.string().nullish(),\n  probabilityScore: z.number().nullish(),\n  severity: z.string().nullish(),\n  severityScore: z.number().nullish(),\n  blocked: z.boolean().nullish()\n});\nvar responseSchema = z.object({\n  candidates: z.array(\n    z.object({\n      content: contentSchema.nullish().or(z.object({}).strict()),\n      finishReason: z.string().nullish(),\n      safetyRatings: z.array(safetyRatingSchema).nullish(),\n      groundingMetadata: groundingMetadataSchema.nullish()\n    })\n  ),\n  usageMetadata: z.object({\n    promptTokenCount: z.number().nullish(),\n    candidatesTokenCount: z.number().nullish(),\n    totalTokenCount: z.number().nullish()\n  }).nullish()\n});\nvar chunkSchema = z.object({\n  candidates: z.array(\n    z.object({\n      content: contentSchema.nullish(),\n      finishReason: z.string().nullish(),\n      safetyRatings: z.array(safetyRatingSchema).nullish(),\n      groundingMetadata: groundingMetadataSchema.nullish()\n    })\n  ).nullish(),\n  usageMetadata: z.object({\n    promptTokenCount: z.number().nullish(),\n    candidatesTokenCount: z.number().nullish(),\n    totalTokenCount: z.number().nullish()\n  }).nullish()\n});\nvar googleGenerativeAIProviderOptionsSchema = z.object({\n  responseModalities: z.array(z.enum([\"TEXT\", \"IMAGE\"])).nullish(),\n  thinkingConfig: z.object({\n    thinkingBudget: z.number().nullish(),\n    includeThoughts: z.boolean().nullish()\n  }).nullish()\n});\nvar GoogleGenerativeAIEmbeddingModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get maxEmbeddingsPerCall() {\n    return 2048;\n  }\n  get supportsParallelCalls() {\n    return true;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal\n  }) {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const mergedHeaders = combineHeaders(\n      await resolve(this.config.headers),\n      headers\n    );\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: `${this.config.baseURL}/models/${this.modelId}:batchEmbedContents`,\n      headers: mergedHeaders,\n      body: {\n        requests: values.map((value) => ({\n          model: `models/${this.modelId}`,\n          content: { role: \"user\", parts: [{ text: value }] },\n          outputDimensionality: this.settings.outputDimensionality,\n          taskType: this.settings.taskType\n        }))\n      },\n      failedResponseHandler: googleFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        googleGenerativeAITextEmbeddingResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.embeddings.map((item) => item.values),\n      usage: void 0,\n      rawResponse: { headers: responseHeaders }\n    };\n  }\n};\nvar googleGenerativeAITextEmbeddingResponseSchema = z.object({\n  embeddings: z.array(z.object({ values: z.array(z.number()) }))\n});\nfunction isSupportedFileUrl(url) {\n  return url.toString().startsWith(\"https://generativelanguage.googleapis.com/v1beta/files/\");\n}\nfunction createGoogleGenerativeAI(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash(options.baseURL)) != null ? _a16 : \"https://generativelanguage.googleapis.com/v1beta\";\n  const getHeaders = () => ({\n    \"x-goog-api-key\": loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"GOOGLE_GENERATIVE_AI_API_KEY\",\n      description: \"Google Generative AI\"\n    }),\n    ...options.headers\n  });\n  const createChatModel = (modelId, settings = {}) => {\n    var _a23;\n    return new GoogleGenerativeAILanguageModel(modelId, settings, {\n      provider: \"google.generative-ai\",\n      baseURL,\n      headers: getHeaders,\n      generateId: (_a23 = options.generateId) != null ? _a23 : generateId,\n      isSupportedUrl: isSupportedFileUrl,\n      fetch: options.fetch\n    });\n  };\n  const createEmbeddingModel = (modelId, settings = {}) => new GoogleGenerativeAIEmbeddingModel(modelId, settings, {\n    provider: \"google.generative-ai\",\n    baseURL,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const provider = function(modelId, settings) {\n    if (new.target) {\n      throw new Error(\n        \"The Google Generative AI model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId, settings);\n  };\n  provider.languageModel = createChatModel;\n  provider.chat = createChatModel;\n  provider.generativeAI = createChatModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  return provider;\n}\nvar google = createGoogleGenerativeAI();\nvar googleErrorDataSchema2 = z$1.object({\n  error: z$1.object({\n    code: z$1.number().nullable(),\n    message: z$1.string(),\n    status: z$1.string()\n  })\n});\nvar googleFailedResponseHandler2 = createJsonErrorResponseHandler2({\n  errorSchema: googleErrorDataSchema2,\n  errorToMessage: (data) => data.error.message\n});\nvar googleGenerativeAIEmbeddingProviderOptions = z$1.object({\n  /**\n   * Optional. Optional reduced dimension for the output embedding.\n   * If set, excessive values in the output embedding are truncated from the end.\n   */\n  outputDimensionality: z$1.number().optional(),\n  /**\n   * Optional. Specifies the task type for generating embeddings.\n   * Supported task types:\n   * - SEMANTIC_SIMILARITY: Optimized for text similarity.\n   * - CLASSIFICATION: Optimized for text classification.\n   * - CLUSTERING: Optimized for clustering texts based on similarity.\n   * - RETRIEVAL_DOCUMENT: Optimized for document retrieval.\n   * - RETRIEVAL_QUERY: Optimized for query-based retrieval.\n   * - QUESTION_ANSWERING: Optimized for answering questions.\n   * - FACT_VERIFICATION: Optimized for verifying factual information.\n   * - CODE_RETRIEVAL_QUERY: Optimized for retrieving code blocks based on natural language queries.\n   */\n  taskType: z$1.enum([\n    \"SEMANTIC_SIMILARITY\",\n    \"CLASSIFICATION\",\n    \"CLUSTERING\",\n    \"RETRIEVAL_DOCUMENT\",\n    \"RETRIEVAL_QUERY\",\n    \"QUESTION_ANSWERING\",\n    \"FACT_VERIFICATION\",\n    \"CODE_RETRIEVAL_QUERY\"\n  ]).optional()\n});\nvar GoogleGenerativeAIEmbeddingModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.maxEmbeddingsPerCall = 2048;\n    this.supportsParallelCalls = true;\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n    providerOptions\n  }) {\n    const googleOptions = await parseProviderOptions2({\n      provider: \"google\",\n      providerOptions,\n      schema: googleGenerativeAIEmbeddingProviderOptions\n    });\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError2({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const mergedHeaders = combineHeaders2(\n      await resolve2(this.config.headers),\n      headers\n    );\n    if (values.length === 1) {\n      const {\n        responseHeaders: responseHeaders2,\n        value: response2,\n        rawValue: rawValue2\n      } = await postJsonToApi2({\n        url: `${this.config.baseURL}/models/${this.modelId}:embedContent`,\n        headers: mergedHeaders,\n        body: {\n          model: `models/${this.modelId}`,\n          content: {\n            parts: [{ text: values[0] }]\n          },\n          outputDimensionality: googleOptions == null ? void 0 : googleOptions.outputDimensionality,\n          taskType: googleOptions == null ? void 0 : googleOptions.taskType\n        },\n        failedResponseHandler: googleFailedResponseHandler2,\n        successfulResponseHandler: createJsonResponseHandler2(\n          googleGenerativeAISingleEmbeddingResponseSchema\n        ),\n        abortSignal,\n        fetch: this.config.fetch\n      });\n      return {\n        embeddings: [response2.embedding.values],\n        usage: void 0,\n        response: { headers: responseHeaders2, body: rawValue2 }\n      };\n    }\n    const {\n      responseHeaders,\n      value: response,\n      rawValue\n    } = await postJsonToApi2({\n      url: `${this.config.baseURL}/models/${this.modelId}:batchEmbedContents`,\n      headers: mergedHeaders,\n      body: {\n        requests: values.map((value) => ({\n          model: `models/${this.modelId}`,\n          content: { role: \"user\", parts: [{ text: value }] },\n          outputDimensionality: googleOptions == null ? void 0 : googleOptions.outputDimensionality,\n          taskType: googleOptions == null ? void 0 : googleOptions.taskType\n        }))\n      },\n      failedResponseHandler: googleFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        googleGenerativeAITextEmbeddingResponseSchema2\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.embeddings.map((item) => item.values),\n      usage: void 0,\n      response: { headers: responseHeaders, body: rawValue }\n    };\n  }\n};\nvar googleGenerativeAITextEmbeddingResponseSchema2 = z$1.object({\n  embeddings: z$1.array(z$1.object({ values: z$1.array(z$1.number()) }))\n});\nvar googleGenerativeAISingleEmbeddingResponseSchema = z$1.object({\n  embedding: z$1.object({ values: z$1.array(z$1.number()) })\n});\nfunction convertJSONSchemaToOpenAPISchema2(jsonSchema) {\n  if (jsonSchema == null || isEmptyObjectSchema2(jsonSchema)) {\n    return void 0;\n  }\n  if (typeof jsonSchema === \"boolean\") {\n    return { type: \"boolean\", properties: {} };\n  }\n  const {\n    type,\n    description,\n    required,\n    properties,\n    items,\n    allOf,\n    anyOf,\n    oneOf,\n    format,\n    const: constValue,\n    minLength,\n    enum: enumValues\n  } = jsonSchema;\n  const result = {};\n  if (description)\n    result.description = description;\n  if (required)\n    result.required = required;\n  if (format)\n    result.format = format;\n  if (constValue !== void 0) {\n    result.enum = [constValue];\n  }\n  if (type) {\n    if (Array.isArray(type)) {\n      if (type.includes(\"null\")) {\n        result.type = type.filter((t) => t !== \"null\")[0];\n        result.nullable = true;\n      } else {\n        result.type = type;\n      }\n    } else if (type === \"null\") {\n      result.type = \"null\";\n    } else {\n      result.type = type;\n    }\n  }\n  if (enumValues !== void 0) {\n    result.enum = enumValues;\n  }\n  if (properties != null) {\n    result.properties = Object.entries(properties).reduce(\n      (acc, [key, value]) => {\n        acc[key] = convertJSONSchemaToOpenAPISchema2(value);\n        return acc;\n      },\n      {}\n    );\n  }\n  if (items) {\n    result.items = Array.isArray(items) ? items.map(convertJSONSchemaToOpenAPISchema2) : convertJSONSchemaToOpenAPISchema2(items);\n  }\n  if (allOf) {\n    result.allOf = allOf.map(convertJSONSchemaToOpenAPISchema2);\n  }\n  if (anyOf) {\n    if (anyOf.some(\n      (schema) => typeof schema === \"object\" && (schema == null ? void 0 : schema.type) === \"null\"\n    )) {\n      const nonNullSchemas = anyOf.filter(\n        (schema) => !(typeof schema === \"object\" && (schema == null ? void 0 : schema.type) === \"null\")\n      );\n      if (nonNullSchemas.length === 1) {\n        const converted = convertJSONSchemaToOpenAPISchema2(nonNullSchemas[0]);\n        if (typeof converted === \"object\") {\n          result.nullable = true;\n          Object.assign(result, converted);\n        }\n      } else {\n        result.anyOf = nonNullSchemas.map(convertJSONSchemaToOpenAPISchema2);\n        result.nullable = true;\n      }\n    } else {\n      result.anyOf = anyOf.map(convertJSONSchemaToOpenAPISchema2);\n    }\n  }\n  if (oneOf) {\n    result.oneOf = oneOf.map(convertJSONSchemaToOpenAPISchema2);\n  }\n  if (minLength !== void 0) {\n    result.minLength = minLength;\n  }\n  return result;\n}\nfunction isEmptyObjectSchema2(jsonSchema) {\n  return jsonSchema != null && typeof jsonSchema === \"object\" && jsonSchema.type === \"object\" && (jsonSchema.properties == null || Object.keys(jsonSchema.properties).length === 0) && !jsonSchema.additionalProperties;\n}\nfunction convertToGoogleGenerativeAIMessages2(prompt, options) {\n  var _a16;\n  const systemInstructionParts = [];\n  const contents = [];\n  let systemMessagesAllowed = true;\n  const isGemmaModel = (_a16 = options == null ? void 0 : options.isGemmaModel) != null ? _a16 : false;\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        if (!systemMessagesAllowed) {\n          throw new UnsupportedFunctionalityError2({\n            functionality: \"system messages are only supported at the beginning of the conversation\"\n          });\n        }\n        systemInstructionParts.push({ text: content });\n        break;\n      }\n      case \"user\": {\n        systemMessagesAllowed = false;\n        const parts = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              parts.push({ text: part.text });\n              break;\n            }\n            case \"file\": {\n              const mediaType = part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType;\n              parts.push(\n                part.data instanceof URL ? {\n                  fileData: {\n                    mimeType: mediaType,\n                    fileUri: part.data.toString()\n                  }\n                } : {\n                  inlineData: {\n                    mimeType: mediaType,\n                    data: convertToBase64(part.data)\n                  }\n                }\n              );\n              break;\n            }\n          }\n        }\n        contents.push({ role: \"user\", parts });\n        break;\n      }\n      case \"assistant\": {\n        systemMessagesAllowed = false;\n        contents.push({\n          role: \"model\",\n          parts: content.map((part) => {\n            var _a23, _b, _c, _d, _e, _f;\n            switch (part.type) {\n              case \"text\": {\n                return part.text.length === 0 ? void 0 : {\n                  text: part.text,\n                  thoughtSignature: (_b = (_a23 = part.providerOptions) == null ? void 0 : _a23.google) == null ? void 0 : _b.thoughtSignature\n                };\n              }\n              case \"reasoning\": {\n                return part.text.length === 0 ? void 0 : {\n                  text: part.text,\n                  thought: true,\n                  thoughtSignature: (_d = (_c = part.providerOptions) == null ? void 0 : _c.google) == null ? void 0 : _d.thoughtSignature\n                };\n              }\n              case \"file\": {\n                if (part.mediaType !== \"image/png\") {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: \"Only PNG images are supported in assistant messages\"\n                  });\n                }\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: \"File data URLs in assistant messages are not supported\"\n                  });\n                }\n                return {\n                  inlineData: {\n                    mimeType: part.mediaType,\n                    data: convertToBase64(part.data)\n                  }\n                };\n              }\n              case \"tool-call\": {\n                return {\n                  functionCall: {\n                    name: part.toolName,\n                    args: part.input\n                  },\n                  thoughtSignature: (_f = (_e = part.providerOptions) == null ? void 0 : _e.google) == null ? void 0 : _f.thoughtSignature\n                };\n              }\n            }\n          }).filter((part) => part !== void 0)\n        });\n        break;\n      }\n      case \"tool\": {\n        systemMessagesAllowed = false;\n        contents.push({\n          role: \"user\",\n          parts: content.map((part) => ({\n            functionResponse: {\n              name: part.toolName,\n              response: {\n                name: part.toolName,\n                content: part.output.value\n              }\n            }\n          }))\n        });\n        break;\n      }\n    }\n  }\n  if (isGemmaModel && systemInstructionParts.length > 0 && contents.length > 0 && contents[0].role === \"user\") {\n    const systemText = systemInstructionParts.map((part) => part.text).join(\"\\n\\n\");\n    contents[0].parts.unshift({ text: systemText + \"\\n\\n\" });\n  }\n  return {\n    systemInstruction: systemInstructionParts.length > 0 && !isGemmaModel ? { parts: systemInstructionParts } : void 0,\n    contents\n  };\n}\nfunction getModelPath2(modelId) {\n  return modelId.includes(\"/\") ? modelId : `models/${modelId}`;\n}\nvar googleGenerativeAIProviderOptions = z$1.object({\n  responseModalities: z$1.array(z$1.enum([\"TEXT\", \"IMAGE\"])).optional(),\n  thinkingConfig: z$1.object({\n    thinkingBudget: z$1.number().optional(),\n    includeThoughts: z$1.boolean().optional()\n  }).optional(),\n  /**\n  Optional.\n  The name of the cached content used as context to serve the prediction.\n  Format: cachedContents/{cachedContent}\n     */\n  cachedContent: z$1.string().optional(),\n  /**\n   * Optional. Enable structured output. Default is true.\n   *\n   * This is useful when the JSON Schema contains elements that are\n   * not supported by the OpenAPI schema version that\n   * Google Generative AI uses. You can use this to disable\n   * structured outputs if you need to.\n   */\n  structuredOutputs: z$1.boolean().optional(),\n  /**\n  Optional. A list of unique safety settings for blocking unsafe content.\n   */\n  safetySettings: z$1.array(\n    z$1.object({\n      category: z$1.enum([\n        \"HARM_CATEGORY_UNSPECIFIED\",\n        \"HARM_CATEGORY_HATE_SPEECH\",\n        \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"HARM_CATEGORY_HARASSMENT\",\n        \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"HARM_CATEGORY_CIVIC_INTEGRITY\"\n      ]),\n      threshold: z$1.enum([\n        \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n        \"BLOCK_LOW_AND_ABOVE\",\n        \"BLOCK_MEDIUM_AND_ABOVE\",\n        \"BLOCK_ONLY_HIGH\",\n        \"BLOCK_NONE\",\n        \"OFF\"\n      ])\n    })\n  ).optional(),\n  threshold: z$1.enum([\n    \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n    \"BLOCK_LOW_AND_ABOVE\",\n    \"BLOCK_MEDIUM_AND_ABOVE\",\n    \"BLOCK_ONLY_HIGH\",\n    \"BLOCK_NONE\",\n    \"OFF\"\n  ]).optional(),\n  /**\n   * Optional. Enables timestamp understanding for audio-only files.\n   *\n   * https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding\n   */\n  audioTimestamp: z$1.boolean().optional(),\n  /**\n   * Optional. Defines labels used in billing reports. Available on Vertex AI only.\n   *\n   * https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls\n   */\n  labels: z$1.record(z$1.string(), z$1.string()).optional()\n});\nfunction prepareTools4({\n  tools,\n  toolChoice,\n  modelId\n}) {\n  var _a16;\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  const isGemini2 = modelId.includes(\"gemini-2\");\n  const supportsDynamicRetrieval = modelId.includes(\"gemini-1.5-flash\") && !modelId.includes(\"-8b\");\n  if (tools == null) {\n    return { tools: void 0, toolConfig: void 0, toolWarnings };\n  }\n  const hasFunctionTools = tools.some((tool2) => tool2.type === \"function\");\n  const hasProviderDefinedTools = tools.some(\n    (tool2) => tool2.type === \"provider-defined\"\n  );\n  if (hasFunctionTools && hasProviderDefinedTools) {\n    toolWarnings.push({\n      type: \"unsupported-tool\",\n      tool: tools.find((tool2) => tool2.type === \"function\"),\n      details: \"Cannot mix function tools with provider-defined tools in the same request. Please use either function tools or provider-defined tools, but not both.\"\n    });\n  }\n  if (hasProviderDefinedTools) {\n    const googleTools2 = {};\n    const providerDefinedTools = tools.filter(\n      (tool2) => tool2.type === \"provider-defined\"\n    );\n    providerDefinedTools.forEach((tool2) => {\n      switch (tool2.id) {\n        case \"google.google_search\":\n          if (isGemini2) {\n            googleTools2.googleSearch = {};\n          } else if (supportsDynamicRetrieval) {\n            googleTools2.googleSearchRetrieval = {\n              dynamicRetrievalConfig: {\n                mode: tool2.args.mode,\n                dynamicThreshold: tool2.args.dynamicThreshold\n              }\n            };\n          } else {\n            googleTools2.googleSearchRetrieval = {};\n          }\n          break;\n        case \"google.url_context\":\n          if (isGemini2) {\n            googleTools2.urlContext = {};\n          } else {\n            toolWarnings.push({\n              type: \"unsupported-tool\",\n              tool: tool2,\n              details: \"The URL context tool is not supported with other Gemini models than Gemini 2.\"\n            });\n          }\n          break;\n        case \"google.code_execution\":\n          if (isGemini2) {\n            googleTools2.codeExecution = {};\n          } else {\n            toolWarnings.push({\n              type: \"unsupported-tool\",\n              tool: tool2,\n              details: \"The code execution tools is not supported with other Gemini models than Gemini 2.\"\n            });\n          }\n          break;\n        default:\n          toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n          break;\n      }\n    });\n    return {\n      tools: Object.keys(googleTools2).length > 0 ? googleTools2 : void 0,\n      toolConfig: void 0,\n      toolWarnings\n    };\n  }\n  const functionDeclarations = [];\n  for (const tool2 of tools) {\n    switch (tool2.type) {\n      case \"function\":\n        functionDeclarations.push({\n          name: tool2.name,\n          description: (_a16 = tool2.description) != null ? _a16 : \"\",\n          parameters: convertJSONSchemaToOpenAPISchema2(tool2.inputSchema)\n        });\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return {\n      tools: { functionDeclarations },\n      toolConfig: void 0,\n      toolWarnings\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"AUTO\" } },\n        toolWarnings\n      };\n    case \"none\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"NONE\" } },\n        toolWarnings\n      };\n    case \"required\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: { functionCallingConfig: { mode: \"ANY\" } },\n        toolWarnings\n      };\n    case \"tool\":\n      return {\n        tools: { functionDeclarations },\n        toolConfig: {\n          functionCallingConfig: {\n            mode: \"ANY\",\n            allowedFunctionNames: [toolChoice.toolName]\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nfunction mapGoogleGenerativeAIFinishReason2({\n  finishReason,\n  hasToolCalls\n}) {\n  switch (finishReason) {\n    case \"STOP\":\n      return hasToolCalls ? \"tool-calls\" : \"stop\";\n    case \"MAX_TOKENS\":\n      return \"length\";\n    case \"IMAGE_SAFETY\":\n    case \"RECITATION\":\n    case \"SAFETY\":\n    case \"BLOCKLIST\":\n    case \"PROHIBITED_CONTENT\":\n    case \"SPII\":\n      return \"content-filter\";\n    case \"FINISH_REASON_UNSPECIFIED\":\n    case \"OTHER\":\n      return \"other\";\n    case \"MALFORMED_FUNCTION_CALL\":\n      return \"error\";\n    default:\n      return \"unknown\";\n  }\n}\nvar groundingChunkSchema2 = z$1.object({\n  web: z$1.object({ uri: z$1.string(), title: z$1.string() }).nullish(),\n  retrievedContext: z$1.object({ uri: z$1.string(), title: z$1.string() }).nullish()\n});\nvar groundingMetadataSchema2 = z$1.object({\n  webSearchQueries: z$1.array(z$1.string()).nullish(),\n  retrievalQueries: z$1.array(z$1.string()).nullish(),\n  searchEntryPoint: z$1.object({ renderedContent: z$1.string() }).nullish(),\n  groundingChunks: z$1.array(groundingChunkSchema2).nullish(),\n  groundingSupports: z$1.array(\n    z$1.object({\n      segment: z$1.object({\n        startIndex: z$1.number().nullish(),\n        endIndex: z$1.number().nullish(),\n        text: z$1.string().nullish()\n      }),\n      segment_text: z$1.string().nullish(),\n      groundingChunkIndices: z$1.array(z$1.number()).nullish(),\n      supportChunkIndices: z$1.array(z$1.number()).nullish(),\n      confidenceScores: z$1.array(z$1.number()).nullish(),\n      confidenceScore: z$1.array(z$1.number()).nullish()\n    })\n  ).nullish(),\n  retrievalMetadata: z$1.union([\n    z$1.object({\n      webDynamicRetrievalScore: z$1.number()\n    }),\n    z$1.object({})\n  ]).nullish()\n});\nvar googleSearch = createProviderDefinedToolFactory({\n  id: \"google.google_search\",\n  name: \"google_search\",\n  inputSchema: z$1.object({\n    mode: z$1.enum([\"MODE_DYNAMIC\", \"MODE_UNSPECIFIED\"]).default(\"MODE_UNSPECIFIED\"),\n    dynamicThreshold: z$1.number().default(1)\n  })\n});\nvar urlMetadataSchema = z$1.object({\n  retrievedUrl: z$1.string(),\n  urlRetrievalStatus: z$1.string()\n});\nvar urlContextMetadataSchema = z$1.object({\n  urlMetadata: z$1.array(urlMetadataSchema)\n});\nvar urlContext = createProviderDefinedToolFactory({\n  id: \"google.url_context\",\n  name: \"url_context\",\n  inputSchema: z$1.object({})\n});\nvar GoogleGenerativeAILanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    var _a16;\n    this.modelId = modelId;\n    this.config = config;\n    this.generateId = (_a16 = config.generateId) != null ? _a16 : generateId2;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportedUrls() {\n    var _a16, _b, _c;\n    return (_c = (_b = (_a16 = this.config).supportedUrls) == null ? void 0 : _b.call(_a16)) != null ? _c : {};\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    tools,\n    toolChoice,\n    providerOptions\n  }) {\n    var _a16, _b;\n    const warnings = [];\n    const googleOptions = await parseProviderOptions2({\n      provider: \"google\",\n      providerOptions,\n      schema: googleGenerativeAIProviderOptions\n    });\n    if (((_a16 = googleOptions == null ? void 0 : googleOptions.thinkingConfig) == null ? void 0 : _a16.includeThoughts) === true && !this.config.provider.startsWith(\"google.vertex.\")) {\n      warnings.push({\n        type: \"other\",\n        message: `The 'includeThoughts' option is only supported with the Google Vertex provider and might not be supported or could behave unexpectedly with the current Google provider (${this.config.provider}).`\n      });\n    }\n    const isGemmaModel = this.modelId.toLowerCase().startsWith(\"gemma-\");\n    const { contents, systemInstruction } = convertToGoogleGenerativeAIMessages2(\n      prompt,\n      { isGemmaModel }\n    );\n    const {\n      tools: googleTools2,\n      toolConfig: googleToolConfig,\n      toolWarnings\n    } = prepareTools4({\n      tools,\n      toolChoice,\n      modelId: this.modelId\n    });\n    return {\n      args: {\n        generationConfig: {\n          // standardized settings:\n          maxOutputTokens,\n          temperature,\n          topK,\n          topP,\n          frequencyPenalty,\n          presencePenalty,\n          stopSequences,\n          seed,\n          // response format:\n          responseMimeType: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? \"application/json\" : void 0,\n          responseSchema: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && // Google GenAI does not support all OpenAPI Schema features,\n          // so this is needed as an escape hatch:\n          // TODO convert into provider option\n          ((_b = googleOptions == null ? void 0 : googleOptions.structuredOutputs) != null ? _b : true) ? convertJSONSchemaToOpenAPISchema2(responseFormat.schema) : void 0,\n          ...(googleOptions == null ? void 0 : googleOptions.audioTimestamp) && {\n            audioTimestamp: googleOptions.audioTimestamp\n          },\n          // provider options:\n          responseModalities: googleOptions == null ? void 0 : googleOptions.responseModalities,\n          thinkingConfig: googleOptions == null ? void 0 : googleOptions.thinkingConfig\n        },\n        contents,\n        systemInstruction: isGemmaModel ? void 0 : systemInstruction,\n        safetySettings: googleOptions == null ? void 0 : googleOptions.safetySettings,\n        tools: googleTools2,\n        toolConfig: googleToolConfig,\n        cachedContent: googleOptions == null ? void 0 : googleOptions.cachedContent,\n        labels: googleOptions == null ? void 0 : googleOptions.labels\n      },\n      warnings: [...warnings, ...toolWarnings]\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l;\n    const { args, warnings } = await this.getArgs(options);\n    const body = JSON.stringify(args);\n    const mergedHeaders = combineHeaders2(\n      await resolve2(this.config.headers),\n      options.headers\n    );\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: `${this.config.baseURL}/${getModelPath2(\n        this.modelId\n      )}:generateContent`,\n      headers: mergedHeaders,\n      body: args,\n      failedResponseHandler: googleFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(responseSchema2),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const candidate = response.candidates[0];\n    const content = [];\n    const parts = (_b = (_a16 = candidate.content) == null ? void 0 : _a16.parts) != null ? _b : [];\n    const usageMetadata = response.usageMetadata;\n    let lastCodeExecutionToolCallId;\n    for (const part of parts) {\n      if (\"executableCode\" in part && ((_c = part.executableCode) == null ? void 0 : _c.code)) {\n        const toolCallId = this.config.generateId();\n        lastCodeExecutionToolCallId = toolCallId;\n        content.push({\n          type: \"tool-call\",\n          toolCallId,\n          toolName: \"code_execution\",\n          input: JSON.stringify(part.executableCode),\n          providerExecuted: true\n        });\n      } else if (\"codeExecutionResult\" in part && part.codeExecutionResult) {\n        content.push({\n          type: \"tool-result\",\n          // Assumes a result directly follows its corresponding call part.\n          toolCallId: lastCodeExecutionToolCallId,\n          toolName: \"code_execution\",\n          result: {\n            outcome: part.codeExecutionResult.outcome,\n            output: part.codeExecutionResult.output\n          },\n          providerExecuted: true\n        });\n        lastCodeExecutionToolCallId = void 0;\n      } else if (\"text\" in part && part.text != null && part.text.length > 0) {\n        content.push({\n          type: part.thought === true ? \"reasoning\" : \"text\",\n          text: part.text,\n          providerMetadata: part.thoughtSignature ? { google: { thoughtSignature: part.thoughtSignature } } : void 0\n        });\n      } else if (\"functionCall\" in part) {\n        content.push({\n          type: \"tool-call\",\n          toolCallId: this.config.generateId(),\n          toolName: part.functionCall.name,\n          input: JSON.stringify(part.functionCall.args),\n          providerMetadata: part.thoughtSignature ? { google: { thoughtSignature: part.thoughtSignature } } : void 0\n        });\n      } else if (\"inlineData\" in part) {\n        content.push({\n          type: \"file\",\n          data: part.inlineData.data,\n          mediaType: part.inlineData.mimeType\n        });\n      }\n    }\n    const sources = (_d = extractSources2({\n      groundingMetadata: candidate.groundingMetadata,\n      generateId: this.config.generateId\n    })) != null ? _d : [];\n    for (const source of sources) {\n      content.push(source);\n    }\n    return {\n      content,\n      finishReason: mapGoogleGenerativeAIFinishReason2({\n        finishReason: candidate.finishReason,\n        hasToolCalls: content.some((part) => part.type === \"tool-call\")\n      }),\n      usage: {\n        inputTokens: (_e = usageMetadata == null ? void 0 : usageMetadata.promptTokenCount) != null ? _e : void 0,\n        outputTokens: (_f = usageMetadata == null ? void 0 : usageMetadata.candidatesTokenCount) != null ? _f : void 0,\n        totalTokens: (_g = usageMetadata == null ? void 0 : usageMetadata.totalTokenCount) != null ? _g : void 0,\n        reasoningTokens: (_h = usageMetadata == null ? void 0 : usageMetadata.thoughtsTokenCount) != null ? _h : void 0,\n        cachedInputTokens: (_i = usageMetadata == null ? void 0 : usageMetadata.cachedContentTokenCount) != null ? _i : void 0\n      },\n      warnings,\n      providerMetadata: {\n        google: {\n          groundingMetadata: (_j = candidate.groundingMetadata) != null ? _j : null,\n          urlContextMetadata: (_k = candidate.urlContextMetadata) != null ? _k : null,\n          safetyRatings: (_l = candidate.safetyRatings) != null ? _l : null,\n          usageMetadata: usageMetadata != null ? usageMetadata : null\n        }\n      },\n      request: { body },\n      response: {\n        // TODO timestamp, model id, id\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = await this.getArgs(options);\n    const body = JSON.stringify(args);\n    const headers = combineHeaders2(\n      await resolve2(this.config.headers),\n      options.headers\n    );\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: `${this.config.baseURL}/${getModelPath2(\n        this.modelId\n      )}:streamGenerateContent?alt=sse`,\n      headers,\n      body: args,\n      failedResponseHandler: googleFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(chunkSchema2),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    let providerMetadata = void 0;\n    const generateId3 = this.config.generateId;\n    let hasToolCalls = false;\n    let currentTextBlockId = null;\n    let currentReasoningBlockId = null;\n    let blockCounter = 0;\n    const emittedSourceUrls = /* @__PURE__ */ new Set();\n    let lastCodeExecutionToolCallId;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            const usageMetadata = value.usageMetadata;\n            if (usageMetadata != null) {\n              usage.inputTokens = (_a16 = usageMetadata.promptTokenCount) != null ? _a16 : void 0;\n              usage.outputTokens = (_b = usageMetadata.candidatesTokenCount) != null ? _b : void 0;\n              usage.totalTokens = (_c = usageMetadata.totalTokenCount) != null ? _c : void 0;\n              usage.reasoningTokens = (_d = usageMetadata.thoughtsTokenCount) != null ? _d : void 0;\n              usage.cachedInputTokens = (_e = usageMetadata.cachedContentTokenCount) != null ? _e : void 0;\n            }\n            const candidate = (_f = value.candidates) == null ? void 0 : _f[0];\n            if (candidate == null) {\n              return;\n            }\n            const content = candidate.content;\n            const sources = extractSources2({\n              groundingMetadata: candidate.groundingMetadata,\n              generateId: generateId3\n            });\n            if (sources != null) {\n              for (const source of sources) {\n                if (source.sourceType === \"url\" && !emittedSourceUrls.has(source.url)) {\n                  emittedSourceUrls.add(source.url);\n                  controller.enqueue(source);\n                }\n              }\n            }\n            if (content != null) {\n              const parts = (_g = content.parts) != null ? _g : [];\n              for (const part of parts) {\n                if (\"executableCode\" in part && ((_h = part.executableCode) == null ? void 0 : _h.code)) {\n                  const toolCallId = generateId3();\n                  lastCodeExecutionToolCallId = toolCallId;\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallId,\n                    toolName: \"code_execution\",\n                    input: JSON.stringify(part.executableCode),\n                    providerExecuted: true\n                  });\n                  hasToolCalls = true;\n                } else if (\"codeExecutionResult\" in part && part.codeExecutionResult) {\n                  const toolCallId = lastCodeExecutionToolCallId;\n                  if (toolCallId) {\n                    controller.enqueue({\n                      type: \"tool-result\",\n                      toolCallId,\n                      toolName: \"code_execution\",\n                      result: {\n                        outcome: part.codeExecutionResult.outcome,\n                        output: part.codeExecutionResult.output\n                      },\n                      providerExecuted: true\n                    });\n                    lastCodeExecutionToolCallId = void 0;\n                  }\n                } else if (\"text\" in part && part.text != null && part.text.length > 0) {\n                  if (part.thought === true) {\n                    if (currentTextBlockId !== null) {\n                      controller.enqueue({\n                        type: \"text-end\",\n                        id: currentTextBlockId\n                      });\n                      currentTextBlockId = null;\n                    }\n                    if (currentReasoningBlockId === null) {\n                      currentReasoningBlockId = String(blockCounter++);\n                      controller.enqueue({\n                        type: \"reasoning-start\",\n                        id: currentReasoningBlockId,\n                        providerMetadata: part.thoughtSignature ? {\n                          google: {\n                            thoughtSignature: part.thoughtSignature\n                          }\n                        } : void 0\n                      });\n                    }\n                    controller.enqueue({\n                      type: \"reasoning-delta\",\n                      id: currentReasoningBlockId,\n                      delta: part.text,\n                      providerMetadata: part.thoughtSignature ? {\n                        google: { thoughtSignature: part.thoughtSignature }\n                      } : void 0\n                    });\n                  } else {\n                    if (currentReasoningBlockId !== null) {\n                      controller.enqueue({\n                        type: \"reasoning-end\",\n                        id: currentReasoningBlockId\n                      });\n                      currentReasoningBlockId = null;\n                    }\n                    if (currentTextBlockId === null) {\n                      currentTextBlockId = String(blockCounter++);\n                      controller.enqueue({\n                        type: \"text-start\",\n                        id: currentTextBlockId,\n                        providerMetadata: part.thoughtSignature ? {\n                          google: {\n                            thoughtSignature: part.thoughtSignature\n                          }\n                        } : void 0\n                      });\n                    }\n                    controller.enqueue({\n                      type: \"text-delta\",\n                      id: currentTextBlockId,\n                      delta: part.text,\n                      providerMetadata: part.thoughtSignature ? {\n                        google: { thoughtSignature: part.thoughtSignature }\n                      } : void 0\n                    });\n                  }\n                }\n              }\n              const inlineDataParts = getInlineDataParts2(content.parts);\n              if (inlineDataParts != null) {\n                for (const part of inlineDataParts) {\n                  controller.enqueue({\n                    type: \"file\",\n                    mediaType: part.inlineData.mimeType,\n                    data: part.inlineData.data\n                  });\n                }\n              }\n              const toolCallDeltas = getToolCallsFromParts2({\n                parts: content.parts,\n                generateId: generateId3\n              });\n              if (toolCallDeltas != null) {\n                for (const toolCall of toolCallDeltas) {\n                  controller.enqueue({\n                    type: \"tool-input-start\",\n                    id: toolCall.toolCallId,\n                    toolName: toolCall.toolName,\n                    providerMetadata: toolCall.providerMetadata\n                  });\n                  controller.enqueue({\n                    type: \"tool-input-delta\",\n                    id: toolCall.toolCallId,\n                    delta: toolCall.args,\n                    providerMetadata: toolCall.providerMetadata\n                  });\n                  controller.enqueue({\n                    type: \"tool-input-end\",\n                    id: toolCall.toolCallId,\n                    providerMetadata: toolCall.providerMetadata\n                  });\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallId: toolCall.toolCallId,\n                    toolName: toolCall.toolName,\n                    input: toolCall.args,\n                    providerMetadata: toolCall.providerMetadata\n                  });\n                  hasToolCalls = true;\n                }\n              }\n            }\n            if (candidate.finishReason != null) {\n              finishReason = mapGoogleGenerativeAIFinishReason2({\n                finishReason: candidate.finishReason,\n                hasToolCalls\n              });\n              providerMetadata = {\n                google: {\n                  groundingMetadata: (_i = candidate.groundingMetadata) != null ? _i : null,\n                  urlContextMetadata: (_j = candidate.urlContextMetadata) != null ? _j : null,\n                  safetyRatings: (_k = candidate.safetyRatings) != null ? _k : null\n                }\n              };\n              if (usageMetadata != null) {\n                providerMetadata.google.usageMetadata = usageMetadata;\n              }\n            }\n          },\n          flush(controller) {\n            if (currentTextBlockId !== null) {\n              controller.enqueue({\n                type: \"text-end\",\n                id: currentTextBlockId\n              });\n            }\n            if (currentReasoningBlockId !== null) {\n              controller.enqueue({\n                type: \"reasoning-end\",\n                id: currentReasoningBlockId\n              });\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n              providerMetadata\n            });\n          }\n        })\n      ),\n      response: { headers: responseHeaders },\n      request: { body }\n    };\n  }\n};\nfunction getToolCallsFromParts2({\n  parts,\n  generateId: generateId3\n}) {\n  const functionCallParts = parts == null ? void 0 : parts.filter(\n    (part) => \"functionCall\" in part\n  );\n  return functionCallParts == null || functionCallParts.length === 0 ? void 0 : functionCallParts.map((part) => ({\n    type: \"tool-call\",\n    toolCallId: generateId3(),\n    toolName: part.functionCall.name,\n    args: JSON.stringify(part.functionCall.args),\n    providerMetadata: part.thoughtSignature ? { google: { thoughtSignature: part.thoughtSignature } } : void 0\n  }));\n}\nfunction getInlineDataParts2(parts) {\n  return parts == null ? void 0 : parts.filter(\n    (part) => \"inlineData\" in part\n  );\n}\nfunction extractSources2({\n  groundingMetadata,\n  generateId: generateId3\n}) {\n  var _a16;\n  return (_a16 = groundingMetadata == null ? void 0 : groundingMetadata.groundingChunks) == null ? void 0 : _a16.filter(\n    (chunk) => chunk.web != null\n  ).map((chunk) => ({\n    type: \"source\",\n    sourceType: \"url\",\n    id: generateId3(),\n    url: chunk.web.uri,\n    title: chunk.web.title\n  }));\n}\nvar contentSchema2 = z$1.object({\n  parts: z$1.array(\n    z$1.union([\n      // note: order matters since text can be fully empty\n      z$1.object({\n        functionCall: z$1.object({\n          name: z$1.string(),\n          args: z$1.unknown()\n        }),\n        thoughtSignature: z$1.string().nullish()\n      }),\n      z$1.object({\n        inlineData: z$1.object({\n          mimeType: z$1.string(),\n          data: z$1.string()\n        })\n      }),\n      z$1.object({\n        executableCode: z$1.object({\n          language: z$1.string(),\n          code: z$1.string()\n        }).nullish(),\n        codeExecutionResult: z$1.object({\n          outcome: z$1.string(),\n          output: z$1.string()\n        }).nullish(),\n        text: z$1.string().nullish(),\n        thought: z$1.boolean().nullish(),\n        thoughtSignature: z$1.string().nullish()\n      })\n    ])\n  ).nullish()\n});\nvar safetyRatingSchema2 = z$1.object({\n  category: z$1.string().nullish(),\n  probability: z$1.string().nullish(),\n  probabilityScore: z$1.number().nullish(),\n  severity: z$1.string().nullish(),\n  severityScore: z$1.number().nullish(),\n  blocked: z$1.boolean().nullish()\n});\nvar usageSchema = z$1.object({\n  cachedContentTokenCount: z$1.number().nullish(),\n  thoughtsTokenCount: z$1.number().nullish(),\n  promptTokenCount: z$1.number().nullish(),\n  candidatesTokenCount: z$1.number().nullish(),\n  totalTokenCount: z$1.number().nullish()\n});\nvar responseSchema2 = z$1.object({\n  candidates: z$1.array(\n    z$1.object({\n      content: contentSchema2.nullish().or(z$1.object({}).strict()),\n      finishReason: z$1.string().nullish(),\n      safetyRatings: z$1.array(safetyRatingSchema2).nullish(),\n      groundingMetadata: groundingMetadataSchema2.nullish(),\n      urlContextMetadata: urlContextMetadataSchema.nullish()\n    })\n  ),\n  usageMetadata: usageSchema.nullish()\n});\nvar chunkSchema2 = z$1.object({\n  candidates: z$1.array(\n    z$1.object({\n      content: contentSchema2.nullish(),\n      finishReason: z$1.string().nullish(),\n      safetyRatings: z$1.array(safetyRatingSchema2).nullish(),\n      groundingMetadata: groundingMetadataSchema2.nullish(),\n      urlContextMetadata: urlContextMetadataSchema.nullish()\n    })\n  ).nullish(),\n  usageMetadata: usageSchema.nullish()\n});\nvar codeExecution = createProviderDefinedToolFactoryWithOutputSchema({\n  id: \"google.code_execution\",\n  name: \"code_execution\",\n  inputSchema: z$1.object({\n    language: z$1.string().describe(\"The programming language of the code.\"),\n    code: z$1.string().describe(\"The code to be executed.\")\n  }),\n  outputSchema: z$1.object({\n    outcome: z$1.string().describe('The outcome of the execution (e.g., \"OUTCOME_OK\").'),\n    output: z$1.string().describe(\"The output from the code execution.\")\n  })\n});\nvar googleTools = {\n  /**\n   * Creates a Google search tool that gives Google direct access to real-time web content.\n   * Must have name \"google_search\".\n   */\n  googleSearch,\n  /**\n   * Creates a URL context tool that gives Google direct access to real-time web content.\n   * Must have name \"url_context\".\n   */\n  urlContext,\n  /**\n   * A tool that enables the model to generate and run Python code.\n   * Must have name \"code_execution\".\n   *\n   * @note Ensure the selected model supports Code Execution.\n   * Multi-tool usage with the code execution tool is typically compatible with Gemini >=2 models.\n   *\n   * @see https://ai.google.dev/gemini-api/docs/code-execution (Google AI)\n   * @see https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/code-execution-api (Vertex AI)\n   */\n  codeExecution\n};\nvar GoogleGenerativeAIImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n  }\n  get maxImagesPerCall() {\n    var _a16;\n    return (_a16 = this.settings.maxImagesPerCall) != null ? _a16 : 4;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c;\n    const {\n      prompt,\n      n = 1,\n      size = \"1024x1024\",\n      aspectRatio = \"1:1\",\n      seed,\n      providerOptions,\n      headers,\n      abortSignal\n    } = options;\n    const warnings = [];\n    if (size != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"size\",\n        details: \"This model does not support the `size` option. Use `aspectRatio` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\",\n        details: \"This model does not support the `seed` option through this provider.\"\n      });\n    }\n    const googleOptions = await parseProviderOptions2({\n      provider: \"google\",\n      providerOptions,\n      schema: googleImageProviderOptionsSchema\n    });\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const parameters = {\n      sampleCount: n\n    };\n    if (aspectRatio != null) {\n      parameters.aspectRatio = aspectRatio;\n    }\n    if (googleOptions) {\n      Object.assign(parameters, googleOptions);\n    }\n    const body = {\n      instances: [{ prompt }],\n      parameters\n    };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: `${this.config.baseURL}/models/${this.modelId}:predict`,\n      headers: combineHeaders2(await resolve2(this.config.headers), headers),\n      body,\n      failedResponseHandler: googleFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        googleImageResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.predictions.map(\n        (p) => p.bytesBase64Encoded\n      ),\n      warnings: warnings != null ? warnings : [],\n      providerMetadata: {\n        google: {\n          images: response.predictions.map((prediction) => ({\n            // Add any prediction-specific metadata here\n          }))\n        }\n      },\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar googleImageResponseSchema = z$1.object({\n  predictions: z$1.array(z$1.object({ bytesBase64Encoded: z$1.string() })).default([])\n});\nvar googleImageProviderOptionsSchema = z$1.object({\n  personGeneration: z$1.enum([\"dont_allow\", \"allow_adult\", \"allow_all\"]).nullish(),\n  aspectRatio: z$1.enum([\"1:1\", \"3:4\", \"4:3\", \"9:16\", \"16:9\"]).nullish()\n});\nfunction createGoogleGenerativeAI2(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash2(options.baseURL)) != null ? _a16 : \"https://generativelanguage.googleapis.com/v1beta\";\n  const getHeaders = () => ({\n    \"x-goog-api-key\": loadApiKey2({\n      apiKey: options.apiKey,\n      environmentVariableName: \"GOOGLE_GENERATIVE_AI_API_KEY\",\n      description: \"Google Generative AI\"\n    }),\n    ...options.headers\n  });\n  const createChatModel = (modelId) => {\n    var _a23;\n    return new GoogleGenerativeAILanguageModel2(modelId, {\n      provider: \"google.generative-ai\",\n      baseURL,\n      headers: getHeaders,\n      generateId: (_a23 = options.generateId) != null ? _a23 : generateId2,\n      supportedUrls: () => ({\n        \"*\": [\n          // Google Generative Language \"files\" endpoint\n          // e.g. https://generativelanguage.googleapis.com/v1beta/files/...\n          new RegExp(`^${baseURL}/files/.*$`),\n          // YouTube URLs (public or unlisted videos)\n          new RegExp(\n            `^https://(?:www\\\\.)?youtube\\\\.com/watch\\\\?v=[\\\\w-]+(?:&[\\\\w=&.-]*)?$`\n          ),\n          new RegExp(`^https://youtu\\\\.be/[\\\\w-]+(?:\\\\?[\\\\w=&.-]*)?$`)\n        ]\n      }),\n      fetch: options.fetch\n    });\n  };\n  const createEmbeddingModel = (modelId) => new GoogleGenerativeAIEmbeddingModel2(modelId, {\n    provider: \"google.generative-ai\",\n    baseURL,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createImageModel = (modelId, settings = {}) => new GoogleGenerativeAIImageModel(modelId, settings, {\n    provider: \"google.generative-ai\",\n    baseURL,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const provider = function(modelId) {\n    if (new.target) {\n      throw new Error(\n        \"The Google Generative AI model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId);\n  };\n  provider.languageModel = createChatModel;\n  provider.chat = createChatModel;\n  provider.generativeAI = createChatModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.image = createImageModel;\n  provider.imageModel = createImageModel;\n  provider.tools = googleTools;\n  return provider;\n}\nvar google2 = createGoogleGenerativeAI2();\nfunction convertToGroqChatMessages(prompt) {\n  const messages = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        messages.push({ role: \"system\", content });\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part) => {\n            var _a16;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"image\": {\n                return {\n                  type: \"image_url\",\n                  image_url: {\n                    url: part.image instanceof URL ? part.image.toString() : `data:${(_a16 = part.mimeType) != null ? _a16 : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`\n                  }\n                };\n              }\n              case \"file\": {\n                throw new UnsupportedFunctionalityError({\n                  functionality: \"File content parts in user messages\"\n                });\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args)\n                }\n              });\n              break;\n            }\n          }\n        }\n        messages.push({\n          role: \"assistant\",\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : void 0\n        });\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          messages.push({\n            role: \"tool\",\n            tool_call_id: toolResponse.toolCallId,\n            content: JSON.stringify(toolResponse.result)\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return messages;\n}\nfunction getResponseMetadata({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nvar groqErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    type: z.string()\n  })\n});\nvar groqFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: groqErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\nfunction prepareTools5({\n  mode\n}) {\n  var _a16;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  const groqTools2 = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n    } else {\n      groqTools2.push({\n        type: \"function\",\n        function: {\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.parameters\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: groqTools2, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: groqTools2, tool_choice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: groqTools2,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nfunction mapGroqFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar GroqChatLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.supportsStructuredOutputs = false;\n    this.defaultObjectGenerationMode = \"json\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportsImageUrls() {\n    return !this.settings.downloadImages;\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    stream,\n    providerMetadata\n  }) {\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type === \"json\" && responseFormat.schema != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is not supported\"\n      });\n    }\n    const groqOptions = parseProviderOptions({\n      provider: \"groq\",\n      providerOptions: providerMetadata,\n      schema: z.object({\n        reasoningFormat: z.enum([\"parsed\", \"raw\", \"hidden\"]).nullish()\n      })\n    });\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      user: this.settings.user,\n      parallel_tool_calls: this.settings.parallelToolCalls,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      stop: stopSequences,\n      seed,\n      // response format:\n      response_format: (\n        // json object response format is not supported for streaming:\n        stream === false && (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? { type: \"json_object\" } : void 0\n      ),\n      // provider options:\n      reasoning_format: groqOptions == null ? void 0 : groqOptions.reasoningFormat,\n      // messages:\n      messages: convertToGroqChatMessages(prompt)\n    };\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, toolWarnings } = prepareTools5({ mode });\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            response_format: (\n              // json object response format is not supported for streaming:\n              stream === false ? { type: \"json_object\" } : void 0\n            )\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: {\n              type: \"function\",\n              function: { name: mode.tool.name }\n            },\n            tools: [\n              {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters\n                }\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g;\n    const { args, warnings } = this.getArgs({ ...options, stream: false });\n    const body = JSON.stringify(args);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: groqFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        groqChatResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n    return {\n      text: (_a16 = choice.message.content) != null ? _a16 : void 0,\n      reasoning: (_b = choice.message.reasoning) != null ? _b : void 0,\n      toolCalls: (_c = choice.message.tool_calls) == null ? void 0 : _c.map((toolCall) => {\n        var _a23;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a23 = toolCall.id) != null ? _a23 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapGroqFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_e = (_d = response.usage) == null ? void 0 : _d.prompt_tokens) != null ? _e : NaN,\n        completionTokens: (_g = (_f = response.usage) == null ? void 0 : _f.completion_tokens) != null ? _g : NaN\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(response),\n      warnings,\n      request: { body }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = this.getArgs({ ...options, stream: true });\n    const body = JSON.stringify({ ...args, stream: true });\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: {\n        ...args,\n        stream: true\n      },\n      failedResponseHandler: groqFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(groqChatChunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: void 0,\n      completionTokens: void 0\n    };\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata(value)\n              });\n            }\n            if (((_a16 = value.x_groq) == null ? void 0 : _a16.usage) != null) {\n              usage = {\n                promptTokens: (_b = value.x_groq.usage.prompt_tokens) != null ? _b : void 0,\n                completionTokens: (_c = value.x_groq.usage.completion_tokens) != null ? _c : void 0\n              };\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapGroqFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.reasoning != null && delta.reasoning.length > 0) {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: delta.reasoning\n              });\n            }\n            if (delta.content != null && delta.content.length > 0) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: delta.content\n              });\n            }\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_d = toolCallDelta.function) == null ? void 0 : _d.name) == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_e = toolCallDelta.function.arguments) != null ? _e : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_f = toolCall2.function) == null ? void 0 : _f.name) != null && ((_g = toolCall2.function) == null ? void 0 : _g.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-call-delta\",\n                        toolCallType: \"function\",\n                        toolCallId: toolCall2.id,\n                        toolName: toolCall2.function.name,\n                        argsTextDelta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallType: \"function\",\n                        toolCallId: (_h = toolCall2.id) != null ? _h : generateId(),\n                        toolName: toolCall2.function.name,\n                        args: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_i = toolCallDelta.function) == null ? void 0 : _i.arguments) != null) {\n                  toolCall.function.arguments += (_k = (_j = toolCallDelta.function) == null ? void 0 : _j.arguments) != null ? _k : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: (_l = toolCallDelta.function.arguments) != null ? _l : \"\"\n                });\n                if (((_m = toolCall.function) == null ? void 0 : _m.name) != null && ((_n = toolCall.function) == null ? void 0 : _n.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: (_o = toolCall.id) != null ? _o : generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n          flush(controller) {\n            var _a16, _b;\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage: {\n                promptTokens: (_a16 = usage.promptTokens) != null ? _a16 : NaN,\n                completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n              },\n              ...{}\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body }\n    };\n  }\n};\nvar groqChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        content: z.string().nullish(),\n        reasoning: z.string().nullish(),\n        tool_calls: z.array(\n          z.object({\n            id: z.string().nullish(),\n            type: z.literal(\"function\"),\n            function: z.object({\n              name: z.string(),\n              arguments: z.string()\n            })\n          })\n        ).nullish()\n      }),\n      index: z.number(),\n      finish_reason: z.string().nullish()\n    })\n  ),\n  usage: z.object({\n    prompt_tokens: z.number().nullish(),\n    completion_tokens: z.number().nullish()\n  }).nullish()\n});\nvar groqChatChunkSchema = z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        delta: z.object({\n          content: z.string().nullish(),\n          reasoning: z.string().nullish(),\n          tool_calls: z.array(\n            z.object({\n              index: z.number(),\n              id: z.string().nullish(),\n              type: z.literal(\"function\").optional(),\n              function: z.object({\n                name: z.string().nullish(),\n                arguments: z.string().nullish()\n              })\n            })\n          ).nullish()\n        }).nullish(),\n        finish_reason: z.string().nullable().optional(),\n        index: z.number()\n      })\n    ),\n    x_groq: z.object({\n      usage: z.object({\n        prompt_tokens: z.number().nullish(),\n        completion_tokens: z.number().nullish()\n      }).nullish()\n    }).nullish()\n  }),\n  groqErrorDataSchema\n]);\nvar groqProviderOptionsSchema = z.object({\n  language: z.string().nullish(),\n  prompt: z.string().nullish(),\n  responseFormat: z.string().nullish(),\n  temperature: z.number().min(0).max(1).nullish(),\n  timestampGranularities: z.array(z.string()).nullish()\n});\nvar GroqTranscriptionModel = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    audio,\n    mediaType,\n    providerOptions\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const warnings = [];\n    const groqOptions = parseProviderOptions({\n      provider: \"groq\",\n      providerOptions,\n      schema: groqProviderOptionsSchema\n    });\n    const formData = new FormData();\n    const blob = audio instanceof Uint8Array ? new Blob([audio]) : new Blob([convertBase64ToUint8Array(audio)]);\n    formData.append(\"model\", this.modelId);\n    formData.append(\"file\", new File([blob], \"audio\", { type: mediaType }));\n    if (groqOptions) {\n      const transcriptionModelOptions = {\n        language: (_a16 = groqOptions.language) != null ? _a16 : void 0,\n        prompt: (_b = groqOptions.prompt) != null ? _b : void 0,\n        response_format: (_c = groqOptions.responseFormat) != null ? _c : void 0,\n        temperature: (_d = groqOptions.temperature) != null ? _d : void 0,\n        timestamp_granularities: (_e = groqOptions.timestampGranularities) != null ? _e : void 0\n      };\n      for (const key in transcriptionModelOptions) {\n        const value = transcriptionModelOptions[key];\n        if (value !== void 0) {\n          formData.append(key, String(value));\n        }\n      }\n    }\n    return {\n      formData,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { formData, warnings } = this.getArgs(options);\n    const {\n      value: response,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postFormDataToApi({\n      url: this.config.url({\n        path: \"/audio/transcriptions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      formData,\n      failedResponseHandler: groqFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        groqTranscriptionResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      text: response.text,\n      segments: (_e = (_d = response.segments) == null ? void 0 : _d.map((segment) => ({\n        text: segment.text,\n        startSecond: segment.start,\n        endSecond: segment.end\n      }))) != null ? _e : [],\n      language: response.language,\n      durationInSeconds: response.duration,\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar groqTranscriptionResponseSchema = z.object({\n  task: z.string(),\n  language: z.string(),\n  duration: z.number(),\n  text: z.string(),\n  segments: z.array(\n    z.object({\n      id: z.number(),\n      seek: z.number(),\n      start: z.number(),\n      end: z.number(),\n      text: z.string(),\n      tokens: z.array(z.number()),\n      temperature: z.number(),\n      avg_logprob: z.number(),\n      compression_ratio: z.number(),\n      no_speech_prob: z.number()\n    })\n  ),\n  x_groq: z.object({\n    id: z.string()\n  })\n});\nfunction createGroq(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash(options.baseURL)) != null ? _a16 : \"https://api.groq.com/openai/v1\";\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"GROQ_API_KEY\",\n      description: \"Groq\"\n    })}`,\n    ...options.headers\n  });\n  const createChatModel = (modelId, settings = {}) => new GroqChatLanguageModel(modelId, settings, {\n    provider: \"groq.chat\",\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId, settings) => {\n    if (new.target) {\n      throw new Error(\n        \"The Groq model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId, settings);\n  };\n  const createTranscriptionModel = (modelId) => {\n    return new GroqTranscriptionModel(modelId, {\n      provider: \"groq.transcription\",\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch\n    });\n  };\n  const provider = function(modelId, settings) {\n    return createLanguageModel(modelId, settings);\n  };\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.transcription = createTranscriptionModel;\n  return provider;\n}\nvar groq = createGroq();\nfunction convertToGroqChatMessages2(prompt) {\n  const messages = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        messages.push({ role: \"system\", content });\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part) => {\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"file\": {\n                if (!part.mediaType.startsWith(\"image/\")) {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: \"Non-image file content parts\"\n                  });\n                }\n                const mediaType = part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType;\n                return {\n                  type: \"image_url\",\n                  image_url: {\n                    url: part.data instanceof URL ? part.data.toString() : `data:${mediaType};base64,${convertToBase64(part.data)}`\n                  }\n                };\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        let reasoning = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            // groq supports reasoning for tool-calls in multi-turn conversations\n            // https://github.com/vercel/ai/issues/7860\n            case \"reasoning\": {\n              reasoning += part.text;\n              break;\n            }\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.input)\n                }\n              });\n              break;\n            }\n          }\n        }\n        messages.push({\n          role: \"assistant\",\n          content: text,\n          ...reasoning.length > 0 ? { reasoning } : null,\n          ...toolCalls.length > 0 ? { tool_calls: toolCalls } : null\n        });\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          const output = toolResponse.output;\n          let contentValue;\n          switch (output.type) {\n            case \"text\":\n            case \"error-text\":\n              contentValue = output.value;\n              break;\n            case \"content\":\n            case \"json\":\n            case \"error-json\":\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n          messages.push({\n            role: \"tool\",\n            tool_call_id: toolResponse.toolCallId,\n            content: contentValue\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return messages;\n}\nfunction getResponseMetadata2({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nvar groqProviderOptions = z$1.object({\n  reasoningFormat: z$1.enum([\"parsed\", \"raw\", \"hidden\"]).optional(),\n  reasoningEffort: z$1.string().optional(),\n  /**\n   * Whether to enable parallel function calling during tool use. Default to true.\n   */\n  parallelToolCalls: z$1.boolean().optional(),\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to\n   * monitor and detect abuse. Learn more.\n   */\n  user: z$1.string().optional(),\n  /**\n   * Whether to use structured outputs.\n   *\n   * @default true\n   */\n  structuredOutputs: z$1.boolean().optional()\n});\nvar groqErrorDataSchema2 = z$1.object({\n  error: z$1.object({\n    message: z$1.string(),\n    type: z$1.string()\n  })\n});\nvar groqFailedResponseHandler2 = createJsonErrorResponseHandler2({\n  errorSchema: groqErrorDataSchema2,\n  errorToMessage: (data) => data.error.message\n});\nvar BROWSER_SEARCH_SUPPORTED_MODELS = [\n  \"openai/gpt-oss-20b\",\n  \"openai/gpt-oss-120b\"\n];\nfunction isBrowserSearchSupportedModel(modelId) {\n  return BROWSER_SEARCH_SUPPORTED_MODELS.includes(modelId);\n}\nfunction getSupportedModelsString() {\n  return BROWSER_SEARCH_SUPPORTED_MODELS.join(\", \");\n}\nfunction prepareTools6({\n  tools,\n  toolChoice,\n  modelId\n}) {\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, toolChoice: void 0, toolWarnings };\n  }\n  const groqTools2 = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      if (tool2.id === \"groq.browser_search\") {\n        if (!isBrowserSearchSupportedModel(modelId)) {\n          toolWarnings.push({\n            type: \"unsupported-tool\",\n            tool: tool2,\n            details: `Browser search is only supported on the following models: ${getSupportedModelsString()}. Current model: ${modelId}`\n          });\n        } else {\n          groqTools2.push({\n            type: \"browser_search\"\n          });\n        }\n      } else {\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n      }\n    } else {\n      groqTools2.push({\n        type: \"function\",\n        function: {\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.inputSchema\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: groqTools2, toolChoice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: groqTools2, toolChoice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: groqTools2,\n        toolChoice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nfunction mapGroqFinishReason2(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar GroqChatLanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.supportedUrls = {\n      \"image/*\": [/^https?:\\/\\/.*$/]\n    };\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    stream,\n    tools,\n    toolChoice,\n    providerOptions\n  }) {\n    var _a16, _b;\n    const warnings = [];\n    const groqOptions = await parseProviderOptions2({\n      provider: \"groq\",\n      providerOptions,\n      schema: groqProviderOptions\n    });\n    const structuredOutputs = (_a16 = groqOptions == null ? void 0 : groqOptions.structuredOutputs) != null ? _a16 : true;\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !structuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const {\n      tools: groqTools2,\n      toolChoice: groqToolChoice,\n      toolWarnings\n    } = prepareTools6({ tools, toolChoice, modelId: this.modelId });\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n        // model specific settings:\n        user: groqOptions == null ? void 0 : groqOptions.user,\n        parallel_tool_calls: groqOptions == null ? void 0 : groqOptions.parallelToolCalls,\n        // standardized settings:\n        max_tokens: maxOutputTokens,\n        temperature,\n        top_p: topP,\n        frequency_penalty: frequencyPenalty,\n        presence_penalty: presencePenalty,\n        stop: stopSequences,\n        seed,\n        // response format:\n        response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? structuredOutputs && responseFormat.schema != null ? {\n          type: \"json_schema\",\n          json_schema: {\n            schema: responseFormat.schema,\n            name: (_b = responseFormat.name) != null ? _b : \"response\",\n            description: responseFormat.description\n          }\n        } : { type: \"json_object\" } : void 0,\n        // provider options:\n        reasoning_format: groqOptions == null ? void 0 : groqOptions.reasoningFormat,\n        reasoning_effort: groqOptions == null ? void 0 : groqOptions.reasoningEffort,\n        // messages:\n        messages: convertToGroqChatMessages2(prompt),\n        // tools:\n        tools: groqTools2,\n        tool_choice: groqToolChoice\n      },\n      warnings: [...warnings, ...toolWarnings]\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g;\n    const { args, warnings } = await this.getArgs({\n      ...options,\n      stream: false\n    });\n    const body = JSON.stringify(args);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: groqFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        groqChatResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const choice = response.choices[0];\n    const content = [];\n    const text = choice.message.content;\n    if (text != null && text.length > 0) {\n      content.push({ type: \"text\", text });\n    }\n    const reasoning = choice.message.reasoning;\n    if (reasoning != null && reasoning.length > 0) {\n      content.push({\n        type: \"reasoning\",\n        text: reasoning\n      });\n    }\n    if (choice.message.tool_calls != null) {\n      for (const toolCall of choice.message.tool_calls) {\n        content.push({\n          type: \"tool-call\",\n          toolCallId: (_a16 = toolCall.id) != null ? _a16 : generateId2(),\n          toolName: toolCall.function.name,\n          input: toolCall.function.arguments\n        });\n      }\n    }\n    return {\n      content,\n      finishReason: mapGroqFinishReason2(choice.finish_reason),\n      usage: {\n        inputTokens: (_c = (_b = response.usage) == null ? void 0 : _b.prompt_tokens) != null ? _c : void 0,\n        outputTokens: (_e = (_d = response.usage) == null ? void 0 : _d.completion_tokens) != null ? _e : void 0,\n        totalTokens: (_g = (_f = response.usage) == null ? void 0 : _f.total_tokens) != null ? _g : void 0\n      },\n      response: {\n        ...getResponseMetadata2(response),\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      warnings,\n      request: { body }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = await this.getArgs({ ...options, stream: true });\n    const body = JSON.stringify({ ...args, stream: true });\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: {\n        ...args,\n        stream: true\n      },\n      failedResponseHandler: groqFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(groqChatChunkSchema2),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    let isFirstChunk = true;\n    let isActiveText = false;\n    let isActiveReasoning = false;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata2(value)\n              });\n            }\n            if (((_a16 = value.x_groq) == null ? void 0 : _a16.usage) != null) {\n              usage.inputTokens = (_b = value.x_groq.usage.prompt_tokens) != null ? _b : void 0;\n              usage.outputTokens = (_c = value.x_groq.usage.completion_tokens) != null ? _c : void 0;\n              usage.totalTokens = (_d = value.x_groq.usage.total_tokens) != null ? _d : void 0;\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapGroqFinishReason2(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.reasoning != null && delta.reasoning.length > 0) {\n              if (!isActiveReasoning) {\n                controller.enqueue({\n                  type: \"reasoning-start\",\n                  id: \"reasoning-0\"\n                });\n                isActiveReasoning = true;\n              }\n              controller.enqueue({\n                type: \"reasoning-delta\",\n                id: \"reasoning-0\",\n                delta: delta.reasoning\n              });\n            }\n            if (delta.content != null && delta.content.length > 0) {\n              if (!isActiveText) {\n                controller.enqueue({ type: \"text-start\", id: \"txt-0\" });\n                isActiveText = true;\n              }\n              controller.enqueue({\n                type: \"text-delta\",\n                id: \"txt-0\",\n                delta: delta.content\n              });\n            }\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_e = toolCallDelta.function) == null ? void 0 : _e.name) == null) {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  controller.enqueue({\n                    type: \"tool-input-start\",\n                    id: toolCallDelta.id,\n                    toolName: toolCallDelta.function.name\n                  });\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_f = toolCallDelta.function.arguments) != null ? _f : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_g = toolCall2.function) == null ? void 0 : _g.name) != null && ((_h = toolCall2.function) == null ? void 0 : _h.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-input-delta\",\n                        id: toolCall2.id,\n                        delta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson2(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-input-end\",\n                        id: toolCall2.id\n                      });\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallId: (_i = toolCall2.id) != null ? _i : generateId2(),\n                        toolName: toolCall2.function.name,\n                        input: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_j = toolCallDelta.function) == null ? void 0 : _j.arguments) != null) {\n                  toolCall.function.arguments += (_l = (_k = toolCallDelta.function) == null ? void 0 : _k.arguments) != null ? _l : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-input-delta\",\n                  id: toolCall.id,\n                  delta: (_m = toolCallDelta.function.arguments) != null ? _m : \"\"\n                });\n                if (((_n = toolCall.function) == null ? void 0 : _n.name) != null && ((_o = toolCall.function) == null ? void 0 : _o.arguments) != null && isParsableJson2(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-input-end\",\n                    id: toolCall.id\n                  });\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallId: (_p = toolCall.id) != null ? _p : generateId2(),\n                    toolName: toolCall.function.name,\n                    input: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n          flush(controller) {\n            if (isActiveReasoning) {\n              controller.enqueue({ type: \"reasoning-end\", id: \"reasoning-0\" });\n            }\n            if (isActiveText) {\n              controller.enqueue({ type: \"text-end\", id: \"txt-0\" });\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n              ...{}\n            });\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar groqChatResponseSchema2 = z$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      message: z$1.object({\n        content: z$1.string().nullish(),\n        reasoning: z$1.string().nullish(),\n        tool_calls: z$1.array(\n          z$1.object({\n            id: z$1.string().nullish(),\n            type: z$1.literal(\"function\"),\n            function: z$1.object({\n              name: z$1.string(),\n              arguments: z$1.string()\n            })\n          })\n        ).nullish()\n      }),\n      index: z$1.number(),\n      finish_reason: z$1.string().nullish()\n    })\n  ),\n  usage: z$1.object({\n    prompt_tokens: z$1.number().nullish(),\n    completion_tokens: z$1.number().nullish(),\n    total_tokens: z$1.number().nullish()\n  }).nullish()\n});\nvar groqChatChunkSchema2 = z$1.union([\n  z$1.object({\n    id: z$1.string().nullish(),\n    created: z$1.number().nullish(),\n    model: z$1.string().nullish(),\n    choices: z$1.array(\n      z$1.object({\n        delta: z$1.object({\n          content: z$1.string().nullish(),\n          reasoning: z$1.string().nullish(),\n          tool_calls: z$1.array(\n            z$1.object({\n              index: z$1.number(),\n              id: z$1.string().nullish(),\n              type: z$1.literal(\"function\").optional(),\n              function: z$1.object({\n                name: z$1.string().nullish(),\n                arguments: z$1.string().nullish()\n              })\n            })\n          ).nullish()\n        }).nullish(),\n        finish_reason: z$1.string().nullable().optional(),\n        index: z$1.number()\n      })\n    ),\n    x_groq: z$1.object({\n      usage: z$1.object({\n        prompt_tokens: z$1.number().nullish(),\n        completion_tokens: z$1.number().nullish(),\n        total_tokens: z$1.number().nullish()\n      }).nullish()\n    }).nullish()\n  }),\n  groqErrorDataSchema2\n]);\nvar groqProviderOptionsSchema2 = z$1.object({\n  language: z$1.string().nullish(),\n  prompt: z$1.string().nullish(),\n  responseFormat: z$1.string().nullish(),\n  temperature: z$1.number().min(0).max(1).nullish(),\n  timestampGranularities: z$1.array(z$1.string()).nullish()\n});\nvar GroqTranscriptionModel2 = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    audio,\n    mediaType,\n    providerOptions\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const warnings = [];\n    const groqOptions = await parseProviderOptions2({\n      provider: \"groq\",\n      providerOptions,\n      schema: groqProviderOptionsSchema2\n    });\n    const formData = new FormData();\n    const blob = audio instanceof Uint8Array ? new Blob([audio]) : new Blob([convertBase64ToUint8Array2(audio)]);\n    formData.append(\"model\", this.modelId);\n    formData.append(\"file\", new File([blob], \"audio\", { type: mediaType }));\n    if (groqOptions) {\n      const transcriptionModelOptions = {\n        language: (_a16 = groqOptions.language) != null ? _a16 : void 0,\n        prompt: (_b = groqOptions.prompt) != null ? _b : void 0,\n        response_format: (_c = groqOptions.responseFormat) != null ? _c : void 0,\n        temperature: (_d = groqOptions.temperature) != null ? _d : void 0,\n        timestamp_granularities: (_e = groqOptions.timestampGranularities) != null ? _e : void 0\n      };\n      for (const key in transcriptionModelOptions) {\n        const value = transcriptionModelOptions[key];\n        if (value !== void 0) {\n          formData.append(key, String(value));\n        }\n      }\n    }\n    return {\n      formData,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { formData, warnings } = await this.getArgs(options);\n    const {\n      value: response,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postFormDataToApi2({\n      url: this.config.url({\n        path: \"/audio/transcriptions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      formData,\n      failedResponseHandler: groqFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        groqTranscriptionResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      text: response.text,\n      segments: (_e = (_d = response.segments) == null ? void 0 : _d.map((segment) => ({\n        text: segment.text,\n        startSecond: segment.start,\n        endSecond: segment.end\n      }))) != null ? _e : [],\n      language: response.language,\n      durationInSeconds: response.duration,\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar groqTranscriptionResponseSchema2 = z$1.object({\n  task: z$1.string(),\n  language: z$1.string(),\n  duration: z$1.number(),\n  text: z$1.string(),\n  segments: z$1.array(\n    z$1.object({\n      id: z$1.number(),\n      seek: z$1.number(),\n      start: z$1.number(),\n      end: z$1.number(),\n      text: z$1.string(),\n      tokens: z$1.array(z$1.number()),\n      temperature: z$1.number(),\n      avg_logprob: z$1.number(),\n      compression_ratio: z$1.number(),\n      no_speech_prob: z$1.number()\n    })\n  ),\n  x_groq: z$1.object({\n    id: z$1.string()\n  })\n});\nvar browserSearch = createProviderDefinedToolFactory({\n  id: \"groq.browser_search\",\n  name: \"browser_search\",\n  inputSchema: z$1.object({})\n});\nvar groqTools = {\n  browserSearch\n};\nfunction createGroq2(options = {}) {\n  var _a16;\n  const baseURL = (_a16 = withoutTrailingSlash2(options.baseURL)) != null ? _a16 : \"https://api.groq.com/openai/v1\";\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey2({\n      apiKey: options.apiKey,\n      environmentVariableName: \"GROQ_API_KEY\",\n      description: \"Groq\"\n    })}`,\n    ...options.headers\n  });\n  const createChatModel = (modelId) => new GroqChatLanguageModel2(modelId, {\n    provider: \"groq.chat\",\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId) => {\n    if (new.target) {\n      throw new Error(\n        \"The Groq model function cannot be called with the new keyword.\"\n      );\n    }\n    return createChatModel(modelId);\n  };\n  const createTranscriptionModel = (modelId) => {\n    return new GroqTranscriptionModel2(modelId, {\n      provider: \"groq.transcription\",\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch\n    });\n  };\n  const provider = function(modelId) {\n    return createLanguageModel(modelId);\n  };\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError2({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.imageModel = (modelId) => {\n    throw new NoSuchModelError2({ modelId, modelType: \"imageModel\" });\n  };\n  provider.transcription = createTranscriptionModel;\n  provider.tools = groqTools;\n  return provider;\n}\nvar groq2 = createGroq2();\nfunction convertToOpenAIChatMessages({\n  prompt,\n  useLegacyFunctionCalling = false,\n  systemMessageMode = \"system\"\n}) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a16, _b, _c, _d;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"image\": {\n                return {\n                  type: \"image_url\",\n                  image_url: {\n                    url: part.image instanceof URL ? part.image.toString() : `data:${(_a16 = part.mimeType) != null ? _a16 : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`,\n                    // OpenAI specific extension: image detail\n                    detail: (_c = (_b = part.providerMetadata) == null ? void 0 : _b.openai) == null ? void 0 : _c.imageDetail\n                  }\n                };\n              }\n              case \"file\": {\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: \"'File content parts with URL data' functionality not supported.\"\n                  });\n                }\n                switch (part.mimeType) {\n                  case \"audio/wav\": {\n                    return {\n                      type: \"input_audio\",\n                      input_audio: { data: part.data, format: \"wav\" }\n                    };\n                  }\n                  case \"audio/mp3\":\n                  case \"audio/mpeg\": {\n                    return {\n                      type: \"input_audio\",\n                      input_audio: { data: part.data, format: \"mp3\" }\n                    };\n                  }\n                  case \"application/pdf\": {\n                    return {\n                      type: \"file\",\n                      file: {\n                        filename: (_d = part.filename) != null ? _d : `part-${index}.pdf`,\n                        file_data: `data:application/pdf;base64,${part.data}`\n                      }\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: `File content part type ${part.mimeType} in user messages`\n                    });\n                  }\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args)\n                }\n              });\n              break;\n            }\n          }\n        }\n        if (useLegacyFunctionCalling) {\n          if (toolCalls.length > 1) {\n            throw new UnsupportedFunctionalityError({\n              functionality: \"useLegacyFunctionCalling with multiple tool calls in one message\"\n            });\n          }\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            function_call: toolCalls.length > 0 ? toolCalls[0].function : void 0\n          });\n        } else {\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            tool_calls: toolCalls.length > 0 ? toolCalls : void 0\n          });\n        }\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          if (useLegacyFunctionCalling) {\n            messages.push({\n              role: \"function\",\n              name: toolResponse.toolName,\n              content: JSON.stringify(toolResponse.result)\n            });\n          } else {\n            messages.push({\n              role: \"tool\",\n              tool_call_id: toolResponse.toolCallId,\n              content: JSON.stringify(toolResponse.result)\n            });\n          }\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\nfunction mapOpenAIChatLogProbsOutput(logprobs) {\n  var _a16, _b;\n  return (_b = (_a16 = logprobs == null ? void 0 : logprobs.content) == null ? void 0 : _a16.map(({ token, logprob, top_logprobs }) => ({\n    token,\n    logprob,\n    topLogprobs: top_logprobs ? top_logprobs.map(({ token: token2, logprob: logprob2 }) => ({\n      token: token2,\n      logprob: logprob2\n    })) : []\n  }))) != null ? _b : void 0;\n}\nfunction mapOpenAIFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar openaiErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish()\n  })\n});\nvar openaiFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: openaiErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\nfunction getResponseMetadata3({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nfunction prepareTools7({\n  mode,\n  useLegacyFunctionCalling = false,\n  structuredOutputs\n}) {\n  var _a16;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  if (useLegacyFunctionCalling) {\n    const openaiFunctions = [];\n    for (const tool2 of tools) {\n      if (tool2.type === \"provider-defined\") {\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n      } else {\n        openaiFunctions.push({\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.parameters\n        });\n      }\n    }\n    if (toolChoice == null) {\n      return {\n        functions: openaiFunctions,\n        function_call: void 0,\n        toolWarnings\n      };\n    }\n    const type2 = toolChoice.type;\n    switch (type2) {\n      case \"auto\":\n      case \"none\":\n      case void 0:\n        return {\n          functions: openaiFunctions,\n          function_call: void 0,\n          toolWarnings\n        };\n      case \"required\":\n        throw new UnsupportedFunctionalityError({\n          functionality: \"useLegacyFunctionCalling and toolChoice: required\"\n        });\n      default:\n        return {\n          functions: openaiFunctions,\n          function_call: { name: toolChoice.toolName },\n          toolWarnings\n        };\n    }\n  }\n  const openaiTools22 = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n    } else {\n      openaiTools22.push({\n        type: \"function\",\n        function: {\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.parameters,\n          strict: structuredOutputs ? true : void 0\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools22, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools22, tool_choice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: openaiTools22,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar OpenAIChatLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get supportsStructuredOutputs() {\n    var _a16;\n    return (_a16 = this.settings.structuredOutputs) != null ? _a16 : isReasoningModel(this.modelId);\n  }\n  get defaultObjectGenerationMode() {\n    if (isAudioModel(this.modelId)) {\n      return \"tool\";\n    }\n    return this.supportsStructuredOutputs ? \"json\" : \"tool\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get supportsImageUrls() {\n    return !this.settings.downloadImages;\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    providerMetadata\n  }) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !this.supportsStructuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const useLegacyFunctionCalling = this.settings.useLegacyFunctionCalling;\n    if (useLegacyFunctionCalling && this.settings.parallelToolCalls === true) {\n      throw new UnsupportedFunctionalityError({\n        functionality: \"useLegacyFunctionCalling with parallelToolCalls\"\n      });\n    }\n    if (useLegacyFunctionCalling && this.supportsStructuredOutputs) {\n      throw new UnsupportedFunctionalityError({\n        functionality: \"structuredOutputs with useLegacyFunctionCalling\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToOpenAIChatMessages(\n      {\n        prompt,\n        useLegacyFunctionCalling,\n        systemMessageMode: getSystemMessageMode(this.modelId)\n      }\n    );\n    warnings.push(...messageWarnings);\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      logit_bias: this.settings.logitBias,\n      logprobs: this.settings.logprobs === true || typeof this.settings.logprobs === \"number\" ? true : void 0,\n      top_logprobs: typeof this.settings.logprobs === \"number\" ? this.settings.logprobs : typeof this.settings.logprobs === \"boolean\" ? this.settings.logprobs ? 0 : void 0 : void 0,\n      user: this.settings.user,\n      parallel_tool_calls: this.settings.parallelToolCalls,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? this.supportsStructuredOutputs && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          strict: true,\n          name: (_a16 = responseFormat.name) != null ? _a16 : \"response\",\n          description: responseFormat.description\n        }\n      } : { type: \"json_object\" } : void 0,\n      stop: stopSequences,\n      seed,\n      // openai specific settings:\n      // TODO remove in next major version; we auto-map maxTokens now\n      max_completion_tokens: (_b = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _b.maxCompletionTokens,\n      store: (_c = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _c.store,\n      metadata: (_d = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _d.metadata,\n      prediction: (_e = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _e.prediction,\n      reasoning_effort: (_g = (_f = providerMetadata == null ? void 0 : providerMetadata.openai) == null ? void 0 : _f.reasoningEffort) != null ? _g : this.settings.reasoningEffort,\n      // messages:\n      messages\n    };\n    if (isReasoningModel(this.modelId)) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.frequency_penalty != null) {\n        baseArgs.frequency_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"frequencyPenalty\",\n          details: \"frequencyPenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.presence_penalty != null) {\n        baseArgs.presence_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"presencePenalty\",\n          details: \"presencePenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logit_bias != null) {\n        baseArgs.logit_bias = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logitBias is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logprobs != null) {\n        baseArgs.logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_logprobs != null) {\n        baseArgs.top_logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"topLogprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.max_tokens != null) {\n        if (baseArgs.max_completion_tokens == null) {\n          baseArgs.max_completion_tokens = baseArgs.max_tokens;\n        }\n        baseArgs.max_tokens = void 0;\n      }\n    } else if (this.modelId.startsWith(\"gpt-4o-search-preview\") || this.modelId.startsWith(\"gpt-4o-mini-search-preview\")) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for the search preview models and has been removed.\"\n        });\n      }\n    }\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, functions, function_call, toolWarnings } = prepareTools7({\n          mode,\n          useLegacyFunctionCalling,\n          structuredOutputs: this.supportsStructuredOutputs\n        });\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice,\n            functions,\n            function_call\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            response_format: this.supportsStructuredOutputs && mode.schema != null ? {\n              type: \"json_schema\",\n              json_schema: {\n                schema: mode.schema,\n                strict: true,\n                name: (_h = mode.name) != null ? _h : \"response\",\n                description: mode.description\n              }\n            } : { type: \"json_object\" }\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: useLegacyFunctionCalling ? {\n            ...baseArgs,\n            function_call: {\n              name: mode.tool.name\n            },\n            functions: [\n              {\n                name: mode.tool.name,\n                description: mode.tool.description,\n                parameters: mode.tool.parameters\n              }\n            ]\n          } : {\n            ...baseArgs,\n            tool_choice: {\n              type: \"function\",\n              function: { name: mode.tool.name }\n            },\n            tools: [\n              {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters,\n                  strict: this.supportsStructuredOutputs ? true : void 0\n                }\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h;\n    const { args: body, warnings } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiChatResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = body;\n    const choice = response.choices[0];\n    const completionTokenDetails = (_a16 = response.usage) == null ? void 0 : _a16.completion_tokens_details;\n    const promptTokenDetails = (_b = response.usage) == null ? void 0 : _b.prompt_tokens_details;\n    const providerMetadata = { openai: {} };\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens) != null) {\n      providerMetadata.openai.reasoningTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens) != null) {\n      providerMetadata.openai.acceptedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens) != null) {\n      providerMetadata.openai.rejectedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens;\n    }\n    if ((promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens) != null) {\n      providerMetadata.openai.cachedPromptTokens = promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens;\n    }\n    return {\n      text: (_c = choice.message.content) != null ? _c : void 0,\n      toolCalls: this.settings.useLegacyFunctionCalling && choice.message.function_call ? [\n        {\n          toolCallType: \"function\",\n          toolCallId: generateId(),\n          toolName: choice.message.function_call.name,\n          args: choice.message.function_call.arguments\n        }\n      ] : (_d = choice.message.tool_calls) == null ? void 0 : _d.map((toolCall) => {\n        var _a23;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a23 = toolCall.id) != null ? _a23 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_f = (_e = response.usage) == null ? void 0 : _e.prompt_tokens) != null ? _f : NaN,\n        completionTokens: (_h = (_g = response.usage) == null ? void 0 : _g.completion_tokens) != null ? _h : NaN\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      request: { body: JSON.stringify(body) },\n      response: getResponseMetadata3(response),\n      warnings,\n      logprobs: mapOpenAIChatLogProbsOutput(choice.logprobs),\n      providerMetadata\n    };\n  }\n  async doStream(options) {\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({ type: \"response-metadata\", ...result.response });\n          if (result.text) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: result.text\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: \"tool-call-delta\",\n                toolCallType: \"function\",\n                toolCallId: toolCall.toolCallId,\n                toolName: toolCall.toolName,\n                argsTextDelta: toolCall.args\n              });\n              controller.enqueue({\n                type: \"tool-call\",\n                ...toolCall\n              });\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings\n      };\n    }\n    const { args, warnings } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.compatibility === \"strict\" ? { include_usage: true } : void 0\n    };\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiChatChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: void 0,\n      completionTokens: void 0\n    };\n    let logprobs;\n    let isFirstChunk = true;\n    const { useLegacyFunctionCalling } = this.settings;\n    const providerMetadata = { openai: {} };\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata3(value)\n              });\n            }\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                prompt_tokens_details,\n                completion_tokens_details\n              } = value.usage;\n              usage = {\n                promptTokens: prompt_tokens != null ? prompt_tokens : void 0,\n                completionTokens: completion_tokens != null ? completion_tokens : void 0\n              };\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens) != null) {\n                providerMetadata.openai.reasoningTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens) != null) {\n                providerMetadata.openai.acceptedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens) != null) {\n                providerMetadata.openai.rejectedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens;\n              }\n              if ((prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens) != null) {\n                providerMetadata.openai.cachedPromptTokens = prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens;\n              }\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.content != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: delta.content\n              });\n            }\n            const mappedLogprobs = mapOpenAIChatLogProbsOutput(\n              choice == null ? void 0 : choice.logprobs\n            );\n            if (mappedLogprobs == null ? void 0 : mappedLogprobs.length) {\n              if (logprobs === void 0) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n            const mappedToolCalls = useLegacyFunctionCalling && delta.function_call != null ? [\n              {\n                type: \"function\",\n                id: generateId(),\n                function: delta.function_call,\n                index: 0\n              }\n            ] : delta.tool_calls;\n            if (mappedToolCalls != null) {\n              for (const toolCallDelta of mappedToolCalls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_a16 = toolCallDelta.function) == null ? void 0 : _a16.name) == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_b = toolCallDelta.function.arguments) != null ? _b : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_c = toolCall2.function) == null ? void 0 : _c.name) != null && ((_d = toolCall2.function) == null ? void 0 : _d.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-call-delta\",\n                        toolCallType: \"function\",\n                        toolCallId: toolCall2.id,\n                        toolName: toolCall2.function.name,\n                        argsTextDelta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallType: \"function\",\n                        toolCallId: (_e = toolCall2.id) != null ? _e : generateId(),\n                        toolName: toolCall2.function.name,\n                        args: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_f = toolCallDelta.function) == null ? void 0 : _f.arguments) != null) {\n                  toolCall.function.arguments += (_h = (_g = toolCallDelta.function) == null ? void 0 : _g.arguments) != null ? _h : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: (_i = toolCallDelta.function.arguments) != null ? _i : \"\"\n                });\n                if (((_j = toolCall.function) == null ? void 0 : _j.name) != null && ((_k = toolCall.function) == null ? void 0 : _k.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: (_l = toolCall.id) != null ? _l : generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n          flush(controller) {\n            var _a16, _b;\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              logprobs,\n              usage: {\n                promptTokens: (_a16 = usage.promptTokens) != null ? _a16 : NaN,\n                completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n              },\n              ...providerMetadata != null ? { providerMetadata } : {}\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings\n    };\n  }\n};\nvar openaiTokenUsageSchema = z.object({\n  prompt_tokens: z.number().nullish(),\n  completion_tokens: z.number().nullish(),\n  prompt_tokens_details: z.object({\n    cached_tokens: z.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z.object({\n    reasoning_tokens: z.number().nullish(),\n    accepted_prediction_tokens: z.number().nullish(),\n    rejected_prediction_tokens: z.number().nullish()\n  }).nullish()\n}).nullish();\nvar openaiChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal(\"assistant\").nullish(),\n        content: z.string().nullish(),\n        function_call: z.object({\n          arguments: z.string(),\n          name: z.string()\n        }).nullish(),\n        tool_calls: z.array(\n          z.object({\n            id: z.string().nullish(),\n            type: z.literal(\"function\"),\n            function: z.object({\n              name: z.string(),\n              arguments: z.string()\n            })\n          })\n        ).nullish()\n      }),\n      index: z.number(),\n      logprobs: z.object({\n        content: z.array(\n          z.object({\n            token: z.string(),\n            logprob: z.number(),\n            top_logprobs: z.array(\n              z.object({\n                token: z.string(),\n                logprob: z.number()\n              })\n            )\n          })\n        ).nullable()\n      }).nullish(),\n      finish_reason: z.string().nullish()\n    })\n  ),\n  usage: openaiTokenUsageSchema\n});\nvar openaiChatChunkSchema = z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        delta: z.object({\n          role: z.enum([\"assistant\"]).nullish(),\n          content: z.string().nullish(),\n          function_call: z.object({\n            name: z.string().optional(),\n            arguments: z.string().optional()\n          }).nullish(),\n          tool_calls: z.array(\n            z.object({\n              index: z.number(),\n              id: z.string().nullish(),\n              type: z.literal(\"function\").nullish(),\n              function: z.object({\n                name: z.string().nullish(),\n                arguments: z.string().nullish()\n              })\n            })\n          ).nullish()\n        }).nullish(),\n        logprobs: z.object({\n          content: z.array(\n            z.object({\n              token: z.string(),\n              logprob: z.number(),\n              top_logprobs: z.array(\n                z.object({\n                  token: z.string(),\n                  logprob: z.number()\n                })\n              )\n            })\n          ).nullable()\n        }).nullish(),\n        finish_reason: z.string().nullish(),\n        index: z.number()\n      })\n    ),\n    usage: openaiTokenUsageSchema\n  }),\n  openaiErrorDataSchema\n]);\nfunction isReasoningModel(modelId) {\n  return modelId.startsWith(\"o\") || modelId.startsWith(\"gpt-5\");\n}\nfunction isAudioModel(modelId) {\n  return modelId.startsWith(\"gpt-4o-audio-preview\");\n}\nfunction getSystemMessageMode(modelId) {\n  var _a16, _b;\n  if (!isReasoningModel(modelId)) {\n    return \"system\";\n  }\n  return (_b = (_a16 = reasoningModels[modelId]) == null ? void 0 : _a16.systemMessageMode) != null ? _b : \"developer\";\n}\nvar reasoningModels = {\n  \"o1-mini\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-mini-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  o3: {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini-2025-01-31\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  }\n};\nfunction convertToOpenAICompletionPrompt({\n  prompt,\n  inputFormat,\n  user = \"user\",\n  assistant = \"assistant\"\n}) {\n  if (inputFormat === \"prompt\" && prompt.length === 1 && prompt[0].role === \"user\" && prompt[0].content.length === 1 && prompt[0].content[0].type === \"text\") {\n    return { prompt: prompt[0].content[0].text };\n  }\n  let text = \"\";\n  if (prompt[0].role === \"system\") {\n    text += `${prompt[0].content}\n\n`;\n    prompt = prompt.slice(1);\n  }\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        throw new InvalidPromptError({\n          message: \"Unexpected system message in prompt: ${content}\",\n          prompt\n        });\n      }\n      case \"user\": {\n        const userMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n            case \"image\": {\n              throw new UnsupportedFunctionalityError({\n                functionality: \"images\"\n              });\n            }\n          }\n        }).join(\"\");\n        text += `${user}:\n${userMessage}\n\n`;\n        break;\n      }\n      case \"assistant\": {\n        const assistantMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n            case \"tool-call\": {\n              throw new UnsupportedFunctionalityError({\n                functionality: \"tool-call messages\"\n              });\n            }\n          }\n        }).join(\"\");\n        text += `${assistant}:\n${assistantMessage}\n\n`;\n        break;\n      }\n      case \"tool\": {\n        throw new UnsupportedFunctionalityError({\n          functionality: \"tool messages\"\n        });\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  text += `${assistant}:\n`;\n  return {\n    prompt: text,\n    stopSequences: [`\n${user}:`]\n  };\n}\nfunction mapOpenAICompletionLogProbs(logprobs) {\n  return logprobs == null ? void 0 : logprobs.tokens.map((token, index) => ({\n    token,\n    logprob: logprobs.token_logprobs[index],\n    topLogprobs: logprobs.top_logprobs ? Object.entries(logprobs.top_logprobs[index]).map(\n      ([token2, logprob]) => ({\n        token: token2,\n        logprob\n      })\n    ) : []\n  }));\n}\nvar OpenAICompletionLanguageModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = void 0;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed\n  }) {\n    var _a16;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const { prompt: completionPrompt, stopSequences } = convertToOpenAICompletionPrompt({ prompt, inputFormat });\n    const stop = [...stopSequences != null ? stopSequences : [], ...userStopSequences != null ? userStopSequences : []];\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      logprobs: typeof this.settings.logprobs === \"number\" ? this.settings.logprobs : typeof this.settings.logprobs === \"boolean\" ? this.settings.logprobs ? 0 : void 0 : void 0,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      // prompt:\n      prompt: completionPrompt,\n      // stop sequences:\n      stop: stop.length > 0 ? stop : void 0\n    };\n    switch (type) {\n      case \"regular\": {\n        if ((_a16 = mode.tools) == null ? void 0 : _a16.length) {\n          throw new UnsupportedFunctionalityError({\n            functionality: \"tools\"\n          });\n        }\n        if (mode.toolChoice) {\n          throw new UnsupportedFunctionalityError({\n            functionality: \"toolChoice\"\n          });\n        }\n        return { args: baseArgs, warnings };\n      }\n      case \"object-json\": {\n        throw new UnsupportedFunctionalityError({\n          functionality: \"object-json mode\"\n        });\n      }\n      case \"object-tool\": {\n        throw new UnsupportedFunctionalityError({\n          functionality: \"object-tool mode\"\n        });\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    const { args, warnings } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompletionResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: response.usage.prompt_tokens,\n        completionTokens: response.usage.completion_tokens\n      },\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      logprobs: mapOpenAICompletionLogProbs(choice.logprobs),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata3(response),\n      warnings,\n      request: { body: JSON.stringify(args) }\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.compatibility === \"strict\" ? { include_usage: true } : void 0\n    };\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiCompletionChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    let logprobs;\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata3(value)\n              });\n            }\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens,\n                completionTokens: value.usage.completion_tokens\n              };\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.text) != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: choice.text\n              });\n            }\n            const mappedLogprobs = mapOpenAICompletionLogProbs(\n              choice == null ? void 0 : choice.logprobs\n            );\n            if (mappedLogprobs == null ? void 0 : mappedLogprobs.length) {\n              if (logprobs === void 0) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n          },\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              logprobs,\n              usage\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) }\n    };\n  }\n};\nvar openaiCompletionResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string(),\n      logprobs: z.object({\n        tokens: z.array(z.string()),\n        token_logprobs: z.array(z.number()),\n        top_logprobs: z.array(z.record(z.string(), z.number())).nullable()\n      }).nullish()\n    })\n  ),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number()\n  })\n});\nvar openaiCompletionChunkSchema = z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        text: z.string(),\n        finish_reason: z.string().nullish(),\n        index: z.number(),\n        logprobs: z.object({\n          tokens: z.array(z.string()),\n          token_logprobs: z.array(z.number()),\n          top_logprobs: z.array(z.record(z.string(), z.number())).nullable()\n        }).nullish()\n      })\n    ),\n    usage: z.object({\n      prompt_tokens: z.number(),\n      completion_tokens: z.number()\n    }).nullish()\n  }),\n  openaiErrorDataSchema\n]);\nvar OpenAIEmbeddingModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get maxEmbeddingsPerCall() {\n    var _a16;\n    return (_a16 = this.settings.maxEmbeddingsPerCall) != null ? _a16 : 2048;\n  }\n  get supportsParallelCalls() {\n    var _a16;\n    return (_a16 = this.settings.supportsParallelCalls) != null ? _a16 : true;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal\n  }) {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/embeddings\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: \"float\",\n        dimensions: this.settings.dimensions,\n        user: this.settings.user\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTextEmbeddingResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.data.map((item) => item.embedding),\n      usage: response.usage ? { tokens: response.usage.prompt_tokens } : void 0,\n      rawResponse: { headers: responseHeaders }\n    };\n  }\n};\nvar openaiTextEmbeddingResponseSchema = z.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish()\n});\nvar modelMaxImagesPerCall = {\n  \"dall-e-3\": 1,\n  \"dall-e-2\": 10,\n  \"gpt-image-1\": 10\n};\nvar hasDefaultResponseFormat = /* @__PURE__ */ new Set([\"gpt-image-1\"]);\nvar OpenAIImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get maxImagesPerCall() {\n    var _a16, _b;\n    return (_b = (_a16 = this.settings.maxImagesPerCall) != null ? _a16 : modelMaxImagesPerCall[this.modelId]) != null ? _b : 1;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a16, _b, _c, _d;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(_d = providerOptions.openai) != null ? _d : {},\n        ...!hasDefaultResponseFormat.has(this.modelId) ? { response_format: \"b64_json\" } : {}\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiImageResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map((item) => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() }))\n});\nvar openAIProviderOptionsSchema = z.object({\n  include: z.array(z.string()).nullish(),\n  language: z.string().nullish(),\n  prompt: z.string().nullish(),\n  temperature: z.number().min(0).max(1).nullish().default(0),\n  timestampGranularities: z.array(z.enum([\"word\", \"segment\"])).nullish().default([\"segment\"])\n});\nvar languageMap = {\n  afrikaans: \"af\",\n  arabic: \"ar\",\n  armenian: \"hy\",\n  azerbaijani: \"az\",\n  belarusian: \"be\",\n  bosnian: \"bs\",\n  bulgarian: \"bg\",\n  catalan: \"ca\",\n  chinese: \"zh\",\n  croatian: \"hr\",\n  czech: \"cs\",\n  danish: \"da\",\n  dutch: \"nl\",\n  english: \"en\",\n  estonian: \"et\",\n  finnish: \"fi\",\n  french: \"fr\",\n  galician: \"gl\",\n  german: \"de\",\n  greek: \"el\",\n  hebrew: \"he\",\n  hindi: \"hi\",\n  hungarian: \"hu\",\n  icelandic: \"is\",\n  indonesian: \"id\",\n  italian: \"it\",\n  japanese: \"ja\",\n  kannada: \"kn\",\n  kazakh: \"kk\",\n  korean: \"ko\",\n  latvian: \"lv\",\n  lithuanian: \"lt\",\n  macedonian: \"mk\",\n  malay: \"ms\",\n  marathi: \"mr\",\n  maori: \"mi\",\n  nepali: \"ne\",\n  norwegian: \"no\",\n  persian: \"fa\",\n  polish: \"pl\",\n  portuguese: \"pt\",\n  romanian: \"ro\",\n  russian: \"ru\",\n  serbian: \"sr\",\n  slovak: \"sk\",\n  slovenian: \"sl\",\n  spanish: \"es\",\n  swahili: \"sw\",\n  swedish: \"sv\",\n  tagalog: \"tl\",\n  tamil: \"ta\",\n  thai: \"th\",\n  turkish: \"tr\",\n  ukrainian: \"uk\",\n  urdu: \"ur\",\n  vietnamese: \"vi\",\n  welsh: \"cy\"\n};\nvar OpenAITranscriptionModel = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    audio,\n    mediaType,\n    providerOptions\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const warnings = [];\n    const openAIOptions = parseProviderOptions({\n      provider: \"openai\",\n      providerOptions,\n      schema: openAIProviderOptionsSchema\n    });\n    const formData = new FormData();\n    const blob = audio instanceof Uint8Array ? new Blob([audio]) : new Blob([convertBase64ToUint8Array(audio)]);\n    formData.append(\"model\", this.modelId);\n    formData.append(\"file\", new File([blob], \"audio\", { type: mediaType }));\n    if (openAIOptions) {\n      const transcriptionModelOptions = {\n        include: (_a16 = openAIOptions.include) != null ? _a16 : void 0,\n        language: (_b = openAIOptions.language) != null ? _b : void 0,\n        prompt: (_c = openAIOptions.prompt) != null ? _c : void 0,\n        temperature: (_d = openAIOptions.temperature) != null ? _d : void 0,\n        timestamp_granularities: (_e = openAIOptions.timestampGranularities) != null ? _e : void 0\n      };\n      for (const key in transcriptionModelOptions) {\n        const value = transcriptionModelOptions[key];\n        if (value !== void 0) {\n          formData.append(key, String(value));\n        }\n      }\n    }\n    return {\n      formData,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { formData, warnings } = this.getArgs(options);\n    const {\n      value: response,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postFormDataToApi({\n      url: this.config.url({\n        path: \"/audio/transcriptions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      formData,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTranscriptionResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const language = response.language != null && response.language in languageMap ? languageMap[response.language] : void 0;\n    return {\n      text: response.text,\n      segments: (_e = (_d = response.words) == null ? void 0 : _d.map((word) => ({\n        text: word.word,\n        startSecond: word.start,\n        endSecond: word.end\n      }))) != null ? _e : [],\n      language,\n      durationInSeconds: (_f = response.duration) != null ? _f : void 0,\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar openaiTranscriptionResponseSchema = z.object({\n  text: z.string(),\n  language: z.string().nullish(),\n  duration: z.number().nullish(),\n  words: z.array(\n    z.object({\n      word: z.string(),\n      start: z.number(),\n      end: z.number()\n    })\n  ).nullish()\n});\nfunction convertToOpenAIResponsesMessages({\n  prompt,\n  systemMessageMode\n}) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a16, _b, _c, _d;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"input_text\", text: part.text };\n              }\n              case \"image\": {\n                return {\n                  type: \"input_image\",\n                  image_url: part.image instanceof URL ? part.image.toString() : `data:${(_a16 = part.mimeType) != null ? _a16 : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`,\n                  // OpenAI specific extension: image detail\n                  detail: (_c = (_b = part.providerMetadata) == null ? void 0 : _b.openai) == null ? void 0 : _c.imageDetail\n                };\n              }\n              case \"file\": {\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: \"File URLs in user messages\"\n                  });\n                }\n                switch (part.mimeType) {\n                  case \"application/pdf\": {\n                    return {\n                      type: \"input_file\",\n                      filename: (_d = part.filename) != null ? _d : `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${part.data}`\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: \"Only PDF files are supported in user messages\"\n                    });\n                  }\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              messages.push({\n                role: \"assistant\",\n                content: [{ type: \"output_text\", text: part.text }]\n              });\n              break;\n            }\n            case \"tool-call\": {\n              messages.push({\n                type: \"function_call\",\n                call_id: part.toolCallId,\n                name: part.toolName,\n                arguments: JSON.stringify(part.args)\n              });\n              break;\n            }\n          }\n        }\n        break;\n      }\n      case \"tool\": {\n        for (const part of content) {\n          messages.push({\n            type: \"function_call_output\",\n            call_id: part.toolCallId,\n            output: JSON.stringify(part.result)\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\nfunction mapOpenAIResponseFinishReason({\n  finishReason,\n  hasToolCalls\n}) {\n  switch (finishReason) {\n    case void 0:\n    case null:\n      return hasToolCalls ? \"tool-calls\" : \"stop\";\n    case \"max_output_tokens\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    default:\n      return hasToolCalls ? \"tool-calls\" : \"unknown\";\n  }\n}\nfunction prepareResponsesTools({\n  mode,\n  strict\n}) {\n  var _a16;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  const openaiTools22 = [];\n  for (const tool2 of tools) {\n    switch (tool2.type) {\n      case \"function\":\n        openaiTools22.push({\n          type: \"function\",\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.parameters,\n          strict: strict ? true : void 0\n        });\n        break;\n      case \"provider-defined\":\n        switch (tool2.id) {\n          case \"openai.web_search_preview\":\n            openaiTools22.push({\n              type: \"web_search_preview\",\n              search_context_size: tool2.args.searchContextSize,\n              user_location: tool2.args.userLocation\n            });\n            break;\n          default:\n            toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools22, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools22, tool_choice: type, toolWarnings };\n    case \"tool\": {\n      if (toolChoice.toolName === \"web_search_preview\") {\n        return {\n          tools: openaiTools22,\n          tool_choice: {\n            type: \"web_search_preview\"\n          },\n          toolWarnings\n        };\n      }\n      return {\n        tools: openaiTools22,\n        tool_choice: {\n          type: \"function\",\n          name: toolChoice.toolName\n        },\n        toolWarnings\n      };\n    }\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar OpenAIResponsesLanguageModel = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = \"json\";\n    this.supportsStructuredOutputs = true;\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    mode,\n    maxTokens,\n    temperature,\n    stopSequences,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    prompt,\n    providerMetadata,\n    responseFormat\n  }) {\n    var _a16, _b, _c;\n    const warnings = [];\n    const modelConfig = getResponsesModelConfig(this.modelId);\n    const type = mode.type;\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (stopSequences != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"stopSequences\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToOpenAIResponsesMessages({\n      prompt,\n      systemMessageMode: modelConfig.systemMessageMode\n    });\n    warnings.push(...messageWarnings);\n    const openaiOptions = parseProviderOptions({\n      provider: \"openai\",\n      providerOptions: providerMetadata,\n      schema: openaiResponsesProviderOptionsSchema\n    });\n    const isStrict = (_a16 = openaiOptions == null ? void 0 : openaiOptions.strictSchemas) != null ? _a16 : true;\n    const baseArgs = {\n      model: this.modelId,\n      input: messages,\n      temperature,\n      top_p: topP,\n      max_output_tokens: maxTokens,\n      ...(responseFormat == null ? void 0 : responseFormat.type) === \"json\" && {\n        text: {\n          format: responseFormat.schema != null ? {\n            type: \"json_schema\",\n            strict: isStrict,\n            name: (_b = responseFormat.name) != null ? _b : \"response\",\n            description: responseFormat.description,\n            schema: responseFormat.schema\n          } : { type: \"json_object\" }\n        }\n      },\n      // provider options:\n      metadata: openaiOptions == null ? void 0 : openaiOptions.metadata,\n      parallel_tool_calls: openaiOptions == null ? void 0 : openaiOptions.parallelToolCalls,\n      previous_response_id: openaiOptions == null ? void 0 : openaiOptions.previousResponseId,\n      store: openaiOptions == null ? void 0 : openaiOptions.store,\n      user: openaiOptions == null ? void 0 : openaiOptions.user,\n      instructions: openaiOptions == null ? void 0 : openaiOptions.instructions,\n      // model-specific settings:\n      ...modelConfig.isReasoningModel && ((openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null || (openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null) && {\n        reasoning: {\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null && {\n            effort: openaiOptions.reasoningEffort\n          },\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null && {\n            summary: openaiOptions.reasoningSummary\n          }\n        }\n      },\n      ...modelConfig.requiredAutoTruncation && {\n        truncation: \"auto\"\n      }\n    };\n    if (modelConfig.isReasoningModel) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n    }\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, toolWarnings } = prepareResponsesTools({\n          mode,\n          strict: isStrict\n          // TODO support provider options on tools\n        });\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice\n          },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            text: {\n              format: mode.schema != null ? {\n                type: \"json_schema\",\n                strict: isStrict,\n                name: (_c = mode.name) != null ? _c : \"response\",\n                description: mode.description,\n                schema: mode.schema\n              } : { type: \"json_object\" }\n            }\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: { type: \"function\", name: mode.tool.name },\n            tools: [\n              {\n                type: \"function\",\n                name: mode.tool.name,\n                description: mode.tool.description,\n                parameters: mode.tool.parameters,\n                strict: isStrict\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g;\n    const { args: body, warnings } = this.getArgs(options);\n    const url = this.config.url({\n      path: \"/responses\",\n      modelId: this.modelId\n    });\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url,\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        z.object({\n          id: z.string(),\n          created_at: z.number(),\n          error: z.object({\n            message: z.string(),\n            code: z.string()\n          }).nullish(),\n          model: z.string(),\n          output: z.array(\n            z.discriminatedUnion(\"type\", [\n              z.object({\n                type: z.literal(\"message\"),\n                role: z.literal(\"assistant\"),\n                content: z.array(\n                  z.object({\n                    type: z.literal(\"output_text\"),\n                    text: z.string(),\n                    annotations: z.array(\n                      z.object({\n                        type: z.literal(\"url_citation\"),\n                        start_index: z.number(),\n                        end_index: z.number(),\n                        url: z.string(),\n                        title: z.string()\n                      })\n                    )\n                  })\n                )\n              }),\n              z.object({\n                type: z.literal(\"function_call\"),\n                call_id: z.string(),\n                name: z.string(),\n                arguments: z.string()\n              }),\n              z.object({\n                type: z.literal(\"web_search_call\")\n              }),\n              z.object({\n                type: z.literal(\"computer_call\")\n              }),\n              z.object({\n                type: z.literal(\"reasoning\"),\n                summary: z.array(\n                  z.object({\n                    type: z.literal(\"summary_text\"),\n                    text: z.string()\n                  })\n                )\n              })\n            ])\n          ),\n          incomplete_details: z.object({ reason: z.string() }).nullable(),\n          usage: usageSchema2\n        })\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    if (response.error) {\n      throw new APICallError({\n        message: response.error.message,\n        url,\n        requestBodyValues: body,\n        statusCode: 400,\n        responseHeaders,\n        responseBody: rawResponse,\n        isRetryable: false\n      });\n    }\n    const outputTextElements = response.output.filter((output) => output.type === \"message\").flatMap((output) => output.content).filter((content) => content.type === \"output_text\");\n    const toolCalls = response.output.filter((output) => output.type === \"function_call\").map((output) => ({\n      toolCallType: \"function\",\n      toolCallId: output.call_id,\n      toolName: output.name,\n      args: output.arguments\n    }));\n    const reasoningSummary = (_b = (_a16 = response.output.find((item) => item.type === \"reasoning\")) == null ? void 0 : _a16.summary) != null ? _b : null;\n    return {\n      text: outputTextElements.map((content) => content.text).join(\"\\n\"),\n      sources: outputTextElements.flatMap(\n        (content) => content.annotations.map((annotation) => {\n          var _a23, _b2, _c2;\n          return {\n            sourceType: \"url\",\n            id: (_c2 = (_b2 = (_a23 = this.config).generateId) == null ? void 0 : _b2.call(_a23)) != null ? _c2 : generateId(),\n            url: annotation.url,\n            title: annotation.title\n          };\n        })\n      ),\n      finishReason: mapOpenAIResponseFinishReason({\n        finishReason: (_c = response.incomplete_details) == null ? void 0 : _c.reason,\n        hasToolCalls: toolCalls.length > 0\n      }),\n      toolCalls: toolCalls.length > 0 ? toolCalls : void 0,\n      reasoning: reasoningSummary ? reasoningSummary.map((summary) => ({\n        type: \"text\",\n        text: summary.text\n      })) : void 0,\n      usage: {\n        promptTokens: response.usage.input_tokens,\n        completionTokens: response.usage.output_tokens\n      },\n      rawCall: {\n        rawPrompt: void 0,\n        rawSettings: {}\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      request: {\n        body: JSON.stringify(body)\n      },\n      response: {\n        id: response.id,\n        timestamp: new Date(response.created_at * 1e3),\n        modelId: response.model\n      },\n      providerMetadata: {\n        openai: {\n          responseId: response.id,\n          cachedPromptTokens: (_e = (_d = response.usage.input_tokens_details) == null ? void 0 : _d.cached_tokens) != null ? _e : null,\n          reasoningTokens: (_g = (_f = response.usage.output_tokens_details) == null ? void 0 : _f.reasoning_tokens) != null ? _g : null\n        }\n      },\n      warnings\n    };\n  }\n  async doStream(options) {\n    const { args: body, warnings } = this.getArgs(options);\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/responses\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: {\n        ...body,\n        stream: true\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiResponsesChunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const self = this;\n    let finishReason = \"unknown\";\n    let promptTokens = NaN;\n    let completionTokens = NaN;\n    let cachedPromptTokens = null;\n    let reasoningTokens = null;\n    let responseId = null;\n    const ongoingToolCalls = {};\n    let hasToolCalls = false;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (isResponseOutputItemAddedChunk(value)) {\n              if (value.item.type === \"function_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: value.item.name,\n                  toolCallId: value.item.call_id\n                };\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: value.item.call_id,\n                  toolName: value.item.name,\n                  argsTextDelta: value.item.arguments\n                });\n              }\n            } else if (isResponseFunctionCallArgumentsDeltaChunk(value)) {\n              const toolCall = ongoingToolCalls[value.output_index];\n              if (toolCall != null) {\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.toolCallId,\n                  toolName: toolCall.toolName,\n                  argsTextDelta: value.delta\n                });\n              }\n            } else if (isResponseCreatedChunk(value)) {\n              responseId = value.response.id;\n              controller.enqueue({\n                type: \"response-metadata\",\n                id: value.response.id,\n                timestamp: new Date(value.response.created_at * 1e3),\n                modelId: value.response.model\n              });\n            } else if (isTextDeltaChunk(value)) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: value.delta\n              });\n            } else if (isResponseReasoningSummaryTextDeltaChunk(value)) {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: value.delta\n              });\n            } else if (isResponseOutputItemDoneChunk(value) && value.item.type === \"function_call\") {\n              ongoingToolCalls[value.output_index] = void 0;\n              hasToolCalls = true;\n              controller.enqueue({\n                type: \"tool-call\",\n                toolCallType: \"function\",\n                toolCallId: value.item.call_id,\n                toolName: value.item.name,\n                args: value.item.arguments\n              });\n            } else if (isResponseFinishedChunk(value)) {\n              finishReason = mapOpenAIResponseFinishReason({\n                finishReason: (_a16 = value.response.incomplete_details) == null ? void 0 : _a16.reason,\n                hasToolCalls\n              });\n              promptTokens = value.response.usage.input_tokens;\n              completionTokens = value.response.usage.output_tokens;\n              cachedPromptTokens = (_c = (_b = value.response.usage.input_tokens_details) == null ? void 0 : _b.cached_tokens) != null ? _c : cachedPromptTokens;\n              reasoningTokens = (_e = (_d = value.response.usage.output_tokens_details) == null ? void 0 : _d.reasoning_tokens) != null ? _e : reasoningTokens;\n            } else if (isResponseAnnotationAddedChunk(value)) {\n              controller.enqueue({\n                type: \"source\",\n                source: {\n                  sourceType: \"url\",\n                  id: (_h = (_g = (_f = self.config).generateId) == null ? void 0 : _g.call(_f)) != null ? _h : generateId(),\n                  url: value.annotation.url,\n                  title: value.annotation.title\n                }\n              });\n            } else if (isErrorChunk(value)) {\n              controller.enqueue({ type: \"error\", error: value });\n            }\n          },\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage: { promptTokens, completionTokens },\n              ...(cachedPromptTokens != null || reasoningTokens != null) && {\n                providerMetadata: {\n                  openai: {\n                    responseId,\n                    cachedPromptTokens,\n                    reasoningTokens\n                  }\n                }\n              }\n            });\n          }\n        })\n      ),\n      rawCall: {\n        rawPrompt: void 0,\n        rawSettings: {}\n      },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings\n    };\n  }\n};\nvar usageSchema2 = z.object({\n  input_tokens: z.number(),\n  input_tokens_details: z.object({ cached_tokens: z.number().nullish() }).nullish(),\n  output_tokens: z.number(),\n  output_tokens_details: z.object({ reasoning_tokens: z.number().nullish() }).nullish()\n});\nvar textDeltaChunkSchema = z.object({\n  type: z.literal(\"response.output_text.delta\"),\n  delta: z.string()\n});\nvar responseFinishedChunkSchema = z.object({\n  type: z.enum([\"response.completed\", \"response.incomplete\"]),\n  response: z.object({\n    incomplete_details: z.object({ reason: z.string() }).nullish(),\n    usage: usageSchema2\n  })\n});\nvar responseCreatedChunkSchema = z.object({\n  type: z.literal(\"response.created\"),\n  response: z.object({\n    id: z.string(),\n    created_at: z.number(),\n    model: z.string()\n  })\n});\nvar responseOutputItemDoneSchema = z.object({\n  type: z.literal(\"response.output_item.done\"),\n  output_index: z.number(),\n  item: z.discriminatedUnion(\"type\", [\n    z.object({\n      type: z.literal(\"message\")\n    }),\n    z.object({\n      type: z.literal(\"function_call\"),\n      id: z.string(),\n      call_id: z.string(),\n      name: z.string(),\n      arguments: z.string(),\n      status: z.literal(\"completed\")\n    })\n  ])\n});\nvar responseFunctionCallArgumentsDeltaSchema = z.object({\n  type: z.literal(\"response.function_call_arguments.delta\"),\n  item_id: z.string(),\n  output_index: z.number(),\n  delta: z.string()\n});\nvar responseOutputItemAddedSchema = z.object({\n  type: z.literal(\"response.output_item.added\"),\n  output_index: z.number(),\n  item: z.discriminatedUnion(\"type\", [\n    z.object({\n      type: z.literal(\"message\")\n    }),\n    z.object({\n      type: z.literal(\"function_call\"),\n      id: z.string(),\n      call_id: z.string(),\n      name: z.string(),\n      arguments: z.string()\n    })\n  ])\n});\nvar responseAnnotationAddedSchema = z.object({\n  type: z.literal(\"response.output_text.annotation.added\"),\n  annotation: z.object({\n    type: z.literal(\"url_citation\"),\n    url: z.string(),\n    title: z.string()\n  })\n});\nvar responseReasoningSummaryTextDeltaSchema = z.object({\n  type: z.literal(\"response.reasoning_summary_text.delta\"),\n  item_id: z.string(),\n  output_index: z.number(),\n  summary_index: z.number(),\n  delta: z.string()\n});\nvar errorChunkSchema = z.object({\n  type: z.literal(\"error\"),\n  code: z.string(),\n  message: z.string(),\n  param: z.string().nullish(),\n  sequence_number: z.number()\n});\nvar openaiResponsesChunkSchema = z.union([\n  textDeltaChunkSchema,\n  responseFinishedChunkSchema,\n  responseCreatedChunkSchema,\n  responseOutputItemDoneSchema,\n  responseFunctionCallArgumentsDeltaSchema,\n  responseOutputItemAddedSchema,\n  responseAnnotationAddedSchema,\n  responseReasoningSummaryTextDeltaSchema,\n  errorChunkSchema,\n  z.object({ type: z.string() }).passthrough()\n  // fallback for unknown chunks\n]);\nfunction isTextDeltaChunk(chunk) {\n  return chunk.type === \"response.output_text.delta\";\n}\nfunction isResponseOutputItemDoneChunk(chunk) {\n  return chunk.type === \"response.output_item.done\";\n}\nfunction isResponseFinishedChunk(chunk) {\n  return chunk.type === \"response.completed\" || chunk.type === \"response.incomplete\";\n}\nfunction isResponseCreatedChunk(chunk) {\n  return chunk.type === \"response.created\";\n}\nfunction isResponseFunctionCallArgumentsDeltaChunk(chunk) {\n  return chunk.type === \"response.function_call_arguments.delta\";\n}\nfunction isResponseOutputItemAddedChunk(chunk) {\n  return chunk.type === \"response.output_item.added\";\n}\nfunction isResponseAnnotationAddedChunk(chunk) {\n  return chunk.type === \"response.output_text.annotation.added\";\n}\nfunction isResponseReasoningSummaryTextDeltaChunk(chunk) {\n  return chunk.type === \"response.reasoning_summary_text.delta\";\n}\nfunction isErrorChunk(chunk) {\n  return chunk.type === \"error\";\n}\nfunction getResponsesModelConfig(modelId) {\n  if (modelId.startsWith(\"o\") || modelId.startsWith(\"gpt-5\")) {\n    if (modelId.startsWith(\"o1-mini\") || modelId.startsWith(\"o1-preview\")) {\n      return {\n        isReasoningModel: true,\n        systemMessageMode: \"remove\",\n        requiredAutoTruncation: false\n      };\n    }\n    return {\n      isReasoningModel: true,\n      systemMessageMode: \"developer\",\n      requiredAutoTruncation: false\n    };\n  }\n  return {\n    isReasoningModel: false,\n    systemMessageMode: \"system\",\n    requiredAutoTruncation: false\n  };\n}\nvar openaiResponsesProviderOptionsSchema = z.object({\n  metadata: z.any().nullish(),\n  parallelToolCalls: z.boolean().nullish(),\n  previousResponseId: z.string().nullish(),\n  store: z.boolean().nullish(),\n  user: z.string().nullish(),\n  reasoningEffort: z.string().nullish(),\n  strictSchemas: z.boolean().nullish(),\n  instructions: z.string().nullish(),\n  reasoningSummary: z.string().nullish()\n});\nvar WebSearchPreviewParameters = z.object({});\nfunction webSearchPreviewTool({\n  searchContextSize,\n  userLocation\n} = {}) {\n  return {\n    type: \"provider-defined\",\n    id: \"openai.web_search_preview\",\n    args: {\n      searchContextSize,\n      userLocation\n    },\n    parameters: WebSearchPreviewParameters\n  };\n}\nvar openaiTools = {\n  webSearchPreview: webSearchPreviewTool\n};\nvar OpenAIProviderOptionsSchema = z.object({\n  instructions: z.string().nullish(),\n  speed: z.number().min(0.25).max(4).default(1).nullish()\n});\nvar OpenAISpeechModel = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  getArgs({\n    text,\n    voice = \"alloy\",\n    outputFormat = \"mp3\",\n    speed,\n    instructions,\n    providerOptions\n  }) {\n    const warnings = [];\n    const openAIOptions = parseProviderOptions({\n      provider: \"openai\",\n      providerOptions,\n      schema: OpenAIProviderOptionsSchema\n    });\n    const requestBody = {\n      model: this.modelId,\n      input: text,\n      voice,\n      response_format: \"mp3\",\n      speed,\n      instructions\n    };\n    if (outputFormat) {\n      if ([\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"].includes(outputFormat)) {\n        requestBody.response_format = outputFormat;\n      } else {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"outputFormat\",\n          details: `Unsupported output format: ${outputFormat}. Using mp3 instead.`\n        });\n      }\n    }\n    if (openAIOptions) {\n      const speechModelOptions = {};\n      for (const key in speechModelOptions) {\n        const value = speechModelOptions[key];\n        if (value !== void 0) {\n          requestBody[key] = value;\n        }\n      }\n    }\n    return {\n      requestBody,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { requestBody, warnings } = this.getArgs(options);\n    const {\n      value: audio,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/audio/speech\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: requestBody,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createBinaryResponseHandler(),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      audio,\n      warnings,\n      request: {\n        body: JSON.stringify(requestBody)\n      },\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nfunction createOpenAI(options = {}) {\n  var _a16, _b, _c;\n  const baseURL = (_a16 = withoutTrailingSlash(options.baseURL)) != null ? _a16 : \"https://api.openai.com/v1\";\n  const compatibility = (_b = options.compatibility) != null ? _b : \"compatible\";\n  const providerName = (_c = options.name) != null ? _c : \"openai\";\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"OPENAI_API_KEY\",\n      description: \"OpenAI\"\n    })}`,\n    \"OpenAI-Organization\": options.organization,\n    \"OpenAI-Project\": options.project,\n    ...options.headers\n  });\n  const createChatModel = (modelId, settings = {}) => new OpenAIChatLanguageModel(modelId, settings, {\n    provider: `${providerName}.chat`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    compatibility,\n    fetch: options.fetch\n  });\n  const createCompletionModel = (modelId, settings = {}) => new OpenAICompletionLanguageModel(modelId, settings, {\n    provider: `${providerName}.completion`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    compatibility,\n    fetch: options.fetch\n  });\n  const createEmbeddingModel = (modelId, settings = {}) => new OpenAIEmbeddingModel(modelId, settings, {\n    provider: `${providerName}.embedding`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createImageModel = (modelId, settings = {}) => new OpenAIImageModel(modelId, settings, {\n    provider: `${providerName}.image`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createTranscriptionModel = (modelId) => new OpenAITranscriptionModel(modelId, {\n    provider: `${providerName}.transcription`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createSpeechModel = (modelId) => new OpenAISpeechModel(modelId, {\n    provider: `${providerName}.speech`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId, settings) => {\n    if (new.target) {\n      throw new Error(\n        \"The OpenAI model function cannot be called with the new keyword.\"\n      );\n    }\n    if (modelId === \"gpt-3.5-turbo-instruct\") {\n      return createCompletionModel(\n        modelId,\n        settings\n      );\n    }\n    return createChatModel(modelId, settings);\n  };\n  const createResponsesModel = (modelId) => {\n    return new OpenAIResponsesLanguageModel(modelId, {\n      provider: `${providerName}.responses`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch\n    });\n  };\n  const provider = function(modelId, settings) {\n    return createLanguageModel(modelId, settings);\n  };\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.completion = createCompletionModel;\n  provider.responses = createResponsesModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.image = createImageModel;\n  provider.imageModel = createImageModel;\n  provider.transcription = createTranscriptionModel;\n  provider.transcriptionModel = createTranscriptionModel;\n  provider.speech = createSpeechModel;\n  provider.speechModel = createSpeechModel;\n  provider.tools = openaiTools;\n  return provider;\n}\nvar openai = createOpenAI({\n  compatibility: \"strict\"\n  // strict for OpenAI API\n});\nvar openaiErrorDataSchema2 = z$1.object({\n  error: z$1.object({\n    message: z$1.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z$1.string().nullish(),\n    param: z$1.any().nullish(),\n    code: z$1.union([z$1.string(), z$1.number()]).nullish()\n  })\n});\nvar openaiFailedResponseHandler2 = createJsonErrorResponseHandler2({\n  errorSchema: openaiErrorDataSchema2,\n  errorToMessage: (data) => data.error.message\n});\nfunction convertToOpenAIChatMessages2({\n  prompt,\n  systemMessageMode = \"system\"\n}) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a16, _b, _c;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"file\": {\n                if (part.mediaType.startsWith(\"image/\")) {\n                  const mediaType = part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType;\n                  return {\n                    type: \"image_url\",\n                    image_url: {\n                      url: part.data instanceof URL ? part.data.toString() : `data:${mediaType};base64,${convertToBase64(part.data)}`,\n                      // OpenAI specific extension: image detail\n                      detail: (_b = (_a16 = part.providerOptions) == null ? void 0 : _a16.openai) == null ? void 0 : _b.imageDetail\n                    }\n                  };\n                } else if (part.mediaType.startsWith(\"audio/\")) {\n                  if (part.data instanceof URL) {\n                    throw new UnsupportedFunctionalityError2({\n                      functionality: \"audio file parts with URLs\"\n                    });\n                  }\n                  switch (part.mediaType) {\n                    case \"audio/wav\": {\n                      return {\n                        type: \"input_audio\",\n                        input_audio: {\n                          data: convertToBase64(part.data),\n                          format: \"wav\"\n                        }\n                      };\n                    }\n                    case \"audio/mp3\":\n                    case \"audio/mpeg\": {\n                      return {\n                        type: \"input_audio\",\n                        input_audio: {\n                          data: convertToBase64(part.data),\n                          format: \"mp3\"\n                        }\n                      };\n                    }\n                    default: {\n                      throw new UnsupportedFunctionalityError2({\n                        functionality: `audio content parts with media type ${part.mediaType}`\n                      });\n                    }\n                  }\n                } else if (part.mediaType === \"application/pdf\") {\n                  if (part.data instanceof URL) {\n                    throw new UnsupportedFunctionalityError2({\n                      functionality: \"PDF file parts with URLs\"\n                    });\n                  }\n                  return {\n                    type: \"file\",\n                    file: typeof part.data === \"string\" && part.data.startsWith(\"file-\") ? { file_id: part.data } : {\n                      filename: (_c = part.filename) != null ? _c : `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${convertToBase64(part.data)}`\n                    }\n                  };\n                } else {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: `file part media type ${part.mediaType}`\n                  });\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.input)\n                }\n              });\n              break;\n            }\n          }\n        }\n        messages.push({\n          role: \"assistant\",\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : void 0\n        });\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          const output = toolResponse.output;\n          let contentValue;\n          switch (output.type) {\n            case \"text\":\n            case \"error-text\":\n              contentValue = output.value;\n              break;\n            case \"content\":\n            case \"json\":\n            case \"error-json\":\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n          messages.push({\n            role: \"tool\",\n            tool_call_id: toolResponse.toolCallId,\n            content: contentValue\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\nfunction getResponseMetadata4({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nfunction mapOpenAIFinishReason2(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar openaiProviderOptions = z$1.object({\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in\n   * the GPT tokenizer) to an associated bias value from -100 to 100.\n   */\n  logitBias: z$1.record(z$1.coerce.number(), z$1.number()).optional(),\n  /**\n   * Return the log probabilities of the tokens.\n   *\n   * Setting to true will return the log probabilities of the tokens that\n   * were generated.\n   *\n   * Setting to a number will return the log probabilities of the top n\n   * tokens that were generated.\n   */\n  logprobs: z$1.union([z$1.boolean(), z$1.number()]).optional(),\n  /**\n   * Whether to enable parallel function calling during tool use. Default to true.\n   */\n  parallelToolCalls: z$1.boolean().optional(),\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to\n   * monitor and detect abuse.\n   */\n  user: z$1.string().optional(),\n  /**\n   * Reasoning effort for reasoning models. Defaults to `medium`.\n   */\n  reasoningEffort: z$1.enum([\"minimal\", \"low\", \"medium\", \"high\"]).optional(),\n  /**\n   * Maximum number of completion tokens to generate. Useful for reasoning models.\n   */\n  maxCompletionTokens: z$1.number().optional(),\n  /**\n   * Whether to enable persistence in responses API.\n   */\n  store: z$1.boolean().optional(),\n  /**\n   * Metadata to associate with the request.\n   */\n  metadata: z$1.record(z$1.string().max(64), z$1.string().max(512)).optional(),\n  /**\n   * Parameters for prediction mode.\n   */\n  prediction: z$1.record(z$1.string(), z$1.any()).optional(),\n  /**\n   * Whether to use structured outputs.\n   *\n   * @default true\n   */\n  structuredOutputs: z$1.boolean().optional(),\n  /**\n   * Service tier for the request.\n   * - 'auto': Default service tier\n   * - 'flex': 50% cheaper processing at the cost of increased latency. Only available for o3 and o4-mini models.\n   * - 'priority': Higher-speed processing with predictably low latency at premium cost. Available for Enterprise customers.\n   *\n   * @default 'auto'\n   */\n  serviceTier: z$1.enum([\"auto\", \"flex\", \"priority\"]).optional(),\n  /**\n   * Whether to use strict JSON schema validation.\n   *\n   * @default false\n   */\n  strictJsonSchema: z$1.boolean().optional(),\n  /**\n   * Controls the verbosity of the model's responses.\n   * Lower values will result in more concise responses, while higher values will result in more verbose responses.\n   */\n  textVerbosity: z$1.enum([\"low\", \"medium\", \"high\"]).optional(),\n  /**\n   * A cache key for prompt caching. Allows manual control over prompt caching behavior.\n   * Useful for improving cache hit rates and working around automatic caching issues.\n   */\n  promptCacheKey: z$1.string().optional(),\n  /**\n   * A stable identifier used to help detect users of your application\n   * that may be violating OpenAI's usage policies. The IDs should be a\n   * string that uniquely identifies each user. We recommend hashing their\n   * username or email address, in order to avoid sending us any identifying\n   * information.\n   */\n  safetyIdentifier: z$1.string().optional()\n});\nvar comparisonFilterSchema = z$1.object({\n  key: z$1.string(),\n  type: z$1.enum([\"eq\", \"ne\", \"gt\", \"gte\", \"lt\", \"lte\"]),\n  value: z$1.union([z$1.string(), z$1.number(), z$1.boolean()])\n});\nvar compoundFilterSchema = z$1.object({\n  type: z$1.enum([\"and\", \"or\"]),\n  filters: z$1.array(\n    z$1.union([comparisonFilterSchema, z$1.lazy(() => compoundFilterSchema)])\n  )\n});\nvar filtersSchema = z$1.union([comparisonFilterSchema, compoundFilterSchema]);\nvar fileSearchArgsSchema = z$1.object({\n  /**\n   * List of vector store IDs to search through. If not provided, searches all available vector stores.\n   */\n  vectorStoreIds: z$1.array(z$1.string()).optional(),\n  /**\n   * Maximum number of search results to return. Defaults to 10.\n   */\n  maxNumResults: z$1.number().optional(),\n  /**\n   * Ranking options for the search.\n   */\n  ranking: z$1.object({\n    ranker: z$1.enum([\"auto\", \"default-2024-08-21\"]).optional()\n  }).optional(),\n  /**\n   * A filter to apply based on file attributes.\n   */\n  filters: filtersSchema.optional()\n});\nvar fileSearch = createProviderDefinedToolFactory({\n  id: \"openai.file_search\",\n  name: \"file_search\",\n  inputSchema: z$1.object({\n    query: z$1.string()\n  })\n});\nvar webSearchPreviewArgsSchema = z$1.object({\n  /**\n   * Search context size to use for the web search.\n   * - high: Most comprehensive context, highest cost, slower response\n   * - medium: Balanced context, cost, and latency (default)\n   * - low: Least context, lowest cost, fastest response\n   */\n  searchContextSize: z$1.enum([\"low\", \"medium\", \"high\"]).optional(),\n  /**\n   * User location information to provide geographically relevant search results.\n   */\n  userLocation: z$1.object({\n    /**\n     * Type of location (always 'approximate')\n     */\n    type: z$1.literal(\"approximate\"),\n    /**\n     * Two-letter ISO country code (e.g., 'US', 'GB')\n     */\n    country: z$1.string().optional(),\n    /**\n     * City name (free text, e.g., 'Minneapolis')\n     */\n    city: z$1.string().optional(),\n    /**\n     * Region name (free text, e.g., 'Minnesota')\n     */\n    region: z$1.string().optional(),\n    /**\n     * IANA timezone (e.g., 'America/Chicago')\n     */\n    timezone: z$1.string().optional()\n  }).optional()\n});\nvar webSearchPreview = createProviderDefinedToolFactory({\n  id: \"openai.web_search_preview\",\n  name: \"web_search_preview\",\n  inputSchema: z$1.object({})\n});\nfunction prepareChatTools({\n  tools,\n  toolChoice,\n  structuredOutputs,\n  strictJsonSchema\n}) {\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, toolChoice: void 0, toolWarnings };\n  }\n  const openaiTools22 = [];\n  for (const tool2 of tools) {\n    switch (tool2.type) {\n      case \"function\":\n        openaiTools22.push({\n          type: \"function\",\n          function: {\n            name: tool2.name,\n            description: tool2.description,\n            parameters: tool2.inputSchema,\n            strict: structuredOutputs ? strictJsonSchema : void 0\n          }\n        });\n        break;\n      case \"provider-defined\":\n        switch (tool2.id) {\n          case \"openai.file_search\": {\n            const args = fileSearchArgsSchema.parse(tool2.args);\n            openaiTools22.push({\n              type: \"file_search\",\n              vector_store_ids: args.vectorStoreIds,\n              max_num_results: args.maxNumResults,\n              ranking_options: args.ranking ? { ranker: args.ranking.ranker } : void 0,\n              filters: args.filters\n            });\n            break;\n          }\n          case \"openai.web_search_preview\": {\n            const args = webSearchPreviewArgsSchema.parse(tool2.args);\n            openaiTools22.push({\n              type: \"web_search_preview\",\n              search_context_size: args.searchContextSize,\n              user_location: args.userLocation\n            });\n            break;\n          }\n          default:\n            toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools22, toolChoice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools22, toolChoice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: openaiTools22,\n        toolChoice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar OpenAIChatLanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.supportedUrls = {\n      \"image/*\": [/^https?:\\/\\/.*$/]\n    };\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    tools,\n    toolChoice,\n    providerOptions\n  }) {\n    var _a16, _b, _c, _d;\n    const warnings = [];\n    const openaiOptions = (_a16 = await parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions,\n      schema: openaiProviderOptions\n    })) != null ? _a16 : {};\n    const structuredOutputs = (_b = openaiOptions.structuredOutputs) != null ? _b : true;\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !structuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToOpenAIChatMessages2(\n      {\n        prompt,\n        systemMessageMode: getSystemMessageMode2(this.modelId)\n      }\n    );\n    warnings.push(...messageWarnings);\n    const strictJsonSchema = (_c = openaiOptions.strictJsonSchema) != null ? _c : false;\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      logit_bias: openaiOptions.logitBias,\n      logprobs: openaiOptions.logprobs === true || typeof openaiOptions.logprobs === \"number\" ? true : void 0,\n      top_logprobs: typeof openaiOptions.logprobs === \"number\" ? openaiOptions.logprobs : typeof openaiOptions.logprobs === \"boolean\" ? openaiOptions.logprobs ? 0 : void 0 : void 0,\n      user: openaiOptions.user,\n      parallel_tool_calls: openaiOptions.parallelToolCalls,\n      // standardized settings:\n      max_tokens: maxOutputTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? structuredOutputs && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          strict: strictJsonSchema,\n          name: (_d = responseFormat.name) != null ? _d : \"response\",\n          description: responseFormat.description\n        }\n      } : { type: \"json_object\" } : void 0,\n      stop: stopSequences,\n      seed,\n      verbosity: openaiOptions.textVerbosity,\n      // openai specific settings:\n      // TODO AI SDK 6: remove, we auto-map maxOutputTokens now\n      max_completion_tokens: openaiOptions.maxCompletionTokens,\n      store: openaiOptions.store,\n      metadata: openaiOptions.metadata,\n      prediction: openaiOptions.prediction,\n      reasoning_effort: openaiOptions.reasoningEffort,\n      service_tier: openaiOptions.serviceTier,\n      prompt_cache_key: openaiOptions.promptCacheKey,\n      safety_identifier: openaiOptions.safetyIdentifier,\n      // messages:\n      messages\n    };\n    if (isReasoningModel2(this.modelId)) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.frequency_penalty != null) {\n        baseArgs.frequency_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"frequencyPenalty\",\n          details: \"frequencyPenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.presence_penalty != null) {\n        baseArgs.presence_penalty = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"presencePenalty\",\n          details: \"presencePenalty is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logit_bias != null) {\n        baseArgs.logit_bias = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logitBias is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.logprobs != null) {\n        baseArgs.logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"logprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_logprobs != null) {\n        baseArgs.top_logprobs = void 0;\n        warnings.push({\n          type: \"other\",\n          message: \"topLogprobs is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.max_tokens != null) {\n        if (baseArgs.max_completion_tokens == null) {\n          baseArgs.max_completion_tokens = baseArgs.max_tokens;\n        }\n        baseArgs.max_tokens = void 0;\n      }\n    } else if (this.modelId.startsWith(\"gpt-4o-search-preview\") || this.modelId.startsWith(\"gpt-4o-mini-search-preview\")) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for the search preview models and has been removed.\"\n        });\n      }\n    }\n    if (openaiOptions.serviceTier === \"flex\" && !supportsFlexProcessing(this.modelId)) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"serviceTier\",\n        details: \"flex processing is only available for o3, o4-mini, and gpt-5 models\"\n      });\n      baseArgs.service_tier = void 0;\n    }\n    if (openaiOptions.serviceTier === \"priority\" && !supportsPriorityProcessing(this.modelId)) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"serviceTier\",\n        details: \"priority processing is only available for supported models (gpt-4, gpt-5, gpt-5-mini, o3, o4-mini) and requires Enterprise access. gpt-5-nano is not supported\"\n      });\n      baseArgs.service_tier = void 0;\n    }\n    const {\n      tools: openaiTools22,\n      toolChoice: openaiToolChoice,\n      toolWarnings\n    } = prepareChatTools({\n      tools,\n      toolChoice,\n      structuredOutputs,\n      strictJsonSchema\n    });\n    return {\n      args: {\n        ...baseArgs,\n        tools: openaiTools22,\n        tool_choice: openaiToolChoice\n      },\n      warnings: [...warnings, ...toolWarnings]\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n;\n    const { args: body, warnings } = await this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiChatResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const choice = response.choices[0];\n    const content = [];\n    const text = choice.message.content;\n    if (text != null && text.length > 0) {\n      content.push({ type: \"text\", text });\n    }\n    for (const toolCall of (_a16 = choice.message.tool_calls) != null ? _a16 : []) {\n      content.push({\n        type: \"tool-call\",\n        toolCallId: (_b = toolCall.id) != null ? _b : generateId2(),\n        toolName: toolCall.function.name,\n        input: toolCall.function.arguments\n      });\n    }\n    for (const annotation of (_c = choice.message.annotations) != null ? _c : []) {\n      content.push({\n        type: \"source\",\n        sourceType: \"url\",\n        id: generateId2(),\n        url: annotation.url,\n        title: annotation.title\n      });\n    }\n    const completionTokenDetails = (_d = response.usage) == null ? void 0 : _d.completion_tokens_details;\n    const promptTokenDetails = (_e = response.usage) == null ? void 0 : _e.prompt_tokens_details;\n    const providerMetadata = { openai: {} };\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens) != null) {\n      providerMetadata.openai.acceptedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens) != null) {\n      providerMetadata.openai.rejectedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens;\n    }\n    if (((_f = choice.logprobs) == null ? void 0 : _f.content) != null) {\n      providerMetadata.openai.logprobs = choice.logprobs.content;\n    }\n    return {\n      content,\n      finishReason: mapOpenAIFinishReason2(choice.finish_reason),\n      usage: {\n        inputTokens: (_h = (_g = response.usage) == null ? void 0 : _g.prompt_tokens) != null ? _h : void 0,\n        outputTokens: (_j = (_i = response.usage) == null ? void 0 : _i.completion_tokens) != null ? _j : void 0,\n        totalTokens: (_l = (_k = response.usage) == null ? void 0 : _k.total_tokens) != null ? _l : void 0,\n        reasoningTokens: (_m = completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens) != null ? _m : void 0,\n        cachedInputTokens: (_n = promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens) != null ? _n : void 0\n      },\n      request: { body },\n      response: {\n        ...getResponseMetadata4(response),\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      warnings,\n      providerMetadata\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = await this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      stream_options: {\n        include_usage: true\n      }\n    };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(\n        openaiChatChunkSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    let isFirstChunk = true;\n    let isActiveText = false;\n    const providerMetadata = { openai: {} };\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p, _q, _r, _s, _t, _u, _v, _w, _x;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata4(value)\n              });\n            }\n            if (value.usage != null) {\n              usage.inputTokens = (_a16 = value.usage.prompt_tokens) != null ? _a16 : void 0;\n              usage.outputTokens = (_b = value.usage.completion_tokens) != null ? _b : void 0;\n              usage.totalTokens = (_c = value.usage.total_tokens) != null ? _c : void 0;\n              usage.reasoningTokens = (_e = (_d = value.usage.completion_tokens_details) == null ? void 0 : _d.reasoning_tokens) != null ? _e : void 0;\n              usage.cachedInputTokens = (_g = (_f = value.usage.prompt_tokens_details) == null ? void 0 : _f.cached_tokens) != null ? _g : void 0;\n              if (((_h = value.usage.completion_tokens_details) == null ? void 0 : _h.accepted_prediction_tokens) != null) {\n                providerMetadata.openai.acceptedPredictionTokens = (_i = value.usage.completion_tokens_details) == null ? void 0 : _i.accepted_prediction_tokens;\n              }\n              if (((_j = value.usage.completion_tokens_details) == null ? void 0 : _j.rejected_prediction_tokens) != null) {\n                providerMetadata.openai.rejectedPredictionTokens = (_k = value.usage.completion_tokens_details) == null ? void 0 : _k.rejected_prediction_tokens;\n              }\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason2(choice.finish_reason);\n            }\n            if (((_l = choice == null ? void 0 : choice.logprobs) == null ? void 0 : _l.content) != null) {\n              providerMetadata.openai.logprobs = choice.logprobs.content;\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.content != null) {\n              if (!isActiveText) {\n                controller.enqueue({ type: \"text-start\", id: \"0\" });\n                isActiveText = true;\n              }\n              controller.enqueue({\n                type: \"text-delta\",\n                id: \"0\",\n                delta: delta.content\n              });\n            }\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_m = toolCallDelta.function) == null ? void 0 : _m.name) == null) {\n                    throw new InvalidResponseDataError2({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  controller.enqueue({\n                    type: \"tool-input-start\",\n                    id: toolCallDelta.id,\n                    toolName: toolCallDelta.function.name\n                  });\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_n = toolCallDelta.function.arguments) != null ? _n : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_o = toolCall2.function) == null ? void 0 : _o.name) != null && ((_p = toolCall2.function) == null ? void 0 : _p.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-input-delta\",\n                        id: toolCall2.id,\n                        delta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson2(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-input-end\",\n                        id: toolCall2.id\n                      });\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallId: (_q = toolCall2.id) != null ? _q : generateId2(),\n                        toolName: toolCall2.function.name,\n                        input: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_r = toolCallDelta.function) == null ? void 0 : _r.arguments) != null) {\n                  toolCall.function.arguments += (_t = (_s = toolCallDelta.function) == null ? void 0 : _s.arguments) != null ? _t : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-input-delta\",\n                  id: toolCall.id,\n                  delta: (_u = toolCallDelta.function.arguments) != null ? _u : \"\"\n                });\n                if (((_v = toolCall.function) == null ? void 0 : _v.name) != null && ((_w = toolCall.function) == null ? void 0 : _w.arguments) != null && isParsableJson2(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-input-end\",\n                    id: toolCall.id\n                  });\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallId: (_x = toolCall.id) != null ? _x : generateId2(),\n                    toolName: toolCall.function.name,\n                    input: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n            if (delta.annotations != null) {\n              for (const annotation of delta.annotations) {\n                controller.enqueue({\n                  type: \"source\",\n                  sourceType: \"url\",\n                  id: generateId2(),\n                  url: annotation.url,\n                  title: annotation.title\n                });\n              }\n            }\n          },\n          flush(controller) {\n            if (isActiveText) {\n              controller.enqueue({ type: \"text-end\", id: \"0\" });\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n              ...providerMetadata != null ? { providerMetadata } : {}\n            });\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar openaiTokenUsageSchema2 = z$1.object({\n  prompt_tokens: z$1.number().nullish(),\n  completion_tokens: z$1.number().nullish(),\n  total_tokens: z$1.number().nullish(),\n  prompt_tokens_details: z$1.object({\n    cached_tokens: z$1.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z$1.object({\n    reasoning_tokens: z$1.number().nullish(),\n    accepted_prediction_tokens: z$1.number().nullish(),\n    rejected_prediction_tokens: z$1.number().nullish()\n  }).nullish()\n}).nullish();\nvar openaiChatResponseSchema2 = z$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      message: z$1.object({\n        role: z$1.literal(\"assistant\").nullish(),\n        content: z$1.string().nullish(),\n        tool_calls: z$1.array(\n          z$1.object({\n            id: z$1.string().nullish(),\n            type: z$1.literal(\"function\"),\n            function: z$1.object({\n              name: z$1.string(),\n              arguments: z$1.string()\n            })\n          })\n        ).nullish(),\n        annotations: z$1.array(\n          z$1.object({\n            type: z$1.literal(\"url_citation\"),\n            start_index: z$1.number(),\n            end_index: z$1.number(),\n            url: z$1.string(),\n            title: z$1.string()\n          })\n        ).nullish()\n      }),\n      index: z$1.number(),\n      logprobs: z$1.object({\n        content: z$1.array(\n          z$1.object({\n            token: z$1.string(),\n            logprob: z$1.number(),\n            top_logprobs: z$1.array(\n              z$1.object({\n                token: z$1.string(),\n                logprob: z$1.number()\n              })\n            )\n          })\n        ).nullish()\n      }).nullish(),\n      finish_reason: z$1.string().nullish()\n    })\n  ),\n  usage: openaiTokenUsageSchema2\n});\nvar openaiChatChunkSchema2 = z$1.union([\n  z$1.object({\n    id: z$1.string().nullish(),\n    created: z$1.number().nullish(),\n    model: z$1.string().nullish(),\n    choices: z$1.array(\n      z$1.object({\n        delta: z$1.object({\n          role: z$1.enum([\"assistant\"]).nullish(),\n          content: z$1.string().nullish(),\n          tool_calls: z$1.array(\n            z$1.object({\n              index: z$1.number(),\n              id: z$1.string().nullish(),\n              type: z$1.literal(\"function\").nullish(),\n              function: z$1.object({\n                name: z$1.string().nullish(),\n                arguments: z$1.string().nullish()\n              })\n            })\n          ).nullish(),\n          annotations: z$1.array(\n            z$1.object({\n              type: z$1.literal(\"url_citation\"),\n              start_index: z$1.number(),\n              end_index: z$1.number(),\n              url: z$1.string(),\n              title: z$1.string()\n            })\n          ).nullish()\n        }).nullish(),\n        logprobs: z$1.object({\n          content: z$1.array(\n            z$1.object({\n              token: z$1.string(),\n              logprob: z$1.number(),\n              top_logprobs: z$1.array(\n                z$1.object({\n                  token: z$1.string(),\n                  logprob: z$1.number()\n                })\n              )\n            })\n          ).nullish()\n        }).nullish(),\n        finish_reason: z$1.string().nullish(),\n        index: z$1.number()\n      })\n    ),\n    usage: openaiTokenUsageSchema2\n  }),\n  openaiErrorDataSchema2\n]);\nfunction isReasoningModel2(modelId) {\n  return (modelId.startsWith(\"o\") || modelId.startsWith(\"gpt-5\")) && !modelId.startsWith(\"gpt-5-chat\");\n}\nfunction supportsFlexProcessing(modelId) {\n  return modelId.startsWith(\"o3\") || modelId.startsWith(\"o4-mini\") || modelId.startsWith(\"gpt-5\") && !modelId.startsWith(\"gpt-5-chat\");\n}\nfunction supportsPriorityProcessing(modelId) {\n  return modelId.startsWith(\"gpt-4\") || modelId.startsWith(\"gpt-5-mini\") || modelId.startsWith(\"gpt-5\") && !modelId.startsWith(\"gpt-5-nano\") && !modelId.startsWith(\"gpt-5-chat\") || modelId.startsWith(\"o3\") || modelId.startsWith(\"o4-mini\");\n}\nfunction getSystemMessageMode2(modelId) {\n  var _a16, _b;\n  if (!isReasoningModel2(modelId)) {\n    return \"system\";\n  }\n  return (_b = (_a16 = reasoningModels2[modelId]) == null ? void 0 : _a16.systemMessageMode) != null ? _b : \"developer\";\n}\nvar reasoningModels2 = {\n  \"o1-mini\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-mini-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview\": {\n    systemMessageMode: \"remove\"\n  },\n  \"o1-preview-2024-09-12\": {\n    systemMessageMode: \"remove\"\n  },\n  o3: {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o3-mini-2025-01-31\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini\": {\n    systemMessageMode: \"developer\"\n  },\n  \"o4-mini-2025-04-16\": {\n    systemMessageMode: \"developer\"\n  }\n};\nfunction convertToOpenAICompletionPrompt2({\n  prompt,\n  user = \"user\",\n  assistant = \"assistant\"\n}) {\n  let text = \"\";\n  if (prompt[0].role === \"system\") {\n    text += `${prompt[0].content}\n\n`;\n    prompt = prompt.slice(1);\n  }\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        throw new InvalidPromptError2({\n          message: \"Unexpected system message in prompt: ${content}\",\n          prompt\n        });\n      }\n      case \"user\": {\n        const userMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n          }\n        }).filter(Boolean).join(\"\");\n        text += `${user}:\n${userMessage}\n\n`;\n        break;\n      }\n      case \"assistant\": {\n        const assistantMessage = content.map((part) => {\n          switch (part.type) {\n            case \"text\": {\n              return part.text;\n            }\n            case \"tool-call\": {\n              throw new UnsupportedFunctionalityError2({\n                functionality: \"tool-call messages\"\n              });\n            }\n          }\n        }).join(\"\");\n        text += `${assistant}:\n${assistantMessage}\n\n`;\n        break;\n      }\n      case \"tool\": {\n        throw new UnsupportedFunctionalityError2({\n          functionality: \"tool messages\"\n        });\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  text += `${assistant}:\n`;\n  return {\n    prompt: text,\n    stopSequences: [`\n${user}:`]\n  };\n}\nfunction getResponseMetadata22({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nfunction mapOpenAIFinishReason22(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar openaiCompletionProviderOptions = z$1.object({\n  /**\n  Echo back the prompt in addition to the completion.\n     */\n  echo: z$1.boolean().optional(),\n  /**\n  Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in\n  the GPT tokenizer) to an associated bias value from -100 to 100. You\n  can use this tokenizer tool to convert text to token IDs. Mathematically,\n  the bias is added to the logits generated by the model prior to sampling.\n  The exact effect will vary per model, but values between -1 and 1 should\n  decrease or increase likelihood of selection; values like -100 or 100\n  should result in a ban or exclusive selection of the relevant token.\n  \n  As an example, you can pass {\"50256\": -100} to prevent the <|endoftext|>\n  token from being generated.\n   */\n  logitBias: z$1.record(z$1.string(), z$1.number()).optional(),\n  /**\n  The suffix that comes after a completion of inserted text.\n   */\n  suffix: z$1.string().optional(),\n  /**\n  A unique identifier representing your end-user, which can help OpenAI to\n  monitor and detect abuse. Learn more.\n   */\n  user: z$1.string().optional(),\n  /**\n  Return the log probabilities of the tokens. Including logprobs will increase\n  the response size and can slow down response times. However, it can\n  be useful to better understand how the model is behaving.\n  Setting to true will return the log probabilities of the tokens that\n  were generated.\n  Setting to a number will return the log probabilities of the top n\n  tokens that were generated.\n     */\n  logprobs: z$1.union([z$1.boolean(), z$1.number()]).optional()\n});\nvar OpenAICompletionLanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.supportedUrls = {\n      // No URLs are supported for completion models.\n    };\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    tools,\n    toolChoice,\n    seed,\n    providerOptions\n  }) {\n    const warnings = [];\n    const openaiOptions = {\n      ...await parseProviderOptions2({\n        provider: \"openai\",\n        providerOptions,\n        schema: openaiCompletionProviderOptions\n      }),\n      ...await parseProviderOptions2({\n        provider: this.providerOptionsName,\n        providerOptions,\n        schema: openaiCompletionProviderOptions\n      })\n    };\n    if (topK != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"topK\" });\n    }\n    if (tools == null ? void 0 : tools.length) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"tools\" });\n    }\n    if (toolChoice != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"toolChoice\" });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const { prompt: completionPrompt, stopSequences } = convertToOpenAICompletionPrompt2({ prompt });\n    const stop = [...stopSequences != null ? stopSequences : [], ...userStopSequences != null ? userStopSequences : []];\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n        // model specific settings:\n        echo: openaiOptions.echo,\n        logit_bias: openaiOptions.logitBias,\n        logprobs: (openaiOptions == null ? void 0 : openaiOptions.logprobs) === true ? 0 : (openaiOptions == null ? void 0 : openaiOptions.logprobs) === false ? void 0 : openaiOptions == null ? void 0 : openaiOptions.logprobs,\n        suffix: openaiOptions.suffix,\n        user: openaiOptions.user,\n        // standardized settings:\n        max_tokens: maxOutputTokens,\n        temperature,\n        top_p: topP,\n        frequency_penalty: frequencyPenalty,\n        presence_penalty: presencePenalty,\n        seed,\n        // prompt:\n        prompt: completionPrompt,\n        // stop sequences:\n        stop: stop.length > 0 ? stop : void 0\n      },\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c;\n    const { args, warnings } = await this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiCompletionResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const choice = response.choices[0];\n    const providerMetadata = { openai: {} };\n    if (choice.logprobs != null) {\n      providerMetadata.openai.logprobs = choice.logprobs;\n    }\n    return {\n      content: [{ type: \"text\", text: choice.text }],\n      usage: {\n        inputTokens: (_a16 = response.usage) == null ? void 0 : _a16.prompt_tokens,\n        outputTokens: (_b = response.usage) == null ? void 0 : _b.completion_tokens,\n        totalTokens: (_c = response.usage) == null ? void 0 : _c.total_tokens\n      },\n      finishReason: mapOpenAIFinishReason22(choice.finish_reason),\n      request: { body: args },\n      response: {\n        ...getResponseMetadata22(response),\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      providerMetadata,\n      warnings\n    };\n  }\n  async doStream(options) {\n    const { args, warnings } = await this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      stream_options: {\n        include_usage: true\n      }\n    };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(\n        openaiCompletionChunkSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    let finishReason = \"unknown\";\n    const providerMetadata = { openai: {} };\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata22(value)\n              });\n              controller.enqueue({ type: \"text-start\", id: \"0\" });\n            }\n            if (value.usage != null) {\n              usage.inputTokens = value.usage.prompt_tokens;\n              usage.outputTokens = value.usage.completion_tokens;\n              usage.totalTokens = value.usage.total_tokens;\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAIFinishReason22(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.logprobs) != null) {\n              providerMetadata.openai.logprobs = choice.logprobs;\n            }\n            if ((choice == null ? void 0 : choice.text) != null && choice.text.length > 0) {\n              controller.enqueue({\n                type: \"text-delta\",\n                id: \"0\",\n                delta: choice.text\n              });\n            }\n          },\n          flush(controller) {\n            if (!isFirstChunk) {\n              controller.enqueue({ type: \"text-end\", id: \"0\" });\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              providerMetadata,\n              usage\n            });\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar usageSchema3 = z$1.object({\n  prompt_tokens: z$1.number(),\n  completion_tokens: z$1.number(),\n  total_tokens: z$1.number()\n});\nvar openaiCompletionResponseSchema2 = z$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      text: z$1.string(),\n      finish_reason: z$1.string(),\n      logprobs: z$1.object({\n        tokens: z$1.array(z$1.string()),\n        token_logprobs: z$1.array(z$1.number()),\n        top_logprobs: z$1.array(z$1.record(z$1.string(), z$1.number())).nullish()\n      }).nullish()\n    })\n  ),\n  usage: usageSchema3.nullish()\n});\nvar openaiCompletionChunkSchema2 = z$1.union([\n  z$1.object({\n    id: z$1.string().nullish(),\n    created: z$1.number().nullish(),\n    model: z$1.string().nullish(),\n    choices: z$1.array(\n      z$1.object({\n        text: z$1.string(),\n        finish_reason: z$1.string().nullish(),\n        index: z$1.number(),\n        logprobs: z$1.object({\n          tokens: z$1.array(z$1.string()),\n          token_logprobs: z$1.array(z$1.number()),\n          top_logprobs: z$1.array(z$1.record(z$1.string(), z$1.number())).nullish()\n        }).nullish()\n      })\n    ),\n    usage: usageSchema3.nullish()\n  }),\n  openaiErrorDataSchema2\n]);\nvar openaiEmbeddingProviderOptions = z$1.object({\n  /**\n  The number of dimensions the resulting output embeddings should have.\n  Only supported in text-embedding-3 and later models.\n     */\n  dimensions: z$1.number().optional(),\n  /**\n  A unique identifier representing your end-user, which can help OpenAI to\n  monitor and detect abuse. Learn more.\n  */\n  user: z$1.string().optional()\n});\nvar OpenAIEmbeddingModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.maxEmbeddingsPerCall = 2048;\n    this.supportsParallelCalls = true;\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n    providerOptions\n  }) {\n    var _a16;\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError2({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const openaiOptions = (_a16 = await parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions,\n      schema: openaiEmbeddingProviderOptions\n    })) != null ? _a16 : {};\n    const {\n      responseHeaders,\n      value: response,\n      rawValue\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/embeddings\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: \"float\",\n        dimensions: openaiOptions.dimensions,\n        user: openaiOptions.user\n      },\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiTextEmbeddingResponseSchema2\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.data.map((item) => item.embedding),\n      usage: response.usage ? { tokens: response.usage.prompt_tokens } : void 0,\n      response: { headers: responseHeaders, body: rawValue }\n    };\n  }\n};\nvar openaiTextEmbeddingResponseSchema2 = z$1.object({\n  data: z$1.array(z$1.object({ embedding: z$1.array(z$1.number()) })),\n  usage: z$1.object({ prompt_tokens: z$1.number() }).nullish()\n});\nvar modelMaxImagesPerCall2 = {\n  \"dall-e-3\": 1,\n  \"dall-e-2\": 10,\n  \"gpt-image-1\": 10\n};\nvar hasDefaultResponseFormat2 = /* @__PURE__ */ new Set([\"gpt-image-1\"]);\nvar OpenAIImageModel2 = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n  }\n  get maxImagesPerCall() {\n    var _a16;\n    return (_a16 = modelMaxImagesPerCall2[this.modelId]) != null ? _a16 : 1;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a16, _b, _c, _d;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { value: response, responseHeaders } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(_d = providerOptions.openai) != null ? _d : {},\n        ...!hasDefaultResponseFormat2.has(this.modelId) ? { response_format: \"b64_json\" } : {}\n      },\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiImageResponseSchema2\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map((item) => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      },\n      providerMetadata: {\n        openai: {\n          images: response.data.map(\n            (item) => item.revised_prompt ? {\n              revisedPrompt: item.revised_prompt\n            } : null\n          )\n        }\n      }\n    };\n  }\n};\nvar openaiImageResponseSchema2 = z$1.object({\n  data: z$1.array(\n    z$1.object({ b64_json: z$1.string(), revised_prompt: z$1.string().optional() })\n  )\n});\nvar codeInterpreterArgsSchema = z$1.object({\n  container: z$1.union([\n    z$1.string(),\n    z$1.object({\n      fileIds: z$1.array(z$1.string()).optional()\n    })\n  ]).optional()\n});\nvar codeInterpreter = createProviderDefinedToolFactory({\n  id: \"openai.code_interpreter\",\n  name: \"code_interpreter\",\n  inputSchema: z$1.object({})\n});\nvar openaiTools2 = {\n  codeInterpreter,\n  fileSearch,\n  webSearchPreview\n};\nfunction isFileId(data, prefixes) {\n  if (!prefixes) return false;\n  return prefixes.some((prefix) => data.startsWith(prefix));\n}\nasync function convertToOpenAIResponsesMessages2({\n  prompt,\n  systemMessageMode,\n  fileIdPrefixes\n}) {\n  var _a16, _b, _c, _d, _e, _f;\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        switch (systemMessageMode) {\n          case \"system\": {\n            messages.push({ role: \"system\", content });\n            break;\n          }\n          case \"developer\": {\n            messages.push({ role: \"developer\", content });\n            break;\n          }\n          case \"remove\": {\n            warnings.push({\n              type: \"other\",\n              message: \"system messages are removed for this model\"\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`\n            );\n          }\n        }\n        break;\n      }\n      case \"user\": {\n        messages.push({\n          role: \"user\",\n          content: content.map((part, index) => {\n            var _a23, _b2, _c2;\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"input_text\", text: part.text };\n              }\n              case \"file\": {\n                if (part.mediaType.startsWith(\"image/\")) {\n                  const mediaType = part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType;\n                  return {\n                    type: \"input_image\",\n                    ...part.data instanceof URL ? { image_url: part.data.toString() } : typeof part.data === \"string\" && isFileId(part.data, fileIdPrefixes) ? { file_id: part.data } : {\n                      image_url: `data:${mediaType};base64,${convertToBase64(part.data)}`\n                    },\n                    detail: (_b2 = (_a23 = part.providerOptions) == null ? void 0 : _a23.openai) == null ? void 0 : _b2.imageDetail\n                  };\n                } else if (part.mediaType === \"application/pdf\") {\n                  if (part.data instanceof URL) {\n                    throw new UnsupportedFunctionalityError2({\n                      functionality: \"PDF file parts with URLs\"\n                    });\n                  }\n                  return {\n                    type: \"input_file\",\n                    ...typeof part.data === \"string\" && isFileId(part.data, fileIdPrefixes) ? { file_id: part.data } : {\n                      filename: (_c2 = part.filename) != null ? _c2 : `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${convertToBase64(part.data)}`\n                    }\n                  };\n                } else {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: `file part media type ${part.mediaType}`\n                  });\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        const reasoningMessages = {};\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              messages.push({\n                role: \"assistant\",\n                content: [{ type: \"output_text\", text: part.text }],\n                id: (_c = (_b = (_a16 = part.providerOptions) == null ? void 0 : _a16.openai) == null ? void 0 : _b.itemId) != null ? _c : void 0\n              });\n              break;\n            }\n            case \"tool-call\": {\n              if (part.providerExecuted) {\n                break;\n              }\n              messages.push({\n                type: \"function_call\",\n                call_id: part.toolCallId,\n                name: part.toolName,\n                arguments: JSON.stringify(part.input),\n                id: (_f = (_e = (_d = part.providerOptions) == null ? void 0 : _d.openai) == null ? void 0 : _e.itemId) != null ? _f : void 0\n              });\n              break;\n            }\n            case \"tool-result\": {\n              warnings.push({\n                type: \"other\",\n                message: `tool result parts in assistant messages are not supported for OpenAI responses`\n              });\n              break;\n            }\n            case \"reasoning\": {\n              const providerOptions = await parseProviderOptions2({\n                provider: \"openai\",\n                providerOptions: part.providerOptions,\n                schema: openaiResponsesReasoningProviderOptionsSchema\n              });\n              const reasoningId = providerOptions == null ? void 0 : providerOptions.itemId;\n              if (reasoningId != null) {\n                const existingReasoningMessage = reasoningMessages[reasoningId];\n                const summaryParts = [];\n                if (part.text.length > 0) {\n                  summaryParts.push({ type: \"summary_text\", text: part.text });\n                } else if (existingReasoningMessage !== void 0) {\n                  warnings.push({\n                    type: \"other\",\n                    message: `Cannot append empty reasoning part to existing reasoning sequence. Skipping reasoning part: ${JSON.stringify(part)}.`\n                  });\n                }\n                if (existingReasoningMessage === void 0) {\n                  reasoningMessages[reasoningId] = {\n                    type: \"reasoning\",\n                    id: reasoningId,\n                    encrypted_content: providerOptions == null ? void 0 : providerOptions.reasoningEncryptedContent,\n                    summary: summaryParts\n                  };\n                  messages.push(reasoningMessages[reasoningId]);\n                } else {\n                  existingReasoningMessage.summary.push(...summaryParts);\n                }\n              } else {\n                warnings.push({\n                  type: \"other\",\n                  message: `Non-OpenAI reasoning parts are not supported. Skipping reasoning part: ${JSON.stringify(part)}.`\n                });\n              }\n              break;\n            }\n          }\n        }\n        break;\n      }\n      case \"tool\": {\n        for (const part of content) {\n          const output = part.output;\n          let contentValue;\n          switch (output.type) {\n            case \"text\":\n            case \"error-text\":\n              contentValue = output.value;\n              break;\n            case \"content\":\n            case \"json\":\n            case \"error-json\":\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n          messages.push({\n            type: \"function_call_output\",\n            call_id: part.toolCallId,\n            output: contentValue\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\nvar openaiResponsesReasoningProviderOptionsSchema = z$1.object({\n  itemId: z$1.string().nullish(),\n  reasoningEncryptedContent: z$1.string().nullish()\n});\nfunction mapOpenAIResponseFinishReason2({\n  finishReason,\n  hasToolCalls\n}) {\n  switch (finishReason) {\n    case void 0:\n    case null:\n      return hasToolCalls ? \"tool-calls\" : \"stop\";\n    case \"max_output_tokens\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    default:\n      return hasToolCalls ? \"tool-calls\" : \"unknown\";\n  }\n}\nfunction prepareResponsesTools2({\n  tools,\n  toolChoice,\n  strictJsonSchema\n}) {\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, toolChoice: void 0, toolWarnings };\n  }\n  const openaiTools22 = [];\n  for (const tool2 of tools) {\n    switch (tool2.type) {\n      case \"function\":\n        openaiTools22.push({\n          type: \"function\",\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.inputSchema,\n          strict: strictJsonSchema\n        });\n        break;\n      case \"provider-defined\": {\n        switch (tool2.id) {\n          case \"openai.file_search\": {\n            const args = fileSearchArgsSchema.parse(tool2.args);\n            openaiTools22.push({\n              type: \"file_search\",\n              vector_store_ids: args.vectorStoreIds,\n              max_num_results: args.maxNumResults,\n              ranking_options: args.ranking ? { ranker: args.ranking.ranker } : void 0,\n              filters: args.filters\n            });\n            break;\n          }\n          case \"openai.web_search_preview\": {\n            const args = webSearchPreviewArgsSchema.parse(tool2.args);\n            openaiTools22.push({\n              type: \"web_search_preview\",\n              search_context_size: args.searchContextSize,\n              user_location: args.userLocation\n            });\n            break;\n          }\n          case \"openai.code_interpreter\": {\n            const args = codeInterpreterArgsSchema.parse(tool2.args);\n            openaiTools22.push({\n              type: \"code_interpreter\",\n              container: args.container == null ? { type: \"auto\", file_ids: void 0 } : typeof args.container === \"string\" ? args.container : { type: \"auto\", file_ids: args.container.fileIds }\n            });\n            break;\n          }\n          default: {\n            toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n            break;\n          }\n        }\n        break;\n      }\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n        break;\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiTools22, toolChoice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiTools22, toolChoice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: openaiTools22,\n        toolChoice: toolChoice.toolName === \"code_interpreter\" || toolChoice.toolName === \"file_search\" || toolChoice.toolName === \"web_search_preview\" ? { type: toolChoice.toolName } : { type: \"function\", name: toolChoice.toolName },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar TOP_LOGPROBS_MAX = 20;\nvar LOGPROBS_SCHEMA = z$1.array(\n  z$1.object({\n    token: z$1.string(),\n    logprob: z$1.number(),\n    top_logprobs: z$1.array(\n      z$1.object({\n        token: z$1.string(),\n        logprob: z$1.number()\n      })\n    )\n  })\n);\nvar OpenAIResponsesLanguageModel2 = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.supportedUrls = {\n      \"image/*\": [/^https?:\\/\\/.*$/]\n    };\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    maxOutputTokens,\n    temperature,\n    stopSequences,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    prompt,\n    providerOptions,\n    tools,\n    toolChoice,\n    responseFormat\n  }) {\n    var _a16, _b;\n    const warnings = [];\n    const modelConfig = getResponsesModelConfig2(this.modelId);\n    if (topK != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"topK\" });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (stopSequences != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"stopSequences\" });\n    }\n    const { messages, warnings: messageWarnings } = await convertToOpenAIResponsesMessages2({\n      prompt,\n      systemMessageMode: modelConfig.systemMessageMode,\n      fileIdPrefixes: this.config.fileIdPrefixes\n    });\n    warnings.push(...messageWarnings);\n    const openaiOptions = await parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions,\n      schema: openaiResponsesProviderOptionsSchema2\n    });\n    const strictJsonSchema = (_a16 = openaiOptions == null ? void 0 : openaiOptions.strictJsonSchema) != null ? _a16 : false;\n    const topLogprobs = typeof (openaiOptions == null ? void 0 : openaiOptions.logprobs) === \"number\" ? openaiOptions == null ? void 0 : openaiOptions.logprobs : (openaiOptions == null ? void 0 : openaiOptions.logprobs) === true ? TOP_LOGPROBS_MAX : void 0;\n    const openaiOptionsInclude = topLogprobs ? Array.isArray(openaiOptions == null ? void 0 : openaiOptions.include) ? [...openaiOptions == null ? void 0 : openaiOptions.include, \"message.output_text.logprobs\"] : [\"message.output_text.logprobs\"] : openaiOptions == null ? void 0 : openaiOptions.include;\n    const baseArgs = {\n      model: this.modelId,\n      input: messages,\n      temperature,\n      top_p: topP,\n      max_output_tokens: maxOutputTokens,\n      ...((responseFormat == null ? void 0 : responseFormat.type) === \"json\" || (openaiOptions == null ? void 0 : openaiOptions.textVerbosity)) && {\n        text: {\n          ...(responseFormat == null ? void 0 : responseFormat.type) === \"json\" && {\n            format: responseFormat.schema != null ? {\n              type: \"json_schema\",\n              strict: strictJsonSchema,\n              name: (_b = responseFormat.name) != null ? _b : \"response\",\n              description: responseFormat.description,\n              schema: responseFormat.schema\n            } : { type: \"json_object\" }\n          },\n          ...(openaiOptions == null ? void 0 : openaiOptions.textVerbosity) && {\n            verbosity: openaiOptions.textVerbosity\n          }\n        }\n      },\n      // provider options:\n      metadata: openaiOptions == null ? void 0 : openaiOptions.metadata,\n      parallel_tool_calls: openaiOptions == null ? void 0 : openaiOptions.parallelToolCalls,\n      previous_response_id: openaiOptions == null ? void 0 : openaiOptions.previousResponseId,\n      store: openaiOptions == null ? void 0 : openaiOptions.store,\n      user: openaiOptions == null ? void 0 : openaiOptions.user,\n      instructions: openaiOptions == null ? void 0 : openaiOptions.instructions,\n      service_tier: openaiOptions == null ? void 0 : openaiOptions.serviceTier,\n      include: openaiOptionsInclude,\n      prompt_cache_key: openaiOptions == null ? void 0 : openaiOptions.promptCacheKey,\n      safety_identifier: openaiOptions == null ? void 0 : openaiOptions.safetyIdentifier,\n      top_logprobs: topLogprobs,\n      // model-specific settings:\n      ...modelConfig.isReasoningModel && ((openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null || (openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null) && {\n        reasoning: {\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null && {\n            effort: openaiOptions.reasoningEffort\n          },\n          ...(openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null && {\n            summary: openaiOptions.reasoningSummary\n          }\n        }\n      },\n      ...modelConfig.requiredAutoTruncation && {\n        truncation: \"auto\"\n      }\n    };\n    if (modelConfig.isReasoningModel) {\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"temperature\",\n          details: \"temperature is not supported for reasoning models\"\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = void 0;\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"topP\",\n          details: \"topP is not supported for reasoning models\"\n        });\n      }\n    } else {\n      if ((openaiOptions == null ? void 0 : openaiOptions.reasoningEffort) != null) {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"reasoningEffort\",\n          details: \"reasoningEffort is not supported for non-reasoning models\"\n        });\n      }\n      if ((openaiOptions == null ? void 0 : openaiOptions.reasoningSummary) != null) {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"reasoningSummary\",\n          details: \"reasoningSummary is not supported for non-reasoning models\"\n        });\n      }\n    }\n    if ((openaiOptions == null ? void 0 : openaiOptions.serviceTier) === \"flex\" && !modelConfig.supportsFlexProcessing) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"serviceTier\",\n        details: \"flex processing is only available for o3, o4-mini, and gpt-5 models\"\n      });\n      delete baseArgs.service_tier;\n    }\n    if ((openaiOptions == null ? void 0 : openaiOptions.serviceTier) === \"priority\" && !modelConfig.supportsPriorityProcessing) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"serviceTier\",\n        details: \"priority processing is only available for supported models (gpt-4, gpt-5, gpt-5-mini, o3, o4-mini) and requires Enterprise access. gpt-5-nano is not supported\"\n      });\n      delete baseArgs.service_tier;\n    }\n    const {\n      tools: openaiTools22,\n      toolChoice: openaiToolChoice,\n      toolWarnings\n    } = prepareResponsesTools2({\n      tools,\n      toolChoice,\n      strictJsonSchema\n    });\n    return {\n      args: {\n        ...baseArgs,\n        tools: openaiTools22,\n        tool_choice: openaiToolChoice\n      },\n      warnings: [...warnings, ...toolWarnings]\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p, _q;\n    const { args: body, warnings } = await this.getArgs(options);\n    const url = this.config.url({\n      path: \"/responses\",\n      modelId: this.modelId\n    });\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url,\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        z$1.object({\n          id: z$1.string(),\n          created_at: z$1.number(),\n          error: z$1.object({\n            code: z$1.string(),\n            message: z$1.string()\n          }).nullish(),\n          model: z$1.string(),\n          output: z$1.array(\n            z$1.discriminatedUnion(\"type\", [\n              z$1.object({\n                type: z$1.literal(\"message\"),\n                role: z$1.literal(\"assistant\"),\n                id: z$1.string(),\n                content: z$1.array(\n                  z$1.object({\n                    type: z$1.literal(\"output_text\"),\n                    text: z$1.string(),\n                    logprobs: LOGPROBS_SCHEMA.nullish(),\n                    annotations: z$1.array(\n                      z$1.discriminatedUnion(\"type\", [\n                        z$1.object({\n                          type: z$1.literal(\"url_citation\"),\n                          start_index: z$1.number(),\n                          end_index: z$1.number(),\n                          url: z$1.string(),\n                          title: z$1.string()\n                        }),\n                        z$1.object({\n                          type: z$1.literal(\"file_citation\"),\n                          start_index: z$1.number(),\n                          end_index: z$1.number(),\n                          file_id: z$1.string(),\n                          quote: z$1.string()\n                        })\n                      ])\n                    )\n                  })\n                )\n              }),\n              z$1.object({\n                type: z$1.literal(\"function_call\"),\n                call_id: z$1.string(),\n                name: z$1.string(),\n                arguments: z$1.string(),\n                id: z$1.string()\n              }),\n              z$1.object({\n                type: z$1.literal(\"web_search_call\"),\n                id: z$1.string(),\n                status: z$1.string().optional(),\n                action: z$1.object({\n                  type: z$1.literal(\"search\"),\n                  query: z$1.string().optional()\n                }).nullish()\n              }),\n              z$1.object({\n                type: z$1.literal(\"computer_call\"),\n                id: z$1.string(),\n                status: z$1.string().optional()\n              }),\n              z$1.object({\n                type: z$1.literal(\"file_search_call\"),\n                id: z$1.string(),\n                status: z$1.string().optional(),\n                queries: z$1.array(z$1.string()).nullish(),\n                results: z$1.array(\n                  z$1.object({\n                    attributes: z$1.object({\n                      file_id: z$1.string(),\n                      filename: z$1.string(),\n                      score: z$1.number(),\n                      text: z$1.string()\n                    })\n                  })\n                ).nullish()\n              }),\n              z$1.object({\n                type: z$1.literal(\"reasoning\"),\n                id: z$1.string(),\n                encrypted_content: z$1.string().nullish(),\n                summary: z$1.array(\n                  z$1.object({\n                    type: z$1.literal(\"summary_text\"),\n                    text: z$1.string()\n                  })\n                )\n              })\n            ])\n          ),\n          incomplete_details: z$1.object({ reason: z$1.string() }).nullable(),\n          usage: usageSchema22\n        })\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    if (response.error) {\n      throw new APICallError2({\n        message: response.error.message,\n        url,\n        requestBodyValues: body,\n        statusCode: 400,\n        responseHeaders,\n        responseBody: rawResponse,\n        isRetryable: false\n      });\n    }\n    const content = [];\n    const logprobs = [];\n    for (const part of response.output) {\n      switch (part.type) {\n        case \"reasoning\": {\n          if (part.summary.length === 0) {\n            part.summary.push({ type: \"summary_text\", text: \"\" });\n          }\n          for (const summary of part.summary) {\n            content.push({\n              type: \"reasoning\",\n              text: summary.text,\n              providerMetadata: {\n                openai: {\n                  itemId: part.id,\n                  reasoningEncryptedContent: (_a16 = part.encrypted_content) != null ? _a16 : null\n                }\n              }\n            });\n          }\n          break;\n        }\n        case \"message\": {\n          for (const contentPart of part.content) {\n            if (((_c = (_b = options.providerOptions) == null ? void 0 : _b.openai) == null ? void 0 : _c.logprobs) && contentPart.logprobs) {\n              logprobs.push(contentPart.logprobs);\n            }\n            content.push({\n              type: \"text\",\n              text: contentPart.text,\n              providerMetadata: {\n                openai: {\n                  itemId: part.id\n                }\n              }\n            });\n            for (const annotation of contentPart.annotations) {\n              if (annotation.type === \"url_citation\") {\n                content.push({\n                  type: \"source\",\n                  sourceType: \"url\",\n                  id: (_f = (_e = (_d = this.config).generateId) == null ? void 0 : _e.call(_d)) != null ? _f : generateId2(),\n                  url: annotation.url,\n                  title: annotation.title\n                });\n              } else if (annotation.type === \"file_citation\") {\n                content.push({\n                  type: \"source\",\n                  sourceType: \"document\",\n                  id: (_i = (_h = (_g = this.config).generateId) == null ? void 0 : _h.call(_g)) != null ? _i : generateId2(),\n                  mediaType: \"text/plain\",\n                  title: annotation.quote,\n                  filename: annotation.file_id\n                });\n              }\n            }\n          }\n          break;\n        }\n        case \"function_call\": {\n          content.push({\n            type: \"tool-call\",\n            toolCallId: part.call_id,\n            toolName: part.name,\n            input: part.arguments,\n            providerMetadata: {\n              openai: {\n                itemId: part.id\n              }\n            }\n          });\n          break;\n        }\n        case \"web_search_call\": {\n          content.push({\n            type: \"tool-call\",\n            toolCallId: part.id,\n            toolName: \"web_search_preview\",\n            input: (_k = (_j = part.action) == null ? void 0 : _j.query) != null ? _k : \"\",\n            providerExecuted: true\n          });\n          content.push({\n            type: \"tool-result\",\n            toolCallId: part.id,\n            toolName: \"web_search_preview\",\n            result: {\n              status: part.status || \"completed\",\n              ...((_l = part.action) == null ? void 0 : _l.query) && { query: part.action.query }\n            },\n            providerExecuted: true\n          });\n          break;\n        }\n        case \"computer_call\": {\n          content.push({\n            type: \"tool-call\",\n            toolCallId: part.id,\n            toolName: \"computer_use\",\n            input: \"\",\n            providerExecuted: true\n          });\n          content.push({\n            type: \"tool-result\",\n            toolCallId: part.id,\n            toolName: \"computer_use\",\n            result: {\n              type: \"computer_use_tool_result\",\n              status: part.status || \"completed\"\n            },\n            providerExecuted: true\n          });\n          break;\n        }\n        case \"file_search_call\": {\n          content.push({\n            type: \"tool-call\",\n            toolCallId: part.id,\n            toolName: \"file_search\",\n            input: \"\",\n            providerExecuted: true\n          });\n          content.push({\n            type: \"tool-result\",\n            toolCallId: part.id,\n            toolName: \"file_search\",\n            result: {\n              type: \"file_search_tool_result\",\n              status: part.status || \"completed\",\n              ...part.queries && { queries: part.queries },\n              ...part.results && { results: part.results }\n            },\n            providerExecuted: true\n          });\n          break;\n        }\n      }\n    }\n    const providerMetadata = {\n      openai: { responseId: response.id }\n    };\n    if (logprobs.length > 0) {\n      providerMetadata.openai.logprobs = logprobs;\n    }\n    return {\n      content,\n      finishReason: mapOpenAIResponseFinishReason2({\n        finishReason: (_m = response.incomplete_details) == null ? void 0 : _m.reason,\n        hasToolCalls: content.some((part) => part.type === \"tool-call\")\n      }),\n      usage: {\n        inputTokens: response.usage.input_tokens,\n        outputTokens: response.usage.output_tokens,\n        totalTokens: response.usage.input_tokens + response.usage.output_tokens,\n        reasoningTokens: (_o = (_n = response.usage.output_tokens_details) == null ? void 0 : _n.reasoning_tokens) != null ? _o : void 0,\n        cachedInputTokens: (_q = (_p = response.usage.input_tokens_details) == null ? void 0 : _p.cached_tokens) != null ? _q : void 0\n      },\n      request: { body },\n      response: {\n        id: response.id,\n        timestamp: new Date(response.created_at * 1e3),\n        modelId: response.model,\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      providerMetadata,\n      warnings\n    };\n  }\n  async doStream(options) {\n    const { args: body, warnings } = await this.getArgs(options);\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/responses\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: {\n        ...body,\n        stream: true\n      },\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createEventSourceResponseHandler2(\n        openaiResponsesChunkSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const self = this;\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    const logprobs = [];\n    let responseId = null;\n    const ongoingToolCalls = {};\n    let hasToolCalls = false;\n    const activeReasoning = {};\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n, _o, _p, _q, _r, _s;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (isResponseOutputItemAddedChunk2(value)) {\n              if (value.item.type === \"function_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: value.item.name,\n                  toolCallId: value.item.call_id\n                };\n                controller.enqueue({\n                  type: \"tool-input-start\",\n                  id: value.item.call_id,\n                  toolName: value.item.name\n                });\n              } else if (value.item.type === \"web_search_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: \"web_search_preview\",\n                  toolCallId: value.item.id\n                };\n                controller.enqueue({\n                  type: \"tool-input-start\",\n                  id: value.item.id,\n                  toolName: \"web_search_preview\"\n                });\n              } else if (value.item.type === \"computer_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: \"computer_use\",\n                  toolCallId: value.item.id\n                };\n                controller.enqueue({\n                  type: \"tool-input-start\",\n                  id: value.item.id,\n                  toolName: \"computer_use\"\n                });\n              } else if (value.item.type === \"file_search_call\") {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: \"file_search\",\n                  toolCallId: value.item.id\n                };\n                controller.enqueue({\n                  type: \"tool-input-start\",\n                  id: value.item.id,\n                  toolName: \"file_search\"\n                });\n              } else if (value.item.type === \"message\") {\n                controller.enqueue({\n                  type: \"text-start\",\n                  id: value.item.id,\n                  providerMetadata: {\n                    openai: {\n                      itemId: value.item.id\n                    }\n                  }\n                });\n              } else if (isResponseOutputItemAddedReasoningChunk(value)) {\n                activeReasoning[value.item.id] = {\n                  encryptedContent: value.item.encrypted_content,\n                  summaryParts: [0]\n                };\n                controller.enqueue({\n                  type: \"reasoning-start\",\n                  id: `${value.item.id}:0`,\n                  providerMetadata: {\n                    openai: {\n                      itemId: value.item.id,\n                      reasoningEncryptedContent: (_a16 = value.item.encrypted_content) != null ? _a16 : null\n                    }\n                  }\n                });\n              }\n            } else if (isResponseOutputItemDoneChunk2(value)) {\n              if (value.item.type === \"function_call\") {\n                ongoingToolCalls[value.output_index] = void 0;\n                hasToolCalls = true;\n                controller.enqueue({\n                  type: \"tool-input-end\",\n                  id: value.item.call_id\n                });\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallId: value.item.call_id,\n                  toolName: value.item.name,\n                  input: value.item.arguments,\n                  providerMetadata: {\n                    openai: {\n                      itemId: value.item.id\n                    }\n                  }\n                });\n              } else if (value.item.type === \"web_search_call\") {\n                ongoingToolCalls[value.output_index] = void 0;\n                hasToolCalls = true;\n                controller.enqueue({\n                  type: \"tool-input-end\",\n                  id: value.item.id\n                });\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallId: value.item.id,\n                  toolName: \"web_search_preview\",\n                  input: (_c = (_b = value.item.action) == null ? void 0 : _b.query) != null ? _c : \"\",\n                  providerExecuted: true\n                });\n                controller.enqueue({\n                  type: \"tool-result\",\n                  toolCallId: value.item.id,\n                  toolName: \"web_search_preview\",\n                  result: {\n                    type: \"web_search_tool_result\",\n                    status: value.item.status || \"completed\",\n                    ...((_d = value.item.action) == null ? void 0 : _d.query) && {\n                      query: value.item.action.query\n                    }\n                  },\n                  providerExecuted: true\n                });\n              } else if (value.item.type === \"computer_call\") {\n                ongoingToolCalls[value.output_index] = void 0;\n                hasToolCalls = true;\n                controller.enqueue({\n                  type: \"tool-input-end\",\n                  id: value.item.id\n                });\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallId: value.item.id,\n                  toolName: \"computer_use\",\n                  input: \"\",\n                  providerExecuted: true\n                });\n                controller.enqueue({\n                  type: \"tool-result\",\n                  toolCallId: value.item.id,\n                  toolName: \"computer_use\",\n                  result: {\n                    type: \"computer_use_tool_result\",\n                    status: value.item.status || \"completed\"\n                  },\n                  providerExecuted: true\n                });\n              } else if (value.item.type === \"file_search_call\") {\n                ongoingToolCalls[value.output_index] = void 0;\n                hasToolCalls = true;\n                controller.enqueue({\n                  type: \"tool-input-end\",\n                  id: value.item.id\n                });\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallId: value.item.id,\n                  toolName: \"file_search\",\n                  input: \"\",\n                  providerExecuted: true\n                });\n                controller.enqueue({\n                  type: \"tool-result\",\n                  toolCallId: value.item.id,\n                  toolName: \"file_search\",\n                  result: {\n                    type: \"file_search_tool_result\",\n                    status: value.item.status || \"completed\",\n                    ...value.item.queries && { queries: value.item.queries },\n                    ...value.item.results && { results: value.item.results }\n                  },\n                  providerExecuted: true\n                });\n              } else if (value.item.type === \"message\") {\n                controller.enqueue({\n                  type: \"text-end\",\n                  id: value.item.id\n                });\n              } else if (isResponseOutputItemDoneReasoningChunk(value)) {\n                const activeReasoningPart = activeReasoning[value.item.id];\n                for (const summaryIndex of activeReasoningPart.summaryParts) {\n                  controller.enqueue({\n                    type: \"reasoning-end\",\n                    id: `${value.item.id}:${summaryIndex}`,\n                    providerMetadata: {\n                      openai: {\n                        itemId: value.item.id,\n                        reasoningEncryptedContent: (_e = value.item.encrypted_content) != null ? _e : null\n                      }\n                    }\n                  });\n                }\n                delete activeReasoning[value.item.id];\n              }\n            } else if (isResponseFunctionCallArgumentsDeltaChunk2(value)) {\n              const toolCall = ongoingToolCalls[value.output_index];\n              if (toolCall != null) {\n                controller.enqueue({\n                  type: \"tool-input-delta\",\n                  id: toolCall.toolCallId,\n                  delta: value.delta\n                });\n              }\n            } else if (isResponseCreatedChunk2(value)) {\n              responseId = value.response.id;\n              controller.enqueue({\n                type: \"response-metadata\",\n                id: value.response.id,\n                timestamp: new Date(value.response.created_at * 1e3),\n                modelId: value.response.model\n              });\n            } else if (isTextDeltaChunk2(value)) {\n              controller.enqueue({\n                type: \"text-delta\",\n                id: value.item_id,\n                delta: value.delta\n              });\n              if (value.logprobs) {\n                logprobs.push(value.logprobs);\n              }\n            } else if (isResponseReasoningSummaryPartAddedChunk(value)) {\n              if (value.summary_index > 0) {\n                (_f = activeReasoning[value.item_id]) == null ? void 0 : _f.summaryParts.push(\n                  value.summary_index\n                );\n                controller.enqueue({\n                  type: \"reasoning-start\",\n                  id: `${value.item_id}:${value.summary_index}`,\n                  providerMetadata: {\n                    openai: {\n                      itemId: value.item_id,\n                      reasoningEncryptedContent: (_h = (_g = activeReasoning[value.item_id]) == null ? void 0 : _g.encryptedContent) != null ? _h : null\n                    }\n                  }\n                });\n              }\n            } else if (isResponseReasoningSummaryTextDeltaChunk2(value)) {\n              controller.enqueue({\n                type: \"reasoning-delta\",\n                id: `${value.item_id}:${value.summary_index}`,\n                delta: value.delta,\n                providerMetadata: {\n                  openai: {\n                    itemId: value.item_id\n                  }\n                }\n              });\n            } else if (isResponseFinishedChunk2(value)) {\n              finishReason = mapOpenAIResponseFinishReason2({\n                finishReason: (_i = value.response.incomplete_details) == null ? void 0 : _i.reason,\n                hasToolCalls\n              });\n              usage.inputTokens = value.response.usage.input_tokens;\n              usage.outputTokens = value.response.usage.output_tokens;\n              usage.totalTokens = value.response.usage.input_tokens + value.response.usage.output_tokens;\n              usage.reasoningTokens = (_k = (_j = value.response.usage.output_tokens_details) == null ? void 0 : _j.reasoning_tokens) != null ? _k : void 0;\n              usage.cachedInputTokens = (_m = (_l = value.response.usage.input_tokens_details) == null ? void 0 : _l.cached_tokens) != null ? _m : void 0;\n            } else if (isResponseAnnotationAddedChunk2(value)) {\n              if (value.annotation.type === \"url_citation\") {\n                controller.enqueue({\n                  type: \"source\",\n                  sourceType: \"url\",\n                  id: (_p = (_o = (_n = self.config).generateId) == null ? void 0 : _o.call(_n)) != null ? _p : generateId2(),\n                  url: value.annotation.url,\n                  title: value.annotation.title\n                });\n              } else if (value.annotation.type === \"file_citation\") {\n                controller.enqueue({\n                  type: \"source\",\n                  sourceType: \"document\",\n                  id: (_s = (_r = (_q = self.config).generateId) == null ? void 0 : _r.call(_q)) != null ? _s : generateId2(),\n                  mediaType: \"text/plain\",\n                  title: value.annotation.quote,\n                  filename: value.annotation.file_id\n                });\n              }\n            } else if (isErrorChunk2(value)) {\n              controller.enqueue({ type: \"error\", error: value });\n            }\n          },\n          flush(controller) {\n            const providerMetadata = {\n              openai: {\n                responseId\n              }\n            };\n            if (logprobs.length > 0) {\n              providerMetadata.openai.logprobs = logprobs;\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n              providerMetadata\n            });\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar usageSchema22 = z$1.object({\n  input_tokens: z$1.number(),\n  input_tokens_details: z$1.object({ cached_tokens: z$1.number().nullish() }).nullish(),\n  output_tokens: z$1.number(),\n  output_tokens_details: z$1.object({ reasoning_tokens: z$1.number().nullish() }).nullish()\n});\nvar textDeltaChunkSchema2 = z$1.object({\n  type: z$1.literal(\"response.output_text.delta\"),\n  item_id: z$1.string(),\n  delta: z$1.string(),\n  logprobs: LOGPROBS_SCHEMA.nullish()\n});\nvar errorChunkSchema2 = z$1.object({\n  type: z$1.literal(\"error\"),\n  code: z$1.string(),\n  message: z$1.string(),\n  param: z$1.string().nullish(),\n  sequence_number: z$1.number()\n});\nvar responseFinishedChunkSchema2 = z$1.object({\n  type: z$1.enum([\"response.completed\", \"response.incomplete\"]),\n  response: z$1.object({\n    incomplete_details: z$1.object({ reason: z$1.string() }).nullish(),\n    usage: usageSchema22\n  })\n});\nvar responseCreatedChunkSchema2 = z$1.object({\n  type: z$1.literal(\"response.created\"),\n  response: z$1.object({\n    id: z$1.string(),\n    created_at: z$1.number(),\n    model: z$1.string()\n  })\n});\nvar responseOutputItemAddedSchema2 = z$1.object({\n  type: z$1.literal(\"response.output_item.added\"),\n  output_index: z$1.number(),\n  item: z$1.discriminatedUnion(\"type\", [\n    z$1.object({\n      type: z$1.literal(\"message\"),\n      id: z$1.string()\n    }),\n    z$1.object({\n      type: z$1.literal(\"reasoning\"),\n      id: z$1.string(),\n      encrypted_content: z$1.string().nullish()\n    }),\n    z$1.object({\n      type: z$1.literal(\"function_call\"),\n      id: z$1.string(),\n      call_id: z$1.string(),\n      name: z$1.string(),\n      arguments: z$1.string()\n    }),\n    z$1.object({\n      type: z$1.literal(\"web_search_call\"),\n      id: z$1.string(),\n      status: z$1.string(),\n      action: z$1.object({\n        type: z$1.literal(\"search\"),\n        query: z$1.string().optional()\n      }).nullish()\n    }),\n    z$1.object({\n      type: z$1.literal(\"computer_call\"),\n      id: z$1.string(),\n      status: z$1.string()\n    }),\n    z$1.object({\n      type: z$1.literal(\"file_search_call\"),\n      id: z$1.string(),\n      status: z$1.string(),\n      queries: z$1.array(z$1.string()).nullish(),\n      results: z$1.array(\n        z$1.object({\n          attributes: z$1.object({\n            file_id: z$1.string(),\n            filename: z$1.string(),\n            score: z$1.number(),\n            text: z$1.string()\n          })\n        })\n      ).optional()\n    })\n  ])\n});\nvar responseOutputItemDoneSchema2 = z$1.object({\n  type: z$1.literal(\"response.output_item.done\"),\n  output_index: z$1.number(),\n  item: z$1.discriminatedUnion(\"type\", [\n    z$1.object({\n      type: z$1.literal(\"message\"),\n      id: z$1.string()\n    }),\n    z$1.object({\n      type: z$1.literal(\"reasoning\"),\n      id: z$1.string(),\n      encrypted_content: z$1.string().nullish()\n    }),\n    z$1.object({\n      type: z$1.literal(\"function_call\"),\n      id: z$1.string(),\n      call_id: z$1.string(),\n      name: z$1.string(),\n      arguments: z$1.string(),\n      status: z$1.literal(\"completed\")\n    }),\n    z$1.object({\n      type: z$1.literal(\"web_search_call\"),\n      id: z$1.string(),\n      status: z$1.literal(\"completed\"),\n      action: z$1.object({\n        type: z$1.literal(\"search\"),\n        query: z$1.string().optional()\n      }).nullish()\n    }),\n    z$1.object({\n      type: z$1.literal(\"computer_call\"),\n      id: z$1.string(),\n      status: z$1.literal(\"completed\")\n    }),\n    z$1.object({\n      type: z$1.literal(\"file_search_call\"),\n      id: z$1.string(),\n      status: z$1.literal(\"completed\"),\n      queries: z$1.array(z$1.string()).nullish(),\n      results: z$1.array(\n        z$1.object({\n          attributes: z$1.object({\n            file_id: z$1.string(),\n            filename: z$1.string(),\n            score: z$1.number(),\n            text: z$1.string()\n          })\n        })\n      ).nullish()\n    })\n  ])\n});\nvar responseFunctionCallArgumentsDeltaSchema2 = z$1.object({\n  type: z$1.literal(\"response.function_call_arguments.delta\"),\n  item_id: z$1.string(),\n  output_index: z$1.number(),\n  delta: z$1.string()\n});\nvar responseAnnotationAddedSchema2 = z$1.object({\n  type: z$1.literal(\"response.output_text.annotation.added\"),\n  annotation: z$1.discriminatedUnion(\"type\", [\n    z$1.object({\n      type: z$1.literal(\"url_citation\"),\n      url: z$1.string(),\n      title: z$1.string()\n    }),\n    z$1.object({\n      type: z$1.literal(\"file_citation\"),\n      file_id: z$1.string(),\n      quote: z$1.string()\n    })\n  ])\n});\nvar responseReasoningSummaryPartAddedSchema = z$1.object({\n  type: z$1.literal(\"response.reasoning_summary_part.added\"),\n  item_id: z$1.string(),\n  summary_index: z$1.number()\n});\nvar responseReasoningSummaryTextDeltaSchema2 = z$1.object({\n  type: z$1.literal(\"response.reasoning_summary_text.delta\"),\n  item_id: z$1.string(),\n  summary_index: z$1.number(),\n  delta: z$1.string()\n});\nvar openaiResponsesChunkSchema2 = z$1.union([\n  textDeltaChunkSchema2,\n  responseFinishedChunkSchema2,\n  responseCreatedChunkSchema2,\n  responseOutputItemAddedSchema2,\n  responseOutputItemDoneSchema2,\n  responseFunctionCallArgumentsDeltaSchema2,\n  responseAnnotationAddedSchema2,\n  responseReasoningSummaryPartAddedSchema,\n  responseReasoningSummaryTextDeltaSchema2,\n  errorChunkSchema2,\n  z$1.object({ type: z$1.string() }).loose()\n  // fallback for unknown chunks\n]);\nfunction isTextDeltaChunk2(chunk) {\n  return chunk.type === \"response.output_text.delta\";\n}\nfunction isResponseOutputItemDoneChunk2(chunk) {\n  return chunk.type === \"response.output_item.done\";\n}\nfunction isResponseOutputItemDoneReasoningChunk(chunk) {\n  return isResponseOutputItemDoneChunk2(chunk) && chunk.item.type === \"reasoning\";\n}\nfunction isResponseFinishedChunk2(chunk) {\n  return chunk.type === \"response.completed\" || chunk.type === \"response.incomplete\";\n}\nfunction isResponseCreatedChunk2(chunk) {\n  return chunk.type === \"response.created\";\n}\nfunction isResponseFunctionCallArgumentsDeltaChunk2(chunk) {\n  return chunk.type === \"response.function_call_arguments.delta\";\n}\nfunction isResponseOutputItemAddedChunk2(chunk) {\n  return chunk.type === \"response.output_item.added\";\n}\nfunction isResponseOutputItemAddedReasoningChunk(chunk) {\n  return isResponseOutputItemAddedChunk2(chunk) && chunk.item.type === \"reasoning\";\n}\nfunction isResponseAnnotationAddedChunk2(chunk) {\n  return chunk.type === \"response.output_text.annotation.added\";\n}\nfunction isResponseReasoningSummaryPartAddedChunk(chunk) {\n  return chunk.type === \"response.reasoning_summary_part.added\";\n}\nfunction isResponseReasoningSummaryTextDeltaChunk2(chunk) {\n  return chunk.type === \"response.reasoning_summary_text.delta\";\n}\nfunction isErrorChunk2(chunk) {\n  return chunk.type === \"error\";\n}\nfunction getResponsesModelConfig2(modelId) {\n  const supportsFlexProcessing2 = modelId.startsWith(\"o3\") || modelId.startsWith(\"o4-mini\") || modelId.startsWith(\"gpt-5\") && !modelId.startsWith(\"gpt-5-chat\");\n  const supportsPriorityProcessing2 = modelId.startsWith(\"gpt-4\") || modelId.startsWith(\"gpt-5-mini\") || modelId.startsWith(\"gpt-5\") && !modelId.startsWith(\"gpt-5-nano\") && !modelId.startsWith(\"gpt-5-chat\") || modelId.startsWith(\"o3\") || modelId.startsWith(\"o4-mini\");\n  const defaults = {\n    requiredAutoTruncation: false,\n    systemMessageMode: \"system\",\n    supportsFlexProcessing: supportsFlexProcessing2,\n    supportsPriorityProcessing: supportsPriorityProcessing2\n  };\n  if (modelId.startsWith(\"gpt-5-chat\")) {\n    return {\n      ...defaults,\n      isReasoningModel: false\n    };\n  }\n  if (modelId.startsWith(\"o\") || modelId.startsWith(\"gpt-5\") || modelId.startsWith(\"codex-\") || modelId.startsWith(\"computer-use\")) {\n    if (modelId.startsWith(\"o1-mini\") || modelId.startsWith(\"o1-preview\")) {\n      return {\n        ...defaults,\n        isReasoningModel: true,\n        systemMessageMode: \"remove\"\n      };\n    }\n    return {\n      ...defaults,\n      isReasoningModel: true,\n      systemMessageMode: \"developer\"\n    };\n  }\n  return {\n    ...defaults,\n    isReasoningModel: false\n  };\n}\nvar openaiResponsesProviderOptionsSchema2 = z$1.object({\n  metadata: z$1.any().nullish(),\n  parallelToolCalls: z$1.boolean().nullish(),\n  previousResponseId: z$1.string().nullish(),\n  store: z$1.boolean().nullish(),\n  user: z$1.string().nullish(),\n  reasoningEffort: z$1.string().nullish(),\n  strictJsonSchema: z$1.boolean().nullish(),\n  instructions: z$1.string().nullish(),\n  reasoningSummary: z$1.string().nullish(),\n  serviceTier: z$1.enum([\"auto\", \"flex\", \"priority\"]).nullish(),\n  include: z$1.array(\n    z$1.enum([\n      \"reasoning.encrypted_content\",\n      \"file_search_call.results\",\n      \"message.output_text.logprobs\"\n    ])\n  ).nullish(),\n  textVerbosity: z$1.enum([\"low\", \"medium\", \"high\"]).nullish(),\n  promptCacheKey: z$1.string().nullish(),\n  safetyIdentifier: z$1.string().nullish(),\n  /**\n   * Return the log probabilities of the tokens.\n   *\n   * Setting to true will return the log probabilities of the tokens that\n   * were generated.\n   *\n   * Setting to a number will return the log probabilities of the top n\n   * tokens that were generated.\n   *\n   * @see https://platform.openai.com/docs/api-reference/responses/create\n   * @see https://cookbook.openai.com/examples/using_logprobs\n   */\n  logprobs: z$1.union([z$1.boolean(), z$1.number().min(1).max(TOP_LOGPROBS_MAX)]).optional()\n});\nvar OpenAIProviderOptionsSchema2 = z$1.object({\n  instructions: z$1.string().nullish(),\n  speed: z$1.number().min(0.25).max(4).default(1).nullish()\n});\nvar OpenAISpeechModel2 = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    text,\n    voice = \"alloy\",\n    outputFormat = \"mp3\",\n    speed,\n    instructions,\n    language,\n    providerOptions\n  }) {\n    const warnings = [];\n    const openAIOptions = await parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions,\n      schema: OpenAIProviderOptionsSchema2\n    });\n    const requestBody = {\n      model: this.modelId,\n      input: text,\n      voice,\n      response_format: \"mp3\",\n      speed,\n      instructions\n    };\n    if (outputFormat) {\n      if ([\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"].includes(outputFormat)) {\n        requestBody.response_format = outputFormat;\n      } else {\n        warnings.push({\n          type: \"unsupported-setting\",\n          setting: \"outputFormat\",\n          details: `Unsupported output format: ${outputFormat}. Using mp3 instead.`\n        });\n      }\n    }\n    if (openAIOptions) {\n      const speechModelOptions = {};\n      for (const key in speechModelOptions) {\n        const value = speechModelOptions[key];\n        if (value !== void 0) {\n          requestBody[key] = value;\n        }\n      }\n    }\n    if (language) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"language\",\n        details: `OpenAI speech models do not support language selection. Language parameter \"${language}\" was ignored.`\n      });\n    }\n    return {\n      requestBody,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { requestBody, warnings } = await this.getArgs(options);\n    const {\n      value: audio,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/audio/speech\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: requestBody,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createBinaryResponseHandler2(),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      audio,\n      warnings,\n      request: {\n        body: JSON.stringify(requestBody)\n      },\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar openAITranscriptionProviderOptions = z$1.object({\n  /**\n   * Additional information to include in the transcription response.\n   */\n  include: z$1.array(z$1.string()).optional(),\n  /**\n   * The language of the input audio in ISO-639-1 format.\n   */\n  language: z$1.string().optional(),\n  /**\n   * An optional text to guide the model's style or continue a previous audio segment.\n   */\n  prompt: z$1.string().optional(),\n  /**\n   * The sampling temperature, between 0 and 1.\n   * @default 0\n   */\n  temperature: z$1.number().min(0).max(1).default(0).optional(),\n  /**\n   * The timestamp granularities to populate for this transcription.\n   * @default ['segment']\n   */\n  timestampGranularities: z$1.array(z$1.enum([\"word\", \"segment\"])).default([\"segment\"]).optional()\n});\nvar languageMap2 = {\n  afrikaans: \"af\",\n  arabic: \"ar\",\n  armenian: \"hy\",\n  azerbaijani: \"az\",\n  belarusian: \"be\",\n  bosnian: \"bs\",\n  bulgarian: \"bg\",\n  catalan: \"ca\",\n  chinese: \"zh\",\n  croatian: \"hr\",\n  czech: \"cs\",\n  danish: \"da\",\n  dutch: \"nl\",\n  english: \"en\",\n  estonian: \"et\",\n  finnish: \"fi\",\n  french: \"fr\",\n  galician: \"gl\",\n  german: \"de\",\n  greek: \"el\",\n  hebrew: \"he\",\n  hindi: \"hi\",\n  hungarian: \"hu\",\n  icelandic: \"is\",\n  indonesian: \"id\",\n  italian: \"it\",\n  japanese: \"ja\",\n  kannada: \"kn\",\n  kazakh: \"kk\",\n  korean: \"ko\",\n  latvian: \"lv\",\n  lithuanian: \"lt\",\n  macedonian: \"mk\",\n  malay: \"ms\",\n  marathi: \"mr\",\n  maori: \"mi\",\n  nepali: \"ne\",\n  norwegian: \"no\",\n  persian: \"fa\",\n  polish: \"pl\",\n  portuguese: \"pt\",\n  romanian: \"ro\",\n  russian: \"ru\",\n  serbian: \"sr\",\n  slovak: \"sk\",\n  slovenian: \"sl\",\n  spanish: \"es\",\n  swahili: \"sw\",\n  swedish: \"sv\",\n  tagalog: \"tl\",\n  tamil: \"ta\",\n  thai: \"th\",\n  turkish: \"tr\",\n  ukrainian: \"uk\",\n  urdu: \"ur\",\n  vietnamese: \"vi\",\n  welsh: \"cy\"\n};\nvar OpenAITranscriptionModel2 = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    audio,\n    mediaType,\n    providerOptions\n  }) {\n    const warnings = [];\n    const openAIOptions = await parseProviderOptions2({\n      provider: \"openai\",\n      providerOptions,\n      schema: openAITranscriptionProviderOptions\n    });\n    const formData = new FormData();\n    const blob = audio instanceof Uint8Array ? new Blob([audio]) : new Blob([convertBase64ToUint8Array2(audio)]);\n    formData.append(\"model\", this.modelId);\n    formData.append(\"file\", new File([blob], \"audio\", { type: mediaType }));\n    if (openAIOptions) {\n      const transcriptionModelOptions = {\n        include: openAIOptions.include,\n        language: openAIOptions.language,\n        prompt: openAIOptions.prompt,\n        response_format: \"verbose_json\",\n        // always use verbose_json to get segments\n        temperature: openAIOptions.temperature,\n        timestamp_granularities: openAIOptions.timestampGranularities\n      };\n      for (const [key, value] of Object.entries(transcriptionModelOptions)) {\n        if (value != null) {\n          formData.append(key, String(value));\n        }\n      }\n    }\n    return {\n      formData,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h;\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { formData, warnings } = await this.getArgs(options);\n    const {\n      value: response,\n      responseHeaders,\n      rawValue: rawResponse\n    } = await postFormDataToApi2({\n      url: this.config.url({\n        path: \"/audio/transcriptions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      formData,\n      failedResponseHandler: openaiFailedResponseHandler2,\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiTranscriptionResponseSchema2\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const language = response.language != null && response.language in languageMap2 ? languageMap2[response.language] : void 0;\n    return {\n      text: response.text,\n      segments: (_g = (_f = (_d = response.segments) == null ? void 0 : _d.map((segment) => ({\n        text: segment.text,\n        startSecond: segment.start,\n        endSecond: segment.end\n      }))) != null ? _f : (_e = response.words) == null ? void 0 : _e.map((word) => ({\n        text: word.word,\n        startSecond: word.start,\n        endSecond: word.end\n      }))) != null ? _g : [],\n      language,\n      durationInSeconds: (_h = response.duration) != null ? _h : void 0,\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n        body: rawResponse\n      }\n    };\n  }\n};\nvar openaiTranscriptionResponseSchema2 = z$1.object({\n  text: z$1.string(),\n  language: z$1.string().nullish(),\n  duration: z$1.number().nullish(),\n  words: z$1.array(\n    z$1.object({\n      word: z$1.string(),\n      start: z$1.number(),\n      end: z$1.number()\n    })\n  ).nullish(),\n  segments: z$1.array(\n    z$1.object({\n      id: z$1.number(),\n      seek: z$1.number(),\n      start: z$1.number(),\n      end: z$1.number(),\n      text: z$1.string(),\n      tokens: z$1.array(z$1.number()),\n      temperature: z$1.number(),\n      avg_logprob: z$1.number(),\n      compression_ratio: z$1.number(),\n      no_speech_prob: z$1.number()\n    })\n  ).nullish()\n});\nfunction createOpenAI2(options = {}) {\n  var _a16, _b;\n  const baseURL = (_a16 = withoutTrailingSlash2(options.baseURL)) != null ? _a16 : \"https://api.openai.com/v1\";\n  const providerName = (_b = options.name) != null ? _b : \"openai\";\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey2({\n      apiKey: options.apiKey,\n      environmentVariableName: \"OPENAI_API_KEY\",\n      description: \"OpenAI\"\n    })}`,\n    \"OpenAI-Organization\": options.organization,\n    \"OpenAI-Project\": options.project,\n    ...options.headers\n  });\n  const createChatModel = (modelId) => new OpenAIChatLanguageModel2(modelId, {\n    provider: `${providerName}.chat`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createCompletionModel = (modelId) => new OpenAICompletionLanguageModel2(modelId, {\n    provider: `${providerName}.completion`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createEmbeddingModel = (modelId) => new OpenAIEmbeddingModel2(modelId, {\n    provider: `${providerName}.embedding`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createImageModel = (modelId) => new OpenAIImageModel2(modelId, {\n    provider: `${providerName}.image`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createTranscriptionModel = (modelId) => new OpenAITranscriptionModel2(modelId, {\n    provider: `${providerName}.transcription`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createSpeechModel = (modelId) => new OpenAISpeechModel2(modelId, {\n    provider: `${providerName}.speech`,\n    url: ({ path }) => `${baseURL}${path}`,\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId) => {\n    if (new.target) {\n      throw new Error(\n        \"The OpenAI model function cannot be called with the new keyword.\"\n      );\n    }\n    return createResponsesModel(modelId);\n  };\n  const createResponsesModel = (modelId) => {\n    return new OpenAIResponsesLanguageModel2(modelId, {\n      provider: `${providerName}.responses`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      fileIdPrefixes: [\"file-\"]\n    });\n  };\n  const provider = function(modelId) {\n    return createLanguageModel(modelId);\n  };\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.completion = createCompletionModel;\n  provider.responses = createResponsesModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.image = createImageModel;\n  provider.imageModel = createImageModel;\n  provider.transcription = createTranscriptionModel;\n  provider.transcriptionModel = createTranscriptionModel;\n  provider.speech = createSpeechModel;\n  provider.speechModel = createSpeechModel;\n  provider.tools = openaiTools2;\n  return provider;\n}\nvar openai2 = createOpenAI2();\nfunction getOpenAIMetadata(message) {\n  var _a16, _b;\n  return (_b = (_a16 = message == null ? void 0 : message.providerMetadata) == null ? void 0 : _a16.openaiCompatible) != null ? _b : {};\n}\nfunction convertToOpenAICompatibleChatMessages(prompt) {\n  const messages = [];\n  for (const { role, content, ...message } of prompt) {\n    const metadata = getOpenAIMetadata({ ...message });\n    switch (role) {\n      case \"system\": {\n        messages.push({ role: \"system\", content, ...metadata });\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({\n            role: \"user\",\n            content: content[0].text,\n            ...getOpenAIMetadata(content[0])\n          });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part) => {\n            var _a16;\n            const partMetadata = getOpenAIMetadata(part);\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text, ...partMetadata };\n              }\n              case \"image\": {\n                return {\n                  type: \"image_url\",\n                  image_url: {\n                    url: part.image instanceof URL ? part.image.toString() : `data:${(_a16 = part.mimeType) != null ? _a16 : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`\n                  },\n                  ...partMetadata\n                };\n              }\n              case \"file\": {\n                throw new UnsupportedFunctionalityError({\n                  functionality: \"File content parts in user messages\"\n                });\n              }\n            }\n          }),\n          ...metadata\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          const partMetadata = getOpenAIMetadata(part);\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args)\n                },\n                ...partMetadata\n              });\n              break;\n            }\n          }\n        }\n        messages.push({\n          role: \"assistant\",\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : void 0,\n          ...metadata\n        });\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          const toolResponseMetadata = getOpenAIMetadata(toolResponse);\n          messages.push({\n            role: \"tool\",\n            tool_call_id: toolResponse.toolCallId,\n            content: JSON.stringify(toolResponse.result),\n            ...toolResponseMetadata\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return messages;\n}\nfunction getResponseMetadata5({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nfunction mapOpenAICompatibleFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\nvar openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish()\n  })\n});\nvar defaultOpenAICompatibleErrorStructure = {\n  errorSchema: openaiCompatibleErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n};\nfunction prepareTools8({\n  mode,\n  structuredOutputs\n}) {\n  var _a16;\n  const tools = ((_a16 = mode.tools) == null ? void 0 : _a16.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, tool_choice: void 0, toolWarnings };\n  }\n  const toolChoice = mode.toolChoice;\n  const openaiCompatTools = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n    } else {\n      openaiCompatTools.push({\n        type: \"function\",\n        function: {\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.parameters\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: openaiCompatTools, tool_choice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: openaiCompatTools, tool_choice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: openaiCompatTools,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar OpenAICompatibleChatLanguageModel = class {\n  // type inferred via constructor\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    var _a16, _b;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    const errorStructure = (_a16 = config.errorStructure) != null ? _a16 : defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(\n      errorStructure.errorSchema\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n    this.supportsStructuredOutputs = (_b = config.supportsStructuredOutputs) != null ? _b : false;\n  }\n  get defaultObjectGenerationMode() {\n    return this.config.defaultObjectGenerationMode;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerMetadata,\n    stopSequences,\n    responseFormat,\n    seed\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !this.supportsStructuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? this.supportsStructuredOutputs === true && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          name: (_a16 = responseFormat.name) != null ? _a16 : \"response\",\n          description: responseFormat.description\n        }\n      } : { type: \"json_object\" } : void 0,\n      stop: stopSequences,\n      seed,\n      ...providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName],\n      reasoning_effort: (_d = (_b = providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]) == null ? void 0 : _b.reasoningEffort) != null ? _d : (_c = providerMetadata == null ? void 0 : providerMetadata[\"openai-compatible\"]) == null ? void 0 : _c.reasoningEffort,\n      // messages:\n      messages: convertToOpenAICompatibleChatMessages(prompt)\n    };\n    switch (type) {\n      case \"regular\": {\n        const { tools, tool_choice, toolWarnings } = prepareTools8({\n          mode,\n          structuredOutputs: this.supportsStructuredOutputs\n        });\n        return {\n          args: { ...baseArgs, tools, tool_choice },\n          warnings: [...warnings, ...toolWarnings]\n        };\n      }\n      case \"object-json\": {\n        return {\n          args: {\n            ...baseArgs,\n            response_format: this.supportsStructuredOutputs === true && mode.schema != null ? {\n              type: \"json_schema\",\n              json_schema: {\n                schema: mode.schema,\n                name: (_e = mode.name) != null ? _e : \"response\",\n                description: mode.description\n              }\n            } : { type: \"json_object\" }\n          },\n          warnings\n        };\n      }\n      case \"object-tool\": {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: {\n              type: \"function\",\n              function: { name: mode.tool.name }\n            },\n            tools: [\n              {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters\n                }\n              }\n            ]\n          },\n          warnings\n        };\n      }\n      default: {\n        const _exhaustiveCheck = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k;\n    const { args, warnings } = this.getArgs({ ...options });\n    const body = JSON.stringify(args);\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        OpenAICompatibleChatResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const choice = responseBody.choices[0];\n    const providerMetadata = {\n      [this.providerOptionsName]: {},\n      ...(_b = (_a16 = this.config.metadataExtractor) == null ? void 0 : _a16.extractMetadata) == null ? void 0 : _b.call(_a16, {\n        parsedBody: rawResponse\n      })\n    };\n    const completionTokenDetails = (_c = responseBody.usage) == null ? void 0 : _c.completion_tokens_details;\n    const promptTokenDetails = (_d = responseBody.usage) == null ? void 0 : _d.prompt_tokens_details;\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens) != null) {\n      providerMetadata[this.providerOptionsName].reasoningTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens) != null) {\n      providerMetadata[this.providerOptionsName].acceptedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens) != null) {\n      providerMetadata[this.providerOptionsName].rejectedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens;\n    }\n    if ((promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens) != null) {\n      providerMetadata[this.providerOptionsName].cachedPromptTokens = promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens;\n    }\n    return {\n      text: (_e = choice.message.content) != null ? _e : void 0,\n      reasoning: (_f = choice.message.reasoning_content) != null ? _f : void 0,\n      toolCalls: (_g = choice.message.tool_calls) == null ? void 0 : _g.map((toolCall) => {\n        var _a23;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a23 = toolCall.id) != null ? _a23 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_i = (_h = responseBody.usage) == null ? void 0 : _h.prompt_tokens) != null ? _i : NaN,\n        completionTokens: (_k = (_j = responseBody.usage) == null ? void 0 : _j.completion_tokens) != null ? _k : NaN\n      },\n      providerMetadata,\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata5(responseBody),\n      warnings,\n      request: { body }\n    };\n  }\n  async doStream(options) {\n    var _a16;\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({ type: \"response-metadata\", ...result.response });\n          if (result.reasoning) {\n            if (Array.isArray(result.reasoning)) {\n              for (const part of result.reasoning) {\n                if (part.type === \"text\") {\n                  controller.enqueue({\n                    type: \"reasoning\",\n                    textDelta: part.text\n                  });\n                }\n              }\n            } else {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: result.reasoning\n              });\n            }\n          }\n          if (result.text) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: result.text\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: \"tool-call\",\n                ...toolCall\n              });\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings\n      };\n    }\n    const { args, warnings } = this.getArgs({ ...options });\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage ? { include_usage: true } : void 0\n    };\n    const metadataExtractor = (_a16 = this.config.metadataExtractor) == null ? void 0 : _a16.createStreamExtractor();\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      completionTokens: void 0,\n      completionTokensDetails: {\n        reasoningTokens: void 0,\n        acceptedPredictionTokens: void 0,\n        rejectedPredictionTokens: void 0\n      },\n      promptTokens: void 0,\n      promptTokensDetails: {\n        cachedTokens: void 0\n      }\n    };\n    let isFirstChunk = true;\n    let providerOptionsName = this.providerOptionsName;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n          transform(chunk, controller) {\n            var _a23, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l;\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            metadataExtractor == null ? void 0 : metadataExtractor.processChunk(chunk.rawValue);\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error.message });\n              return;\n            }\n            if (isFirstChunk) {\n              isFirstChunk = false;\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata5(value)\n              });\n            }\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                prompt_tokens_details,\n                completion_tokens_details\n              } = value.usage;\n              usage.promptTokens = prompt_tokens != null ? prompt_tokens : void 0;\n              usage.completionTokens = completion_tokens != null ? completion_tokens : void 0;\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens) != null) {\n                usage.completionTokensDetails.reasoningTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens) != null) {\n                usage.completionTokensDetails.acceptedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens;\n              }\n              if ((completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens) != null) {\n                usage.completionTokensDetails.rejectedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens;\n              }\n              if ((prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens) != null) {\n                usage.promptTokensDetails.cachedTokens = prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens;\n              }\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason\n              );\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            if (delta.reasoning_content != null) {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: delta.reasoning_content\n              });\n            }\n            if (delta.content != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                textDelta: delta.content\n              });\n            }\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== \"function\") {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`\n                    });\n                  }\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`\n                    });\n                  }\n                  if (((_a23 = toolCallDelta.function) == null ? void 0 : _a23.name) == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`\n                    });\n                  }\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: \"function\",\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: (_b = toolCallDelta.function.arguments) != null ? _b : \"\"\n                    },\n                    hasFinished: false\n                  };\n                  const toolCall2 = toolCalls[index];\n                  if (((_c = toolCall2.function) == null ? void 0 : _c.name) != null && ((_d = toolCall2.function) == null ? void 0 : _d.arguments) != null) {\n                    if (toolCall2.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: \"tool-call-delta\",\n                        toolCallType: \"function\",\n                        toolCallId: toolCall2.id,\n                        toolName: toolCall2.function.name,\n                        argsTextDelta: toolCall2.function.arguments\n                      });\n                    }\n                    if (isParsableJson(toolCall2.function.arguments)) {\n                      controller.enqueue({\n                        type: \"tool-call\",\n                        toolCallType: \"function\",\n                        toolCallId: (_e = toolCall2.id) != null ? _e : generateId(),\n                        toolName: toolCall2.function.name,\n                        args: toolCall2.function.arguments\n                      });\n                      toolCall2.hasFinished = true;\n                    }\n                  }\n                  continue;\n                }\n                const toolCall = toolCalls[index];\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n                if (((_f = toolCallDelta.function) == null ? void 0 : _f.arguments) != null) {\n                  toolCall.function.arguments += (_h = (_g = toolCallDelta.function) == null ? void 0 : _g.arguments) != null ? _h : \"\";\n                }\n                controller.enqueue({\n                  type: \"tool-call-delta\",\n                  toolCallType: \"function\",\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: (_i = toolCallDelta.function.arguments) != null ? _i : \"\"\n                });\n                if (((_j = toolCall.function) == null ? void 0 : _j.name) != null && ((_k = toolCall.function) == null ? void 0 : _k.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                  controller.enqueue({\n                    type: \"tool-call\",\n                    toolCallType: \"function\",\n                    toolCallId: (_l = toolCall.id) != null ? _l : generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n          flush(controller) {\n            var _a23, _b;\n            const providerMetadata = {\n              [providerOptionsName]: {},\n              ...metadataExtractor == null ? void 0 : metadataExtractor.buildMetadata()\n            };\n            if (usage.completionTokensDetails.reasoningTokens != null) {\n              providerMetadata[providerOptionsName].reasoningTokens = usage.completionTokensDetails.reasoningTokens;\n            }\n            if (usage.completionTokensDetails.acceptedPredictionTokens != null) {\n              providerMetadata[providerOptionsName].acceptedPredictionTokens = usage.completionTokensDetails.acceptedPredictionTokens;\n            }\n            if (usage.completionTokensDetails.rejectedPredictionTokens != null) {\n              providerMetadata[providerOptionsName].rejectedPredictionTokens = usage.completionTokensDetails.rejectedPredictionTokens;\n            }\n            if (usage.promptTokensDetails.cachedTokens != null) {\n              providerMetadata[providerOptionsName].cachedPromptTokens = usage.promptTokensDetails.cachedTokens;\n            }\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage: {\n                promptTokens: (_a23 = usage.promptTokens) != null ? _a23 : NaN,\n                completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n              },\n              providerMetadata\n            });\n          }\n        })\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) }\n    };\n  }\n};\nvar openaiCompatibleTokenUsageSchema = z.object({\n  prompt_tokens: z.number().nullish(),\n  completion_tokens: z.number().nullish(),\n  prompt_tokens_details: z.object({\n    cached_tokens: z.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z.object({\n    reasoning_tokens: z.number().nullish(),\n    accepted_prediction_tokens: z.number().nullish(),\n    rejected_prediction_tokens: z.number().nullish()\n  }).nullish()\n}).nullish();\nvar OpenAICompatibleChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal(\"assistant\").nullish(),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        tool_calls: z.array(\n          z.object({\n            id: z.string().nullish(),\n            type: z.literal(\"function\"),\n            function: z.object({\n              name: z.string(),\n              arguments: z.string()\n            })\n          })\n        ).nullish()\n      }),\n      finish_reason: z.string().nullish()\n    })\n  ),\n  usage: openaiCompatibleTokenUsageSchema\n});\nvar createOpenAICompatibleChatChunkSchema = (errorSchema) => z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        delta: z.object({\n          role: z.enum([\"assistant\"]).nullish(),\n          content: z.string().nullish(),\n          reasoning_content: z.string().nullish(),\n          tool_calls: z.array(\n            z.object({\n              index: z.number().optional(),\n              id: z.string().nullish(),\n              type: z.literal(\"function\").nullish(),\n              function: z.object({\n                name: z.string().nullish(),\n                arguments: z.string().nullish()\n              })\n            })\n          ).nullish()\n        }).nullish(),\n        finish_reason: z.string().nullish()\n      })\n    ),\n    usage: openaiCompatibleTokenUsageSchema\n  }),\n  errorSchema\n]);\nz.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string()\n    })\n  ),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number()\n  }).nullish()\n});\nz.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish()\n});\nvar OpenAICompatibleImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get maxImagesPerCall() {\n    var _a16;\n    return (_a16 = this.settings.maxImagesPerCall) != null ? _a16 : 10;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(_d = providerOptions.openai) != null ? _d : {},\n        response_format: \"b64_json\",\n        ...this.settings.user ? { user: this.settings.user } : {}\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        (_e = this.config.errorStructure) != null ? _e : defaultOpenAICompatibleErrorStructure\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleImageResponseSchema\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map((item) => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiCompatibleImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() }))\n});\nfunction supportsStructuredOutputs(modelId) {\n  return [\n    \"grok-3\",\n    \"grok-3-beta\",\n    \"grok-3-latest\",\n    \"grok-3-fast\",\n    \"grok-3-fast-beta\",\n    \"grok-3-fast-latest\",\n    \"grok-3-mini\",\n    \"grok-3-mini-beta\",\n    \"grok-3-mini-latest\",\n    \"grok-3-mini-fast\",\n    \"grok-3-mini-fast-beta\",\n    \"grok-3-mini-fast-latest\",\n    \"grok-2-1212\",\n    \"grok-2-vision-1212\"\n  ].includes(modelId);\n}\nvar xaiErrorSchema = z.object({\n  code: z.string(),\n  error: z.string()\n});\nvar xaiErrorStructure = {\n  errorSchema: xaiErrorSchema,\n  errorToMessage: (data) => data.error\n};\nfunction createXai(options = {}) {\n  var _a16;\n  const baseURL = withoutTrailingSlash(\n    (_a16 = options.baseURL) != null ? _a16 : \"https://api.x.ai/v1\"\n  );\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: \"XAI_API_KEY\",\n      description: \"xAI API key\"\n    })}`,\n    ...options.headers\n  });\n  const createLanguageModel = (modelId, settings = {}) => {\n    const structuredOutputs = supportsStructuredOutputs(modelId);\n    return new OpenAICompatibleChatLanguageModel(modelId, settings, {\n      provider: \"xai.chat\",\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      defaultObjectGenerationMode: structuredOutputs ? \"json\" : \"tool\",\n      errorStructure: xaiErrorStructure,\n      supportsStructuredOutputs: structuredOutputs,\n      includeUsage: true\n    });\n  };\n  const createImageModel = (modelId, settings = {}) => {\n    return new OpenAICompatibleImageModel(modelId, settings, {\n      provider: \"xai.image\",\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      errorStructure: xaiErrorStructure\n    });\n  };\n  const provider = (modelId, settings) => createLanguageModel(modelId, settings);\n  provider.languageModel = createLanguageModel;\n  provider.chat = createLanguageModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.imageModel = createImageModel;\n  provider.image = createImageModel;\n  return provider;\n}\nvar xai = createXai();\nz$1.object({\n  /**\n   * A unique identifier representing your end-user, which can help the provider to\n   * monitor and detect abuse.\n   */\n  user: z$1.string().optional(),\n  /**\n   * Reasoning effort for reasoning models. Defaults to `medium`.\n   */\n  reasoningEffort: z$1.string().optional()\n});\nvar openaiCompatibleErrorDataSchema2 = z$1.object({\n  error: z$1.object({\n    message: z$1.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z$1.string().nullish(),\n    param: z$1.any().nullish(),\n    code: z$1.union([z$1.string(), z$1.number()]).nullish()\n  })\n});\nvar defaultOpenAICompatibleErrorStructure2 = {\n  errorSchema: openaiCompatibleErrorDataSchema2,\n  errorToMessage: (data) => data.error.message\n};\nvar openaiCompatibleTokenUsageSchema2 = z$1.object({\n  prompt_tokens: z$1.number().nullish(),\n  completion_tokens: z$1.number().nullish(),\n  total_tokens: z$1.number().nullish(),\n  prompt_tokens_details: z$1.object({\n    cached_tokens: z$1.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z$1.object({\n    reasoning_tokens: z$1.number().nullish(),\n    accepted_prediction_tokens: z$1.number().nullish(),\n    rejected_prediction_tokens: z$1.number().nullish()\n  }).nullish()\n}).nullish();\nz$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      message: z$1.object({\n        role: z$1.literal(\"assistant\").nullish(),\n        content: z$1.string().nullish(),\n        reasoning_content: z$1.string().nullish(),\n        reasoning: z$1.string().nullish(),\n        tool_calls: z$1.array(\n          z$1.object({\n            id: z$1.string().nullish(),\n            function: z$1.object({\n              name: z$1.string(),\n              arguments: z$1.string()\n            })\n          })\n        ).nullish()\n      }),\n      finish_reason: z$1.string().nullish()\n    })\n  ),\n  usage: openaiCompatibleTokenUsageSchema2\n});\nz$1.object({\n  /**\n   * Echo back the prompt in addition to the completion.\n   */\n  echo: z$1.boolean().optional(),\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in\n   * the GPT tokenizer) to an associated bias value from -100 to 100.\n   */\n  logitBias: z$1.record(z$1.string(), z$1.number()).optional(),\n  /**\n   * The suffix that comes after a completion of inserted text.\n   */\n  suffix: z$1.string().optional(),\n  /**\n   * A unique identifier representing your end-user, which can help providers to\n   * monitor and detect abuse.\n   */\n  user: z$1.string().optional()\n});\nvar usageSchema4 = z$1.object({\n  prompt_tokens: z$1.number(),\n  completion_tokens: z$1.number(),\n  total_tokens: z$1.number()\n});\nz$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      text: z$1.string(),\n      finish_reason: z$1.string()\n    })\n  ),\n  usage: usageSchema4.nullish()\n});\nz$1.object({\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in text-embedding-3 and later models.\n   */\n  dimensions: z$1.number().optional(),\n  /**\n   * A unique identifier representing your end-user, which can help providers to\n   * monitor and detect abuse.\n   */\n  user: z$1.string().optional()\n});\nz$1.object({\n  data: z$1.array(z$1.object({ embedding: z$1.array(z$1.number()) })),\n  usage: z$1.object({ prompt_tokens: z$1.number() }).nullish(),\n  providerMetadata: z$1.record(z$1.string(), z$1.record(z$1.string(), z$1.any())).optional()\n});\nvar OpenAICompatibleImageModel2 = class {\n  constructor(modelId, config) {\n    this.modelId = modelId;\n    this.config = config;\n    this.specificationVersion = \"v2\";\n    this.maxImagesPerCall = 10;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a16, _b, _c, _d, _e;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"seed\" });\n    }\n    const currentDate = (_c = (_b = (_a16 = this.config._internal) == null ? void 0 : _a16.currentDate) == null ? void 0 : _b.call(_a16)) != null ? _c : /* @__PURE__ */ new Date();\n    const { value: response, responseHeaders } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(_d = providerOptions.openai) != null ? _d : {},\n        response_format: \"b64_json\"\n      },\n      failedResponseHandler: createJsonErrorResponseHandler2(\n        (_e = this.config.errorStructure) != null ? _e : defaultOpenAICompatibleErrorStructure2\n      ),\n      successfulResponseHandler: createJsonResponseHandler2(\n        openaiCompatibleImageResponseSchema2\n      ),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map((item) => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiCompatibleImageResponseSchema2 = z$1.object({\n  data: z$1.array(z$1.object({ b64_json: z$1.string() }))\n});\nfunction convertToXaiChatMessages(prompt) {\n  const messages = [];\n  const warnings = [];\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case \"system\": {\n        messages.push({ role: \"system\", content });\n        break;\n      }\n      case \"user\": {\n        if (content.length === 1 && content[0].type === \"text\") {\n          messages.push({ role: \"user\", content: content[0].text });\n          break;\n        }\n        messages.push({\n          role: \"user\",\n          content: content.map((part) => {\n            switch (part.type) {\n              case \"text\": {\n                return { type: \"text\", text: part.text };\n              }\n              case \"file\": {\n                if (part.mediaType.startsWith(\"image/\")) {\n                  const mediaType = part.mediaType === \"image/*\" ? \"image/jpeg\" : part.mediaType;\n                  return {\n                    type: \"image_url\",\n                    image_url: {\n                      url: part.data instanceof URL ? part.data.toString() : `data:${mediaType};base64,${convertToBase64(part.data)}`\n                    }\n                  };\n                } else {\n                  throw new UnsupportedFunctionalityError2({\n                    functionality: `file part media type ${part.mediaType}`\n                  });\n                }\n              }\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        let text = \"\";\n        const toolCalls = [];\n        for (const part of content) {\n          switch (part.type) {\n            case \"text\": {\n              text += part.text;\n              break;\n            }\n            case \"tool-call\": {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: \"function\",\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.input)\n                }\n              });\n              break;\n            }\n          }\n        }\n        messages.push({\n          role: \"assistant\",\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : void 0\n        });\n        break;\n      }\n      case \"tool\": {\n        for (const toolResponse of content) {\n          const output = toolResponse.output;\n          let contentValue;\n          switch (output.type) {\n            case \"text\":\n            case \"error-text\":\n              contentValue = output.value;\n              break;\n            case \"content\":\n            case \"json\":\n            case \"error-json\":\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n          messages.push({\n            role: \"tool\",\n            tool_call_id: toolResponse.toolCallId,\n            content: contentValue\n          });\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n  return { messages, warnings };\n}\nfunction getResponseMetadata6({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\nfunction mapXaiFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"tool_calls\":\n    case \"function_call\":\n      return \"tool-calls\";\n    case \"content_filter\":\n      return \"content-filter\";\n    default:\n      return \"unknown\";\n  }\n}\nvar webSourceSchema = z$1.object({\n  type: z$1.literal(\"web\"),\n  country: z$1.string().length(2).optional(),\n  excludedWebsites: z$1.array(z$1.string()).max(5).optional(),\n  allowedWebsites: z$1.array(z$1.string()).max(5).optional(),\n  safeSearch: z$1.boolean().optional()\n});\nvar xSourceSchema = z$1.object({\n  type: z$1.literal(\"x\"),\n  xHandles: z$1.array(z$1.string()).optional()\n});\nvar newsSourceSchema = z$1.object({\n  type: z$1.literal(\"news\"),\n  country: z$1.string().length(2).optional(),\n  excludedWebsites: z$1.array(z$1.string()).max(5).optional(),\n  safeSearch: z$1.boolean().optional()\n});\nvar rssSourceSchema = z$1.object({\n  type: z$1.literal(\"rss\"),\n  links: z$1.array(z$1.string().url()).max(1)\n  // currently only supports one RSS link\n});\nvar searchSourceSchema = z$1.discriminatedUnion(\"type\", [\n  webSourceSchema,\n  xSourceSchema,\n  newsSourceSchema,\n  rssSourceSchema\n]);\nvar xaiProviderOptions = z$1.object({\n  /**\n   * reasoning effort for reasoning models\n   * only supported by grok-3-mini and grok-3-mini-fast models\n   */\n  reasoningEffort: z$1.enum([\"low\", \"high\"]).optional(),\n  searchParameters: z$1.object({\n    /**\n     * search mode preference\n     * - \"off\": disables search completely\n     * - \"auto\": model decides whether to search (default)\n     * - \"on\": always enables search\n     */\n    mode: z$1.enum([\"off\", \"auto\", \"on\"]),\n    /**\n     * whether to return citations in the response\n     * defaults to true\n     */\n    returnCitations: z$1.boolean().optional(),\n    /**\n     * start date for search data (ISO8601 format: YYYY-MM-DD)\n     */\n    fromDate: z$1.string().optional(),\n    /**\n     * end date for search data (ISO8601 format: YYYY-MM-DD)\n     */\n    toDate: z$1.string().optional(),\n    /**\n     * maximum number of search results to consider\n     * defaults to 20\n     */\n    maxSearchResults: z$1.number().min(1).max(50).optional(),\n    /**\n     * data sources to search from\n     * defaults to [\"web\", \"x\"] if not specified\n     */\n    sources: z$1.array(searchSourceSchema).optional()\n  }).optional()\n});\nvar xaiErrorDataSchema = z$1.object({\n  error: z$1.object({\n    message: z$1.string(),\n    type: z$1.string().nullish(),\n    param: z$1.any().nullish(),\n    code: z$1.union([z$1.string(), z$1.number()]).nullish()\n  })\n});\nvar xaiFailedResponseHandler = createJsonErrorResponseHandler2({\n  errorSchema: xaiErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n});\nfunction prepareTools9({\n  tools,\n  toolChoice\n}) {\n  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return { tools: void 0, toolChoice: void 0, toolWarnings };\n  }\n  const xaiTools = [];\n  for (const tool2 of tools) {\n    if (tool2.type === \"provider-defined\") {\n      toolWarnings.push({ type: \"unsupported-tool\", tool: tool2 });\n    } else {\n      xaiTools.push({\n        type: \"function\",\n        function: {\n          name: tool2.name,\n          description: tool2.description,\n          parameters: tool2.inputSchema\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return { tools: xaiTools, toolChoice: void 0, toolWarnings };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n      return { tools: xaiTools, toolChoice: type, toolWarnings };\n    case \"required\":\n      return { tools: xaiTools, toolChoice: \"required\", toolWarnings };\n    case \"tool\":\n      return {\n        tools: xaiTools,\n        toolChoice: {\n          type: \"function\",\n          function: { name: toolChoice.toolName }\n        },\n        toolWarnings\n      };\n    default: {\n      const _exhaustiveCheck = type;\n      throw new UnsupportedFunctionalityError2({\n        functionality: `tool choice type: ${_exhaustiveCheck}`\n      });\n    }\n  }\n}\nvar XaiChatLanguageModel = class {\n  constructor(modelId, config) {\n    this.specificationVersion = \"v2\";\n    this.supportedUrls = {\n      \"image/*\": [/^https?:\\/\\/.*$/]\n    };\n    this.modelId = modelId;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    seed,\n    responseFormat,\n    providerOptions,\n    tools,\n    toolChoice\n  }) {\n    var _a16, _b, _c;\n    const warnings = [];\n    const options = (_a16 = await parseProviderOptions2({\n      provider: \"xai\",\n      providerOptions,\n      schema: xaiProviderOptions\n    })) != null ? _a16 : {};\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"frequencyPenalty\"\n      });\n    }\n    if (presencePenalty != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"presencePenalty\"\n      });\n    }\n    if (stopSequences != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"stopSequences\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type === \"json\" && responseFormat.schema != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is not supported\"\n      });\n    }\n    const { messages, warnings: messageWarnings } = convertToXaiChatMessages(prompt);\n    warnings.push(...messageWarnings);\n    const {\n      tools: xaiTools,\n      toolChoice: xaiToolChoice,\n      toolWarnings\n    } = prepareTools9({\n      tools,\n      toolChoice\n    });\n    warnings.push(...toolWarnings);\n    const baseArgs = {\n      // model id\n      model: this.modelId,\n      // standard generation settings\n      max_tokens: maxOutputTokens,\n      temperature,\n      top_p: topP,\n      seed,\n      reasoning_effort: options.reasoningEffort,\n      // response format\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          name: (_b = responseFormat.name) != null ? _b : \"response\",\n          schema: responseFormat.schema,\n          strict: true\n        }\n      } : { type: \"json_object\" } : void 0,\n      // search parameters\n      search_parameters: options.searchParameters ? {\n        mode: options.searchParameters.mode,\n        return_citations: options.searchParameters.returnCitations,\n        from_date: options.searchParameters.fromDate,\n        to_date: options.searchParameters.toDate,\n        max_search_results: options.searchParameters.maxSearchResults,\n        sources: (_c = options.searchParameters.sources) == null ? void 0 : _c.map((source) => ({\n          type: source.type,\n          ...source.type === \"web\" && {\n            country: source.country,\n            excluded_websites: source.excludedWebsites,\n            allowed_websites: source.allowedWebsites,\n            safe_search: source.safeSearch\n          },\n          ...source.type === \"x\" && {\n            x_handles: source.xHandles\n          },\n          ...source.type === \"news\" && {\n            country: source.country,\n            excluded_websites: source.excludedWebsites,\n            safe_search: source.safeSearch\n          },\n          ...source.type === \"rss\" && {\n            links: source.links\n          }\n        }))\n      } : void 0,\n      // messages in xai format\n      messages,\n      // tools in xai format\n      tools: xaiTools,\n      tool_choice: xaiToolChoice\n    };\n    return {\n      args: baseArgs,\n      warnings\n    };\n  }\n  async doGenerate(options) {\n    var _a16, _b, _c;\n    const { args: body, warnings } = await this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: `${(_a16 = this.config.baseURL) != null ? _a16 : \"https://api.x.ai/v1\"}/chat/completions`,\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: xaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler2(\n        xaiChatResponseSchema\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const choice = response.choices[0];\n    const content = [];\n    if (choice.message.content != null && choice.message.content.length > 0) {\n      let text = choice.message.content;\n      const lastMessage = body.messages[body.messages.length - 1];\n      if ((lastMessage == null ? void 0 : lastMessage.role) === \"assistant\" && text === lastMessage.content) {\n        text = \"\";\n      }\n      if (text.length > 0) {\n        content.push({ type: \"text\", text });\n      }\n    }\n    if (choice.message.reasoning_content != null && choice.message.reasoning_content.length > 0) {\n      content.push({\n        type: \"reasoning\",\n        text: choice.message.reasoning_content\n      });\n    }\n    if (choice.message.tool_calls != null) {\n      for (const toolCall of choice.message.tool_calls) {\n        content.push({\n          type: \"tool-call\",\n          toolCallId: toolCall.id,\n          toolName: toolCall.function.name,\n          input: toolCall.function.arguments\n        });\n      }\n    }\n    if (response.citations != null) {\n      for (const url of response.citations) {\n        content.push({\n          type: \"source\",\n          sourceType: \"url\",\n          id: this.config.generateId(),\n          url\n        });\n      }\n    }\n    return {\n      content,\n      finishReason: mapXaiFinishReason(choice.finish_reason),\n      usage: {\n        inputTokens: response.usage.prompt_tokens,\n        outputTokens: response.usage.completion_tokens,\n        totalTokens: response.usage.total_tokens,\n        reasoningTokens: (_c = (_b = response.usage.completion_tokens_details) == null ? void 0 : _b.reasoning_tokens) != null ? _c : void 0\n      },\n      request: { body },\n      response: {\n        ...getResponseMetadata6(response),\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      warnings\n    };\n  }\n  async doStream(options) {\n    var _a16;\n    const { args, warnings } = await this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      stream_options: {\n        include_usage: true\n      }\n    };\n    const { responseHeaders, value: response } = await postJsonToApi2({\n      url: `${(_a16 = this.config.baseURL) != null ? _a16 : \"https://api.x.ai/v1\"}/chat/completions`,\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: xaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler2(xaiChatChunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    let finishReason = \"unknown\";\n    const usage = {\n      inputTokens: void 0,\n      outputTokens: void 0,\n      totalTokens: void 0\n    };\n    let isFirstChunk = true;\n    const contentBlocks = {};\n    const lastReasoningDeltas = {};\n    const self = this;\n    return {\n      stream: response.pipeThrough(\n        new TransformStream({\n          start(controller) {\n            controller.enqueue({ type: \"stream-start\", warnings });\n          },\n          transform(chunk, controller) {\n            var _a23, _b;\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: \"raw\", rawValue: chunk.rawValue });\n            }\n            if (!chunk.success) {\n              controller.enqueue({ type: \"error\", error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n            if (isFirstChunk) {\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata6(value)\n              });\n              isFirstChunk = false;\n            }\n            if (value.citations != null) {\n              for (const url of value.citations) {\n                controller.enqueue({\n                  type: \"source\",\n                  sourceType: \"url\",\n                  id: self.config.generateId(),\n                  url\n                });\n              }\n            }\n            if (value.usage != null) {\n              usage.inputTokens = value.usage.prompt_tokens;\n              usage.outputTokens = value.usage.completion_tokens;\n              usage.totalTokens = value.usage.total_tokens;\n              usage.reasoningTokens = (_b = (_a23 = value.usage.completion_tokens_details) == null ? void 0 : _a23.reasoning_tokens) != null ? _b : void 0;\n            }\n            const choice = value.choices[0];\n            if ((choice == null ? void 0 : choice.finish_reason) != null) {\n              finishReason = mapXaiFinishReason(choice.finish_reason);\n            }\n            if ((choice == null ? void 0 : choice.delta) == null) {\n              return;\n            }\n            const delta = choice.delta;\n            const choiceIndex = choice.index;\n            if (delta.content != null && delta.content.length > 0) {\n              const textContent = delta.content;\n              const lastMessage = body.messages[body.messages.length - 1];\n              if ((lastMessage == null ? void 0 : lastMessage.role) === \"assistant\" && textContent === lastMessage.content) {\n                return;\n              }\n              const blockId = `text-${value.id || choiceIndex}`;\n              if (contentBlocks[blockId] == null) {\n                contentBlocks[blockId] = { type: \"text\" };\n                controller.enqueue({\n                  type: \"text-start\",\n                  id: blockId\n                });\n              }\n              controller.enqueue({\n                type: \"text-delta\",\n                id: blockId,\n                delta: textContent\n              });\n            }\n            if (delta.reasoning_content != null && delta.reasoning_content.length > 0) {\n              const blockId = `reasoning-${value.id || choiceIndex}`;\n              if (lastReasoningDeltas[blockId] === delta.reasoning_content) {\n                return;\n              }\n              lastReasoningDeltas[blockId] = delta.reasoning_content;\n              if (contentBlocks[blockId] == null) {\n                contentBlocks[blockId] = { type: \"reasoning\" };\n                controller.enqueue({\n                  type: \"reasoning-start\",\n                  id: blockId\n                });\n              }\n              controller.enqueue({\n                type: \"reasoning-delta\",\n                id: blockId,\n                delta: delta.reasoning_content\n              });\n            }\n            if (delta.tool_calls != null) {\n              for (const toolCall of delta.tool_calls) {\n                const toolCallId = toolCall.id;\n                controller.enqueue({\n                  type: \"tool-input-start\",\n                  id: toolCallId,\n                  toolName: toolCall.function.name\n                });\n                controller.enqueue({\n                  type: \"tool-input-delta\",\n                  id: toolCallId,\n                  delta: toolCall.function.arguments\n                });\n                controller.enqueue({\n                  type: \"tool-input-end\",\n                  id: toolCallId\n                });\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallId,\n                  toolName: toolCall.function.name,\n                  input: toolCall.function.arguments\n                });\n              }\n            }\n          },\n          flush(controller) {\n            for (const [blockId, block] of Object.entries(contentBlocks)) {\n              controller.enqueue({\n                type: block.type === \"text\" ? \"text-end\" : \"reasoning-end\",\n                id: blockId\n              });\n            }\n            controller.enqueue({ type: \"finish\", finishReason, usage });\n          }\n        })\n      ),\n      request: { body },\n      response: { headers: responseHeaders }\n    };\n  }\n};\nvar xaiUsageSchema = z$1.object({\n  prompt_tokens: z$1.number(),\n  completion_tokens: z$1.number(),\n  total_tokens: z$1.number(),\n  completion_tokens_details: z$1.object({\n    reasoning_tokens: z$1.number().nullish()\n  }).nullish()\n});\nvar xaiChatResponseSchema = z$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      message: z$1.object({\n        role: z$1.literal(\"assistant\"),\n        content: z$1.string().nullish(),\n        reasoning_content: z$1.string().nullish(),\n        tool_calls: z$1.array(\n          z$1.object({\n            id: z$1.string(),\n            type: z$1.literal(\"function\"),\n            function: z$1.object({\n              name: z$1.string(),\n              arguments: z$1.string()\n            })\n          })\n        ).nullish()\n      }),\n      index: z$1.number(),\n      finish_reason: z$1.string().nullish()\n    })\n  ),\n  object: z$1.literal(\"chat.completion\"),\n  usage: xaiUsageSchema,\n  citations: z$1.array(z$1.string().url()).nullish()\n});\nvar xaiChatChunkSchema = z$1.object({\n  id: z$1.string().nullish(),\n  created: z$1.number().nullish(),\n  model: z$1.string().nullish(),\n  choices: z$1.array(\n    z$1.object({\n      delta: z$1.object({\n        role: z$1.enum([\"assistant\"]).optional(),\n        content: z$1.string().nullish(),\n        reasoning_content: z$1.string().nullish(),\n        tool_calls: z$1.array(\n          z$1.object({\n            id: z$1.string(),\n            type: z$1.literal(\"function\"),\n            function: z$1.object({\n              name: z$1.string(),\n              arguments: z$1.string()\n            })\n          })\n        ).nullish()\n      }),\n      finish_reason: z$1.string().nullish(),\n      index: z$1.number()\n    })\n  ),\n  usage: xaiUsageSchema.nullish(),\n  citations: z$1.array(z$1.string().url()).nullish()\n});\nvar xaiErrorStructure2 = {\n  errorSchema: xaiErrorDataSchema,\n  errorToMessage: (data) => data.error.message\n};\nfunction createXai2(options = {}) {\n  var _a16;\n  const baseURL = withoutTrailingSlash2(\n    (_a16 = options.baseURL) != null ? _a16 : \"https://api.x.ai/v1\"\n  );\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey2({\n      apiKey: options.apiKey,\n      environmentVariableName: \"XAI_API_KEY\",\n      description: \"xAI API key\"\n    })}`,\n    ...options.headers\n  });\n  const createLanguageModel = (modelId) => {\n    return new XaiChatLanguageModel(modelId, {\n      provider: \"xai.chat\",\n      baseURL,\n      headers: getHeaders,\n      generateId: generateId2,\n      fetch: options.fetch\n    });\n  };\n  const createImageModel = (modelId) => {\n    return new OpenAICompatibleImageModel2(modelId, {\n      provider: \"xai.image\",\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      errorStructure: xaiErrorStructure2\n    });\n  };\n  const provider = (modelId) => createLanguageModel(modelId);\n  provider.languageModel = createLanguageModel;\n  provider.chat = createLanguageModel;\n  provider.textEmbeddingModel = (modelId) => {\n    throw new NoSuchModelError2({ modelId, modelType: \"textEmbeddingModel\" });\n  };\n  provider.imageModel = createImageModel;\n  provider.image = createImageModel;\n  return provider;\n}\nvar xai2 = createXai2();\nasync function getSerializedAgentTools(tools) {\n  return Object.entries(tools || {}).reduce((acc, [key, tool2]) => {\n    const _tool = tool2;\n    const toolId = _tool.id ?? `tool-${key}`;\n    let inputSchemaForReturn = void 0;\n    if (_tool.inputSchema) {\n      if (_tool.inputSchema?.jsonSchema) {\n        inputSchemaForReturn = stringify(_tool.inputSchema.jsonSchema);\n      } else {\n        inputSchemaForReturn = stringify(zodToJsonSchema(_tool.inputSchema));\n      }\n    }\n    let outputSchemaForReturn = void 0;\n    if (_tool.outputSchema) {\n      if (_tool.outputSchema?.jsonSchema) {\n        outputSchemaForReturn = stringify(_tool.outputSchema.jsonSchema);\n      } else {\n        outputSchemaForReturn = stringify(zodToJsonSchema(_tool.outputSchema));\n      }\n    }\n    acc[key] = {\n      ..._tool,\n      id: toolId,\n      inputSchema: inputSchemaForReturn,\n      outputSchema: outputSchemaForReturn\n    };\n    return acc;\n  }, {});\n}\nasync function getAgentsHandler({ mastra, runtimeContext }) {\n  try {\n    const agents = mastra.getAgents();\n    const serializedAgentsMap = await Promise.all(\n      Object.entries(agents).map(async ([id, agent]) => {\n        const instructions = await agent.getInstructions({ runtimeContext });\n        const tools = await agent.getTools({ runtimeContext });\n        const llm = await agent.getLLM({ runtimeContext });\n        const defaultGenerateOptions = await agent.getDefaultGenerateOptions({ runtimeContext });\n        const defaultStreamOptions = await agent.getDefaultStreamOptions({ runtimeContext });\n        const serializedAgentTools = await getSerializedAgentTools(tools);\n        let serializedAgentWorkflows = {};\n        if (\"getWorkflows\" in agent) {\n          const logger = mastra.getLogger();\n          try {\n            const workflows = await agent.getWorkflows({ runtimeContext });\n            serializedAgentWorkflows = Object.entries(workflows || {}).reduce((acc, [key, workflow]) => {\n              return {\n                ...acc,\n                [key]: {\n                  name: workflow.name\n                }\n              };\n            }, {});\n          } catch (error) {\n            logger.error(\"Error getting workflows for agent\", { agentName: agent.name, error });\n          }\n        }\n        const model = llm?.getModel();\n        return {\n          id,\n          name: agent.name,\n          instructions,\n          tools: serializedAgentTools,\n          workflows: serializedAgentWorkflows,\n          provider: llm?.getProvider(),\n          modelId: llm?.getModelId(),\n          modelVersion: model?.specificationVersion,\n          defaultGenerateOptions,\n          defaultStreamOptions\n        };\n      })\n    );\n    const serializedAgents = serializedAgentsMap.reduce((acc, { id, ...rest }) => {\n      acc[id] = rest;\n      return acc;\n    }, {});\n    return serializedAgents;\n  } catch (error) {\n    return handleError(error, \"Error getting agents\");\n  }\n}\nasync function getAgentByIdHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  isPlayground = false\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const tools = await agent.getTools({ runtimeContext });\n    const serializedAgentTools = await getSerializedAgentTools(tools);\n    let serializedAgentWorkflows = {};\n    if (\"getWorkflows\" in agent) {\n      const logger = mastra.getLogger();\n      try {\n        const workflows = await agent.getWorkflows({ runtimeContext });\n        serializedAgentWorkflows = Object.entries(workflows || {}).reduce((acc, [key, workflow]) => {\n          return {\n            ...acc,\n            [key]: {\n              name: workflow.name,\n              steps: Object.entries(workflow.steps).reduce((acc2, [key2, step]) => {\n                return {\n                  ...acc2,\n                  [key2]: {\n                    id: step.id,\n                    description: step.description\n                  }\n                };\n              }, {})\n            }\n          };\n        }, {});\n      } catch (error) {\n        logger.error(\"Error getting workflows for agent\", { agentName: agent.name, error });\n      }\n    }\n    let proxyRuntimeContext = runtimeContext;\n    if (isPlayground) {\n      proxyRuntimeContext = new Proxy(runtimeContext, {\n        get(target, prop) {\n          if (prop === \"get\") {\n            return function(key) {\n              const value = target.get(key);\n              return value ?? `<${key}>`;\n            };\n          }\n          return Reflect.get(target, prop);\n        }\n      });\n    }\n    const instructions = await agent.getInstructions({ runtimeContext: proxyRuntimeContext });\n    const llm = await agent.getLLM({ runtimeContext });\n    const defaultGenerateOptions = await agent.getDefaultGenerateOptions({ runtimeContext: proxyRuntimeContext });\n    const defaultStreamOptions = await agent.getDefaultStreamOptions({ runtimeContext: proxyRuntimeContext });\n    const model = llm?.getModel();\n    return {\n      name: agent.name,\n      instructions,\n      tools: serializedAgentTools,\n      workflows: serializedAgentWorkflows,\n      provider: llm?.getProvider(),\n      modelId: llm?.getModelId(),\n      modelVersion: model?.specificationVersion,\n      defaultGenerateOptions,\n      defaultStreamOptions\n    };\n  } catch (error) {\n    return handleError(error, \"Error getting agent\");\n  }\n}\nasync function getEvalsByAgentIdHandler({\n  mastra,\n  runtimeContext,\n  agentId\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    const evals = await mastra.getStorage()?.getEvalsByAgentName?.(agent.name, \"test\") || [];\n    const instructions = await agent.getInstructions({ runtimeContext });\n    return {\n      id: agentId,\n      name: agent.name,\n      instructions,\n      evals\n    };\n  } catch (error) {\n    return handleError(error, \"Error getting test evals\");\n  }\n}\nasync function getLiveEvalsByAgentIdHandler({\n  mastra,\n  runtimeContext,\n  agentId\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    const evals = await mastra.getStorage()?.getEvalsByAgentName?.(agent.name, \"live\") || [];\n    const instructions = await agent.getInstructions({ runtimeContext });\n    return {\n      id: agentId,\n      name: agent.name,\n      instructions,\n      evals\n    };\n  } catch (error) {\n    return handleError(error, \"Error getting live evals\");\n  }\n}\nfunction generateHandler({\n  mastra,\n  ...args\n}) {\n  const logger = mastra.getLogger();\n  logger?.warn(\n    \"Deprecation NOTICE:\\nGenerate method will switch to use generateVNext implementation September 16th. Please use generateLegacyHandler if you don't want to upgrade just yet.\"\n  );\n  return generateLegacyHandler({ mastra, ...args });\n}\nasync function generateLegacyHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  body,\n  abortSignal\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const { messages, resourceId, resourceid, runtimeContext: agentRuntimeContext, ...rest } = body;\n    const finalResourceId = resourceId ?? resourceid;\n    const finalRuntimeContext = new RuntimeContext([\n      ...Array.from(runtimeContext.entries()),\n      ...Array.from(Object.entries(agentRuntimeContext ?? {}))\n    ]);\n    validateBody({ messages });\n    const result = await agent.generate(messages, {\n      ...rest,\n      abortSignal,\n      // @ts-expect-error TODO fix types\n      resourceId: finalResourceId,\n      runtimeContext: finalRuntimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error generating from agent\");\n  }\n}\nasync function generateVNextHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  body,\n  abortSignal\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const { messages, runtimeContext: agentRuntimeContext, ...rest } = body;\n    const finalRuntimeContext = new RuntimeContext([\n      ...Array.from(runtimeContext.entries()),\n      ...Array.from(Object.entries(agentRuntimeContext ?? {}))\n    ]);\n    validateBody({ messages });\n    const result = await agent.generateVNext(messages, {\n      ...rest,\n      runtimeContext: finalRuntimeContext,\n      format: rest.format || \"mastra\",\n      options: {\n        ...rest?.options ?? {},\n        abortSignal\n      }\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error generating from agent\");\n  }\n}\nasync function streamGenerateHandler({\n  mastra,\n  ...args\n}) {\n  const logger = mastra.getLogger();\n  logger?.warn(\n    \"Deprecation NOTICE:\\n Stream method will switch to use streamVNext implementation September 16th. Please use streamGenerateLegacyHandler if you don't want to upgrade just yet.\"\n  );\n  return streamGenerateLegacyHandler({ mastra, ...args });\n}\nasync function streamGenerateLegacyHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  body,\n  abortSignal\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const { messages, resourceId, resourceid, runtimeContext: agentRuntimeContext, ...rest } = body;\n    const finalResourceId = resourceId ?? resourceid;\n    const finalRuntimeContext = new RuntimeContext([\n      ...Array.from(runtimeContext.entries()),\n      ...Array.from(Object.entries(agentRuntimeContext ?? {}))\n    ]);\n    validateBody({ messages });\n    const streamResult = await agent.stream(messages, {\n      ...rest,\n      abortSignal,\n      // @ts-expect-error TODO fix types\n      resourceId: finalResourceId,\n      runtimeContext: finalRuntimeContext\n    });\n    const streamResponse = rest.output ? streamResult.toTextStreamResponse({\n      headers: {\n        \"Transfer-Encoding\": \"chunked\"\n      }\n    }) : streamResult.toDataStreamResponse({\n      sendUsage: true,\n      sendReasoning: true,\n      getErrorMessage: (error) => {\n        return `An error occurred while processing your request. ${error instanceof Error ? error.message : JSON.stringify(error)}`;\n      },\n      headers: {\n        \"Transfer-Encoding\": \"chunked\"\n      }\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"error streaming agent response\");\n  }\n}\nfunction streamVNextGenerateHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  body,\n  abortSignal\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const { messages, runtimeContext: agentRuntimeContext, ...rest } = body;\n    const finalRuntimeContext = new RuntimeContext([\n      ...Array.from(runtimeContext.entries()),\n      ...Array.from(Object.entries(agentRuntimeContext ?? {}))\n    ]);\n    validateBody({ messages });\n    const streamResult = agent.streamVNext(messages, {\n      ...rest,\n      runtimeContext: finalRuntimeContext,\n      options: {\n        ...rest?.options ?? {},\n        abortSignal\n      },\n      format: body.format ?? \"mastra\"\n    });\n    return streamResult;\n  } catch (error) {\n    return handleError(error, \"error streaming agent response\");\n  }\n}\nasync function streamVNextUIMessageHandler({\n  mastra,\n  runtimeContext,\n  agentId,\n  body,\n  abortSignal\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const { messages, runtimeContext: agentRuntimeContext, ...rest } = body;\n    const finalRuntimeContext = new RuntimeContext([\n      ...Array.from(runtimeContext.entries()),\n      ...Array.from(Object.entries(agentRuntimeContext ?? {}))\n    ]);\n    validateBody({ messages });\n    const streamResult = await agent.streamVNext(messages, {\n      ...rest,\n      runtimeContext: finalRuntimeContext,\n      options: {\n        ...rest?.options ?? {},\n        abortSignal\n      },\n      format: \"aisdk\"\n    });\n    return streamResult.toUIMessageStreamResponse();\n  } catch (error) {\n    return handleError(error, \"error streaming agent response\");\n  }\n}\nasync function updateAgentModelHandler({\n  mastra,\n  agentId,\n  body\n}) {\n  try {\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const agentModel = await agent.getModel();\n    const modelVersion = agentModel.specificationVersion;\n    const { modelId, provider } = body;\n    const providerMap = {\n      v1: {\n        openai: openai(modelId),\n        anthropic: anthropic(modelId),\n        groq: groq(modelId),\n        xai: xai(modelId),\n        google: google(modelId)\n      },\n      v2: {\n        openai: openai2(modelId),\n        anthropic: anthropic2(modelId),\n        groq: groq2(modelId),\n        xai: xai2(modelId),\n        google: google2(modelId)\n      }\n    };\n    const modelVersionKey = modelVersion === \"v2\" ? \"v2\" : \"v1\";\n    let model = providerMap[modelVersionKey][provider];\n    agent.__updateModel({ model });\n    return { message: \"Agent model updated\" };\n  } catch (error) {\n    return handleError(error, \"error updating agent model\");\n  }\n}\n\nexport { agents_exports, generateHandler, generateLegacyHandler, generateVNextHandler, getAgentByIdHandler, getAgentsHandler, getEvalsByAgentIdHandler, getLiveEvalsByAgentIdHandler, getSerializedAgentTools, streamGenerateHandler, streamGenerateLegacyHandler, streamVNextGenerateHandler, streamVNextUIMessageHandler, updateAgentModelHandler };\n//# sourceMappingURL=chunk-GGCXLQ4J.js.map\n//# sourceMappingURL=chunk-GGCXLQ4J.js.map","// src/middleware/body-limit/index.ts\nimport { HTTPException } from \"../../http-exception.js\";\nvar ERROR_MESSAGE = \"Payload Too Large\";\nvar BodyLimitError = class extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"BodyLimitError\";\n  }\n};\nvar bodyLimit = (options) => {\n  const onError = options.onError || (() => {\n    const res = new Response(ERROR_MESSAGE, {\n      status: 413\n    });\n    throw new HTTPException(413, { res });\n  });\n  const maxSize = options.maxSize;\n  return async function bodyLimit2(c, next) {\n    if (!c.req.raw.body) {\n      return next();\n    }\n    const hasTransferEncoding = c.req.raw.headers.has(\"transfer-encoding\");\n    const hasContentLength = c.req.raw.headers.has(\"content-length\");\n    if (hasTransferEncoding && hasContentLength) {\n    }\n    if (hasContentLength && !hasTransferEncoding) {\n      const contentLength = parseInt(c.req.raw.headers.get(\"content-length\") || \"0\", 10);\n      return contentLength > maxSize ? onError(c) : next();\n    }\n    let size = 0;\n    const rawReader = c.req.raw.body.getReader();\n    const reader = new ReadableStream({\n      async start(controller) {\n        try {\n          for (; ; ) {\n            const { done, value } = await rawReader.read();\n            if (done) {\n              break;\n            }\n            size += value.length;\n            if (size > maxSize) {\n              controller.error(new BodyLimitError(ERROR_MESSAGE));\n              break;\n            }\n            controller.enqueue(value);\n          }\n        } finally {\n          controller.close();\n        }\n      }\n    });\n    const requestInit = { body: reader, duplex: \"half\" };\n    c.req.raw = new Request(c.req.raw, requestInit);\n    await next();\n    if (c.error instanceof BodyLimitError) {\n      c.res = await onError(c);\n    }\n  };\n};\nexport {\n  bodyLimit\n};\n","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { stringify } from './chunk-LF2ZLOFP.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\nimport { isVercelTool } from '@mastra/core/tools';\nimport { zodToJsonSchema } from '@mastra/core/utils/zod-to-json';\n\n// src/server/handlers/tools.ts\nvar tools_exports = {};\n__export(tools_exports, {\n  executeAgentToolHandler: () => executeAgentToolHandler,\n  executeToolHandler: () => executeToolHandler,\n  getAgentToolHandler: () => getAgentToolHandler,\n  getToolByIdHandler: () => getToolByIdHandler,\n  getToolsHandler: () => getToolsHandler\n});\nasync function getToolsHandler({ tools }) {\n  try {\n    if (!tools) {\n      return {};\n    }\n    const serializedTools = Object.entries(tools).reduce(\n      (acc, [id, _tool]) => {\n        const tool = _tool;\n        acc[id] = {\n          ...tool,\n          inputSchema: tool.inputSchema ? stringify(zodToJsonSchema(tool.inputSchema)) : void 0,\n          outputSchema: tool.outputSchema ? stringify(zodToJsonSchema(tool.outputSchema)) : void 0\n        };\n        return acc;\n      },\n      {}\n    );\n    return serializedTools;\n  } catch (error) {\n    return handleError(error, \"Error getting tools\");\n  }\n}\nasync function getToolByIdHandler({ tools, toolId }) {\n  try {\n    const tool = Object.values(tools || {}).find((tool2) => tool2.id === toolId);\n    if (!tool) {\n      throw new HTTPException(404, { message: \"Tool not found\" });\n    }\n    const serializedTool = {\n      ...tool,\n      inputSchema: tool.inputSchema ? stringify(zodToJsonSchema(tool.inputSchema)) : void 0,\n      outputSchema: tool.outputSchema ? stringify(zodToJsonSchema(tool.outputSchema)) : void 0\n    };\n    return serializedTool;\n  } catch (error) {\n    return handleError(error, \"Error getting tool\");\n  }\n}\nfunction executeToolHandler(tools) {\n  return async ({\n    mastra,\n    runId,\n    toolId,\n    data,\n    runtimeContext\n  }) => {\n    try {\n      if (!toolId) {\n        throw new HTTPException(400, { message: \"Tool ID is required\" });\n      }\n      const tool = Object.values(tools || {}).find((tool2) => tool2.id === toolId);\n      if (!tool) {\n        throw new HTTPException(404, { message: \"Tool not found\" });\n      }\n      if (!tool?.execute) {\n        throw new HTTPException(400, { message: \"Tool is not executable\" });\n      }\n      validateBody({ data });\n      if (isVercelTool(tool)) {\n        const result2 = await tool.execute(data);\n        return result2;\n      }\n      const result = await tool.execute({\n        context: data,\n        mastra,\n        runId,\n        runtimeContext,\n        // TODO: Pass proper tracing context when server API supports tracing\n        tracingContext: { currentSpan: void 0 }\n      });\n      return result;\n    } catch (error) {\n      return handleError(error, \"Error executing tool\");\n    }\n  };\n}\nasync function getAgentToolHandler({\n  mastra,\n  agentId,\n  toolId,\n  runtimeContext\n}) {\n  try {\n    const agent = agentId ? mastra.getAgent(agentId) : null;\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const agentTools = await agent.getTools({ runtimeContext });\n    const tool = Object.values(agentTools || {}).find((tool2) => tool2.id === toolId);\n    if (!tool) {\n      throw new HTTPException(404, { message: \"Tool not found\" });\n    }\n    const serializedTool = {\n      ...tool,\n      inputSchema: tool.inputSchema ? stringify(zodToJsonSchema(tool.inputSchema)) : void 0,\n      outputSchema: tool.outputSchema ? stringify(zodToJsonSchema(tool.outputSchema)) : void 0\n    };\n    return serializedTool;\n  } catch (error) {\n    return handleError(error, \"Error getting agent tool\");\n  }\n}\nasync function executeAgentToolHandler({\n  mastra,\n  agentId,\n  toolId,\n  data,\n  runtimeContext\n}) {\n  try {\n    const agent = agentId ? mastra.getAgent(agentId) : null;\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Tool not found\" });\n    }\n    const agentTools = await agent.getTools({ runtimeContext });\n    const tool = Object.values(agentTools || {}).find((tool2) => tool2.id === toolId);\n    if (!tool) {\n      throw new HTTPException(404, { message: \"Tool not found\" });\n    }\n    if (!tool?.execute) {\n      throw new HTTPException(400, { message: \"Tool is not executable\" });\n    }\n    const result = await tool.execute({\n      context: data,\n      runtimeContext,\n      mastra,\n      runId: agentId,\n      // TODO: Pass proper tracing context when server API supports tracing\n      tracingContext: { currentSpan: void 0 }\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error executing tool\");\n  }\n}\n\nexport { executeAgentToolHandler, executeToolHandler, getAgentToolHandler, getToolByIdHandler, getToolsHandler, tools_exports };\n//# sourceMappingURL=chunk-A3ZHTDWB.js.map\n//# sourceMappingURL=chunk-A3ZHTDWB.js.map","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\nimport { Readable } from 'stream';\n\n// src/server/handlers/voice.ts\nvar voice_exports = {};\n__export(voice_exports, {\n  generateSpeechHandler: () => generateSpeechHandler,\n  getListenerHandler: () => getListenerHandler,\n  getSpeakersHandler: () => getSpeakersHandler,\n  transcribeSpeechHandler: () => transcribeSpeechHandler\n});\nasync function getSpeakersHandler({ mastra, agentId }) {\n  try {\n    if (!agentId) {\n      throw new HTTPException(400, { message: \"Agent ID is required\" });\n    }\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const voice = await agent.getVoice();\n    if (!voice) {\n      throw new HTTPException(400, { message: \"Agent does not have voice capabilities\" });\n    }\n    const speakers = await voice.getSpeakers();\n    return speakers;\n  } catch (error) {\n    return handleError(error, \"Error getting speakers\");\n  }\n}\nasync function generateSpeechHandler({\n  mastra,\n  agentId,\n  body\n}) {\n  try {\n    if (!agentId) {\n      throw new HTTPException(400, { message: \"Agent ID is required\" });\n    }\n    validateBody({\n      text: body?.text\n    });\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const voice = await agent.getVoice();\n    if (!voice) {\n      throw new HTTPException(400, { message: \"Agent does not have voice capabilities\" });\n    }\n    const audioStream = await voice.speak(body.text, { speaker: body.speakerId });\n    if (!audioStream) {\n      throw new HTTPException(500, { message: \"Failed to generate speech\" });\n    }\n    return audioStream;\n  } catch (error) {\n    return handleError(error, \"Error generating speech\");\n  }\n}\nasync function transcribeSpeechHandler({\n  mastra,\n  agentId,\n  body\n}) {\n  try {\n    if (!agentId) {\n      throw new HTTPException(400, { message: \"Agent ID is required\" });\n    }\n    if (!body?.audioData) {\n      throw new HTTPException(400, { message: \"Audio data is required\" });\n    }\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const voice = await agent.getVoice();\n    if (!voice) {\n      throw new HTTPException(400, { message: \"Agent does not have voice capabilities\" });\n    }\n    const audioStream = new Readable();\n    audioStream.push(body.audioData);\n    audioStream.push(null);\n    const text = await voice.listen(audioStream, body.options);\n    return { text };\n  } catch (error) {\n    return handleError(error, \"Error transcribing speech\");\n  }\n}\nasync function getListenerHandler({ mastra, agentId }) {\n  try {\n    if (!agentId) {\n      throw new HTTPException(400, { message: \"Agent ID is required\" });\n    }\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      throw new HTTPException(404, { message: \"Agent not found\" });\n    }\n    const voice = await agent.getVoice();\n    if (!voice) {\n      throw new HTTPException(400, { message: \"Agent does not have voice capabilities\" });\n    }\n    const listeners = await voice.getListener();\n    return listeners;\n  } catch (error) {\n    return handleError(error, \"Error getting listeners\");\n  }\n}\n\nexport { generateSpeechHandler, getListenerHandler, getSpeakersHandler, transcribeSpeechHandler, voice_exports };\n//# sourceMappingURL=chunk-QBWF6U7Z.js.map\n//# sourceMappingURL=chunk-QBWF6U7Z.js.map","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/logs.ts\nvar logs_exports = {};\n__export(logs_exports, {\n  getLogTransports: () => getLogTransports,\n  getLogsByRunIdHandler: () => getLogsByRunIdHandler,\n  getLogsHandler: () => getLogsHandler\n});\nasync function getLogsHandler({\n  mastra,\n  transportId,\n  params\n}) {\n  try {\n    validateBody({ transportId });\n    const { fromDate, toDate, logLevel, filters: _filters, page, perPage } = params || {};\n    const filters = _filters ? Object.fromEntries(\n      (Array.isArray(_filters) ? _filters : [_filters]).map((attr) => {\n        const [key, value] = attr.split(\":\");\n        return [key, value];\n      })\n    ) : void 0;\n    const logs = await mastra.getLogs(transportId, {\n      fromDate,\n      toDate,\n      logLevel,\n      filters,\n      page: page ? Number(page) : void 0,\n      perPage: perPage ? Number(perPage) : void 0\n    });\n    return logs;\n  } catch (error) {\n    return handleError(error, \"Error getting logs\");\n  }\n}\nasync function getLogsByRunIdHandler({\n  mastra,\n  runId,\n  transportId,\n  params\n}) {\n  try {\n    validateBody({ runId, transportId });\n    const { fromDate, toDate, logLevel, filters: _filters, page, perPage } = params || {};\n    const filters = _filters ? Object.fromEntries(\n      (Array.isArray(_filters) ? _filters : [_filters]).map((attr) => {\n        const [key, value] = attr.split(\":\");\n        return [key, value];\n      })\n    ) : void 0;\n    const logs = await mastra.getLogsByRunId({\n      runId,\n      transportId,\n      fromDate,\n      toDate,\n      logLevel,\n      filters,\n      page: page ? Number(page) : void 0,\n      perPage: perPage ? Number(perPage) : void 0\n    });\n    return logs;\n  } catch (error) {\n    return handleError(error, \"Error getting logs by run ID\");\n  }\n}\nasync function getLogTransports({ mastra }) {\n  try {\n    const logger = mastra.getLogger();\n    const transports = logger.getTransports();\n    return {\n      transports: transports ? [...transports.keys()] : []\n    };\n  } catch (error) {\n    return handleError(error, \"Error getting log Transports\");\n  }\n}\n\nexport { getLogTransports, getLogsByRunIdHandler, getLogsHandler, logs_exports };\n//# sourceMappingURL=chunk-ROA7BCHD.js.map\n//# sourceMappingURL=chunk-ROA7BCHD.js.map","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\nimport { generateEmptyFromSchema } from '@mastra/core/utils';\n\n// src/server/handlers/memory.ts\nvar memory_exports = {};\n__export(memory_exports, {\n  createThreadHandler: () => createThreadHandler,\n  deleteMessagesHandler: () => deleteMessagesHandler,\n  deleteThreadHandler: () => deleteThreadHandler,\n  getMemoryConfigHandler: () => getMemoryConfigHandler,\n  getMemoryStatusHandler: () => getMemoryStatusHandler,\n  getMessagesHandler: () => getMessagesHandler,\n  getMessagesPaginatedHandler: () => getMessagesPaginatedHandler,\n  getThreadByIdHandler: () => getThreadByIdHandler,\n  getThreadsHandler: () => getThreadsHandler,\n  getThreadsPaginatedHandler: () => getThreadsPaginatedHandler,\n  getWorkingMemoryHandler: () => getWorkingMemoryHandler,\n  saveMessagesHandler: () => saveMessagesHandler,\n  searchMemoryHandler: () => searchMemoryHandler,\n  updateThreadHandler: () => updateThreadHandler,\n  updateWorkingMemoryHandler: () => updateWorkingMemoryHandler\n});\nasync function getMemoryFromContext({\n  mastra,\n  agentId,\n  networkId,\n  runtimeContext\n}) {\n  const agent = agentId ? mastra.getAgent(agentId) : null;\n  if (agentId && !agent) {\n    throw new HTTPException(404, { message: \"Agent not found\" });\n  }\n  const network = networkId ? mastra.vnext_getNetwork(networkId) : null;\n  if (networkId && !network) {\n    throw new HTTPException(404, { message: \"Network not found\" });\n  }\n  if (agent) {\n    return await agent?.getMemory() || mastra.getMemory();\n  }\n  if (network) {\n    return await network?.getMemory({ runtimeContext }) || mastra.getMemory();\n  }\n  return mastra.getMemory();\n}\nasync function getMemoryStatusHandler({\n  mastra,\n  agentId,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      return { result: false };\n    }\n    return { result: true };\n  } catch (error) {\n    return handleError(error, \"Error getting memory status\");\n  }\n}\nasync function getMemoryConfigHandler({\n  mastra,\n  agentId,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const config = memory.getMergedThreadConfig({});\n    return { config };\n  } catch (error) {\n    return handleError(error, \"Error getting memory configuration\");\n  }\n}\nasync function getThreadsHandler({\n  mastra,\n  agentId,\n  resourceId,\n  networkId,\n  runtimeContext,\n  orderBy,\n  sortDirection\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    validateBody({ resourceId });\n    const threads = await memory.getThreadsByResourceId({\n      resourceId,\n      orderBy,\n      sortDirection\n    });\n    return threads;\n  } catch (error) {\n    return handleError(error, \"Error getting threads\");\n  }\n}\nasync function getThreadsPaginatedHandler({\n  mastra,\n  agentId,\n  resourceId,\n  networkId,\n  runtimeContext,\n  page,\n  perPage,\n  orderBy,\n  sortDirection\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    validateBody({ resourceId });\n    const result = await memory.getThreadsByResourceIdPaginated({\n      resourceId,\n      page,\n      perPage,\n      orderBy,\n      sortDirection\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error getting paginated threads\");\n  }\n}\nasync function getThreadByIdHandler({\n  mastra,\n  agentId,\n  threadId,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    validateBody({ threadId });\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    return thread;\n  } catch (error) {\n    return handleError(error, \"Error getting thread\");\n  }\n}\nasync function saveMessagesHandler({\n  mastra,\n  agentId,\n  body,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    if (!body?.messages) {\n      throw new HTTPException(400, { message: \"Messages are required\" });\n    }\n    if (!Array.isArray(body.messages)) {\n      throw new HTTPException(400, { message: \"Messages should be an array\" });\n    }\n    const invalidMessages = body.messages.filter((message) => !message.threadId || !message.resourceId);\n    if (invalidMessages.length > 0) {\n      throw new HTTPException(400, {\n        message: `All messages must have threadId and resourceId fields. Found ${invalidMessages.length} invalid message(s).`\n      });\n    }\n    const processedMessages = body.messages.map((message) => ({\n      ...message,\n      id: message.id || memory.generateId(),\n      createdAt: message.createdAt ? new Date(message.createdAt) : /* @__PURE__ */ new Date()\n    }));\n    const result = await memory.saveMessages({ messages: processedMessages, memoryConfig: {} });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error saving messages\");\n  }\n}\nasync function createThreadHandler({\n  mastra,\n  agentId,\n  body,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    validateBody({ resourceId: body?.resourceId });\n    const result = await memory.createThread({\n      resourceId: body?.resourceId,\n      title: body?.title,\n      metadata: body?.metadata,\n      threadId: body?.threadId\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error saving thread to memory\");\n  }\n}\nasync function updateThreadHandler({\n  mastra,\n  agentId,\n  threadId,\n  body,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!body) {\n      throw new HTTPException(400, { message: \"Body is required\" });\n    }\n    const { title, metadata, resourceId } = body;\n    const updatedAt = /* @__PURE__ */ new Date();\n    validateBody({ threadId });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    const updatedThread = {\n      ...thread,\n      title: title || thread.title,\n      metadata: metadata || thread.metadata,\n      resourceId: resourceId || thread.resourceId,\n      createdAt: thread.createdAt,\n      updatedAt\n    };\n    const result = await memory.saveThread({ thread: updatedThread });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error updating thread\");\n  }\n}\nasync function deleteThreadHandler({\n  mastra,\n  agentId,\n  threadId,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    validateBody({ threadId });\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    await memory.deleteThread(threadId);\n    return { result: \"Thread deleted\" };\n  } catch (error) {\n    return handleError(error, \"Error deleting thread\");\n  }\n}\nasync function getMessagesPaginatedHandler({\n  mastra,\n  threadId,\n  resourceId,\n  selectBy,\n  format\n}) {\n  try {\n    validateBody({ threadId });\n    const storage = mastra.getStorage();\n    if (!storage) {\n      throw new HTTPException(400, { message: \"Storage is not initialized\" });\n    }\n    const thread = await storage.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    const result = await storage.getMessagesPaginated({ threadId, resourceId, selectBy, format });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error getting messages\");\n  }\n}\nasync function getMessagesHandler({\n  mastra,\n  agentId,\n  threadId,\n  limit,\n  networkId,\n  runtimeContext\n}) {\n  if (limit !== void 0 && (!Number.isInteger(limit) || limit <= 0)) {\n    throw new HTTPException(400, { message: \"Invalid limit: must be a positive integer\" });\n  }\n  try {\n    validateBody({ threadId });\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    const result = await memory.query({\n      threadId,\n      ...limit && { selectBy: { last: limit } }\n    });\n    return { messages: result.messages, uiMessages: result.uiMessages };\n  } catch (error) {\n    return handleError(error, \"Error getting messages\");\n  }\n}\nasync function getWorkingMemoryHandler({\n  mastra,\n  agentId,\n  threadId,\n  resourceId,\n  networkId,\n  runtimeContext,\n  memoryConfig\n}) {\n  try {\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    validateBody({ threadId });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    const threadExists = !!thread;\n    const template = await memory.getWorkingMemoryTemplate({ memoryConfig });\n    const workingMemoryTemplate = template?.format === \"json\" ? { ...template, content: JSON.stringify(generateEmptyFromSchema(template.content)) } : template;\n    const workingMemory = await memory.getWorkingMemory({ threadId, resourceId, memoryConfig });\n    const config = memory.getMergedThreadConfig(memoryConfig || {});\n    const source = config.workingMemory?.scope === \"resource\" && resourceId ? \"resource\" : \"thread\";\n    return { workingMemory, source, workingMemoryTemplate, threadExists };\n  } catch (error) {\n    return handleError(error, \"Error getting working memory\");\n  }\n}\nasync function updateWorkingMemoryHandler({\n  mastra,\n  agentId,\n  threadId,\n  body,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    validateBody({ threadId });\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    const { resourceId, memoryConfig, workingMemory } = body;\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const thread = await memory.getThreadById({ threadId });\n    if (!thread) {\n      throw new HTTPException(404, { message: \"Thread not found\" });\n    }\n    await memory.updateWorkingMemory({ threadId, resourceId, workingMemory, memoryConfig });\n    return { success: true };\n  } catch (error) {\n    return handleError(error, \"Error updating working memory\");\n  }\n}\nasync function deleteMessagesHandler({\n  mastra,\n  agentId,\n  messageIds,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    if (messageIds === void 0 || messageIds === null) {\n      throw new HTTPException(400, { message: \"messageIds is required\" });\n    }\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    await memory.deleteMessages(messageIds);\n    let count = 1;\n    if (Array.isArray(messageIds)) {\n      count = messageIds.length;\n    }\n    return { success: true, message: `${count} message${count === 1 ? \"\" : \"s\"} deleted successfully` };\n  } catch (error) {\n    return handleError(error, \"Error deleting messages\");\n  }\n}\nasync function searchMemoryHandler({\n  mastra,\n  agentId,\n  searchQuery,\n  resourceId,\n  threadId,\n  limit = 20,\n  networkId,\n  runtimeContext,\n  memoryConfig\n}) {\n  try {\n    validateBody({ searchQuery, resourceId });\n    const memory = await getMemoryFromContext({ mastra, agentId, networkId, runtimeContext });\n    if (!memory) {\n      throw new HTTPException(400, { message: \"Memory is not initialized\" });\n    }\n    const config = memory.getMergedThreadConfig(memoryConfig || {});\n    const hasSemanticRecall = !!config?.semanticRecall;\n    const resourceScope = typeof config?.semanticRecall === \"object\" && config?.semanticRecall?.scope === \"resource\";\n    if (threadId && !resourceScope) {\n      const thread = await memory.getThreadById({ threadId });\n      if (!thread) {\n        throw new HTTPException(404, { message: \"Thread not found\" });\n      }\n      if (thread.resourceId !== resourceId) {\n        throw new HTTPException(403, { message: \"Thread does not belong to the specified resource\" });\n      }\n    }\n    const searchResults = [];\n    const messageMap = /* @__PURE__ */ new Map();\n    if (threadId && !resourceScope) {\n      const thread = await memory.getThreadById({ threadId });\n      if (!thread) {\n        return {\n          results: [],\n          count: 0,\n          query: searchQuery,\n          searchScope: \"thread\",\n          searchType: hasSemanticRecall ? \"semantic\" : \"text\"\n        };\n      }\n    }\n    if (!threadId || resourceScope) {\n      const threads = await memory.getThreadsByResourceId({ resourceId });\n      if (threads.length === 0) {\n        return {\n          results: [],\n          count: 0,\n          query: searchQuery,\n          searchScope: \"resource\",\n          searchType: hasSemanticRecall ? \"semantic\" : \"text\"\n        };\n      }\n      for (const thread of threads) {\n        const result = await memory.rememberMessages({\n          threadId: thread.id,\n          resourceId,\n          vectorMessageSearch: searchQuery,\n          config\n        });\n        const threadMessages = (await memory.query({ threadId: thread.id })).uiMessages;\n        result.messagesV2.forEach((msg) => {\n          if (messageMap.has(msg.id)) return;\n          messageMap.set(msg.id, true);\n          const content = msg.content.content || msg.content.parts?.map((p) => p.type === \"text\" ? p.text : \"\").join(\" \") || \"\";\n          if (!hasSemanticRecall && !content.toLowerCase().includes(searchQuery.toLowerCase())) {\n            return;\n          }\n          const messageIndex = threadMessages.findIndex((m) => m.id === msg.id);\n          const searchResult = {\n            id: msg.id,\n            role: msg.role,\n            content,\n            createdAt: msg.createdAt,\n            threadId: msg.threadId || thread.id,\n            threadTitle: thread.title || msg.threadId || thread.id\n          };\n          if (messageIndex !== -1) {\n            searchResult.context = {\n              before: threadMessages.slice(Math.max(0, messageIndex - 2), messageIndex).map((m) => ({\n                id: m.id,\n                role: m.role,\n                content: m.content,\n                createdAt: m.createdAt || /* @__PURE__ */ new Date()\n              })),\n              after: threadMessages.slice(messageIndex + 1, messageIndex + 3).map((m) => ({\n                id: m.id,\n                role: m.role,\n                content: m.content,\n                createdAt: m.createdAt || /* @__PURE__ */ new Date()\n              }))\n            };\n          }\n          searchResults.push(searchResult);\n        });\n      }\n    } else if (threadId) {\n      const thread = await memory.getThreadById({ threadId });\n      if (!thread) {\n        return {\n          results: [],\n          count: 0,\n          query: searchQuery,\n          searchScope: \"thread\",\n          searchType: hasSemanticRecall ? \"semantic\" : \"text\"\n        };\n      }\n      const result = await memory.rememberMessages({\n        threadId,\n        resourceId,\n        vectorMessageSearch: searchQuery,\n        config\n      });\n      const threadMessages = (await memory.query({ threadId })).uiMessages;\n      result.messagesV2.forEach((msg) => {\n        if (messageMap.has(msg.id)) return;\n        messageMap.set(msg.id, true);\n        const content = msg.content.content || msg.content.parts?.map((p) => p.type === \"text\" ? p.text : \"\").join(\" \") || \"\";\n        if (!hasSemanticRecall && !content.toLowerCase().includes(searchQuery.toLowerCase())) {\n          return;\n        }\n        const messageIndex = threadMessages.findIndex((m) => m.id === msg.id);\n        const searchResult = {\n          id: msg.id,\n          role: msg.role,\n          content,\n          createdAt: msg.createdAt,\n          threadId,\n          threadTitle: thread?.title || threadId\n        };\n        if (messageIndex !== -1) {\n          searchResult.context = {\n            before: threadMessages.slice(Math.max(0, messageIndex - 2), messageIndex).map((m) => ({\n              id: m.id,\n              role: m.role,\n              content: m.content,\n              createdAt: m.createdAt || /* @__PURE__ */ new Date()\n            })),\n            after: threadMessages.slice(messageIndex + 1, messageIndex + 3).map((m) => ({\n              id: m.id,\n              role: m.role,\n              content: m.content,\n              createdAt: m.createdAt || /* @__PURE__ */ new Date()\n            }))\n          };\n        }\n        searchResults.push(searchResult);\n      });\n    }\n    const sortedResults = searchResults.sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()).slice(0, limit);\n    return {\n      results: sortedResults,\n      count: sortedResults.length,\n      query: searchQuery,\n      searchScope: resourceScope ? \"resource\" : \"thread\",\n      searchType: hasSemanticRecall ? \"semantic\" : \"text\"\n    };\n  } catch (error) {\n    return handleError(error, \"Error searching memory\");\n  }\n}\n\nexport { createThreadHandler, deleteMessagesHandler, deleteThreadHandler, getMemoryConfigHandler, getMemoryStatusHandler, getMessagesHandler, getMessagesPaginatedHandler, getThreadByIdHandler, getThreadsHandler, getThreadsPaginatedHandler, getWorkingMemoryHandler, memory_exports, saveMessagesHandler, searchMemoryHandler, updateThreadHandler, updateWorkingMemoryHandler };\n//# sourceMappingURL=chunk-LUPY3MQY.js.map\n//# sourceMappingURL=chunk-LUPY3MQY.js.map","import { validateBody } from './chunk-OW4FX5TS.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/network.ts\nvar network_exports = {};\n__export(network_exports, {\n  generateHandler: () => generateHandler,\n  getNetworkByIdHandler: () => getNetworkByIdHandler,\n  getNetworksHandler: () => getNetworksHandler,\n  streamGenerateHandler: () => streamGenerateHandler\n});\nasync function getNetworksHandler({\n  mastra,\n  runtimeContext\n}) {\n  try {\n    const networks = mastra.getNetworks();\n    const serializedNetworks = await Promise.all(\n      networks.map(async (network) => {\n        const routingAgent = network.getRoutingAgent();\n        const routingLLM = await routingAgent.getLLM({ runtimeContext });\n        const agents = network.getAgents();\n        return {\n          id: network.formatAgentId(routingAgent.name),\n          name: routingAgent.name,\n          instructions: routingAgent.instructions,\n          agents: await Promise.all(\n            agents.map(async (agent) => {\n              const llm = await agent.getLLM({ runtimeContext });\n              return {\n                name: agent.name,\n                provider: llm?.getProvider(),\n                modelId: llm?.getModelId()\n              };\n            })\n          ),\n          routingModel: {\n            provider: routingLLM?.getProvider(),\n            modelId: routingLLM?.getModelId()\n          }\n        };\n      })\n    );\n    return serializedNetworks;\n  } catch (error) {\n    return handleError(error, \"Error getting networks\");\n  }\n}\nasync function getNetworkByIdHandler({\n  mastra,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const networks = mastra.getNetworks();\n    const network = networks.find((network2) => {\n      const routingAgent2 = network2.getRoutingAgent();\n      return network2.formatAgentId(routingAgent2.name) === networkId;\n    });\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    const routingAgent = network.getRoutingAgent();\n    const routingLLM = await routingAgent.getLLM({ runtimeContext });\n    const agents = network.getAgents();\n    const serializedNetwork = {\n      id: network.formatAgentId(routingAgent.name),\n      name: routingAgent.name,\n      instructions: routingAgent.instructions,\n      agents: await Promise.all(\n        agents.map(async (agent) => {\n          const llm = await agent.getLLM({ runtimeContext });\n          return {\n            name: agent.name,\n            provider: llm?.getProvider(),\n            modelId: llm?.getModelId()\n          };\n        })\n      ),\n      routingModel: {\n        provider: routingLLM?.getProvider(),\n        modelId: routingLLM?.getModelId()\n      }\n    };\n    return serializedNetwork;\n  } catch (error) {\n    return handleError(error, \"Error getting network by ID\");\n  }\n}\nasync function generateHandler({\n  mastra,\n  runtimeContext,\n  networkId,\n  body\n}) {\n  try {\n    const network = mastra.getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ messages: body.messages });\n    const { messages, ...rest } = body;\n    const result = await network.generate(messages, { ...rest, runtimeContext });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error generating from network\");\n  }\n}\nasync function streamGenerateHandler({\n  mastra,\n  networkId,\n  body,\n  runtimeContext\n}) {\n  try {\n    const network = mastra.getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ messages: body.messages });\n    const { messages, output, ...rest } = body;\n    const streamResult = await network.stream(messages, {\n      output,\n      ...rest,\n      runtimeContext\n    });\n    const streamResponse = output ? streamResult.toTextStreamResponse() : streamResult.toDataStreamResponse({\n      sendUsage: true,\n      sendReasoning: true,\n      getErrorMessage: (error) => {\n        return `An error occurred while processing your request. ${error instanceof Error ? error.message : JSON.stringify(error)}`;\n      }\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"Error streaming from network\");\n  }\n}\n\nexport { generateHandler, getNetworkByIdHandler, getNetworksHandler, network_exports, streamGenerateHandler };\n//# sourceMappingURL=chunk-B43YAQJR.js.map\n//# sourceMappingURL=chunk-B43YAQJR.js.map","import { validateBody } from '../../chunk-OW4FX5TS.js';\nimport { stringify } from '../../chunk-LF2ZLOFP.js';\nimport { handleError } from '../../chunk-CY4TP3FK.js';\nimport { HTTPException } from '../../chunk-MMROOK5J.js';\nimport { zodToJsonSchema } from '@mastra/core/utils/zod-to-json';\n\nasync function getVNextNetworksHandler({\n  mastra,\n  runtimeContext\n}) {\n  try {\n    const networks = mastra.vnext_getNetworks();\n    const serializedNetworks = await Promise.all(\n      networks.map(async (network) => {\n        const routingAgent = await network.getRoutingAgent({ runtimeContext });\n        const routingLLM = await routingAgent.getLLM({ runtimeContext });\n        const agents = await network.getAgents({ runtimeContext });\n        const workflows = await network.getWorkflows({ runtimeContext });\n        const tools = await network.getTools({ runtimeContext });\n        const networkInstruction = await network.getInstructions({ runtimeContext });\n        return {\n          id: network.id,\n          name: network.name,\n          instructions: networkInstruction,\n          tools: await Promise.all(\n            Object.values(tools).map(async (tool) => {\n              return {\n                id: tool.id,\n                description: tool.description\n              };\n            })\n          ),\n          agents: await Promise.all(\n            Object.values(agents).map(async (agent) => {\n              const llm = await agent.getLLM({ runtimeContext });\n              return {\n                name: agent.name,\n                provider: llm?.getProvider(),\n                modelId: llm?.getModelId()\n              };\n            })\n          ),\n          workflows: await Promise.all(\n            Object.values(workflows).map(async (workflow) => {\n              return {\n                name: workflow.name,\n                description: workflow.description,\n                inputSchema: workflow.inputSchema ? stringify(zodToJsonSchema(workflow.inputSchema)) : void 0,\n                outputSchema: workflow.outputSchema ? stringify(zodToJsonSchema(workflow.outputSchema)) : void 0\n              };\n            })\n          ),\n          routingModel: {\n            provider: routingLLM?.getProvider(),\n            modelId: routingLLM?.getModelId()\n          }\n        };\n      })\n    );\n    return serializedNetworks;\n  } catch (error) {\n    return handleError(error, \"Error getting networks\");\n  }\n}\nasync function getVNextNetworkByIdHandler({\n  mastra,\n  networkId,\n  runtimeContext\n}) {\n  try {\n    const network = mastra.vnext_getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    const routingAgent = await network.getRoutingAgent({ runtimeContext });\n    const routingLLM = await routingAgent.getLLM({ runtimeContext });\n    const agents = await network.getAgents({ runtimeContext });\n    const workflows = await network.getWorkflows({ runtimeContext });\n    const tools = await network.getTools({ runtimeContext });\n    const networkInstruction = await network.getInstructions({ runtimeContext });\n    const serializedNetwork = {\n      id: network.id,\n      name: network.name,\n      instructions: networkInstruction,\n      agents: await Promise.all(\n        Object.values(agents).map(async (agent) => {\n          const llm = await agent.getLLM({ runtimeContext });\n          return {\n            name: agent.name,\n            provider: llm?.getProvider(),\n            modelId: llm?.getModelId()\n          };\n        })\n      ),\n      workflows: await Promise.all(\n        Object.values(workflows).map(async (workflow) => {\n          return {\n            name: workflow.name,\n            description: workflow.description,\n            inputSchema: workflow.inputSchema ? stringify(zodToJsonSchema(workflow.inputSchema)) : void 0,\n            outputSchema: workflow.outputSchema ? stringify(zodToJsonSchema(workflow.outputSchema)) : void 0\n          };\n        })\n      ),\n      tools: await Promise.all(\n        Object.values(tools).map(async (tool) => {\n          return {\n            id: tool.id,\n            description: tool.description\n          };\n        })\n      ),\n      routingModel: {\n        provider: routingLLM?.getProvider(),\n        modelId: routingLLM?.getModelId()\n      }\n    };\n    return serializedNetwork;\n  } catch (error) {\n    return handleError(error, \"Error getting network by ID\");\n  }\n}\nasync function generateVNextNetworkHandler({\n  mastra,\n  runtimeContext,\n  networkId,\n  body\n}) {\n  try {\n    const network = mastra.vnext_getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ message: body.message });\n    const { message, threadId, resourceId } = body;\n    const result = await network.generate(message, { runtimeContext, threadId, resourceId });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error generating from network\");\n  }\n}\nasync function streamGenerateVNextNetworkHandler({\n  mastra,\n  networkId,\n  body,\n  runtimeContext\n}) {\n  try {\n    const network = mastra.vnext_getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ message: body.message });\n    const { message, threadId, resourceId } = body;\n    const streamResult = await network.stream(message, {\n      runtimeContext,\n      threadId,\n      resourceId\n    });\n    return streamResult;\n  } catch (error) {\n    return handleError(error, \"Error streaming from network\");\n  }\n}\nasync function loopVNextNetworkHandler({\n  mastra,\n  networkId,\n  body,\n  runtimeContext\n}) {\n  try {\n    const network = mastra.vnext_getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ message: body.message });\n    const { message } = body;\n    const result = await network.loop(message, {\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error looping network\");\n  }\n}\nasync function loopStreamVNextNetworkHandler({\n  mastra,\n  networkId,\n  body,\n  runtimeContext\n}) {\n  try {\n    const network = mastra.vnext_getNetwork(networkId);\n    if (!network) {\n      throw new HTTPException(404, { message: \"Network not found\" });\n    }\n    validateBody({ message: body.message });\n    const { message, threadId, resourceId, maxIterations } = body;\n    const result = await network.loopStream(message, {\n      runtimeContext,\n      threadId,\n      resourceId,\n      maxIterations\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error streaming network loop\");\n  }\n}\n\nexport { generateVNextNetworkHandler, getVNextNetworkByIdHandler, getVNextNetworksHandler, loopStreamVNextNetworkHandler, loopVNextNetworkHandler, streamGenerateVNextNetworkHandler };\n//# sourceMappingURL=vNextNetwork.js.map\n//# sourceMappingURL=vNextNetwork.js.map","import { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/observability.ts\nvar observability_exports = {};\n__export(observability_exports, {\n  getAITraceHandler: () => getAITraceHandler,\n  getAITracesPaginatedHandler: () => getAITracesPaginatedHandler\n});\nasync function getAITraceHandler({ mastra, traceId }) {\n  try {\n    if (!traceId) {\n      throw new HTTPException(400, { message: \"Trace ID is required\" });\n    }\n    const storage = mastra.getStorage();\n    if (!storage) {\n      throw new HTTPException(500, { message: \"Storage is not available\" });\n    }\n    const trace = await storage.getAITrace(traceId);\n    if (!trace) {\n      throw new HTTPException(404, { message: `Trace with ID '${traceId}' not found` });\n    }\n    return trace;\n  } catch (error) {\n    handleError(error, \"Error getting AI trace\");\n  }\n}\nasync function getAITracesPaginatedHandler({ mastra, body }) {\n  try {\n    const storage = mastra.getStorage();\n    if (!storage) {\n      throw new HTTPException(500, { message: \"Storage is not available\" });\n    }\n    if (!body) {\n      throw new HTTPException(400, { message: \"Request body is required\" });\n    }\n    const { filters, pagination } = body;\n    if (pagination?.page && pagination.page < 0) {\n      throw new HTTPException(400, { message: \"Page must be a non-negative integer\" });\n    }\n    if (pagination?.perPage && pagination.perPage < 0) {\n      throw new HTTPException(400, { message: \"Per page must be a non-negative integer\" });\n    }\n    if (pagination?.dateRange) {\n      const { start, end } = pagination.dateRange;\n      if (start && !(start instanceof Date)) {\n        throw new HTTPException(400, { message: \"Invalid date format in date range\" });\n      }\n      if (end && !(end instanceof Date)) {\n        throw new HTTPException(400, { message: \"Invalid date format in date range\" });\n      }\n    }\n    return storage.getAITracesPaginated({\n      pagination,\n      filters\n    });\n  } catch (error) {\n    handleError(error, \"Error getting AI traces paginated\");\n  }\n}\n\nexport { getAITraceHandler, getAITracesPaginatedHandler, observability_exports };\n//# sourceMappingURL=chunk-R7NOGUZG.js.map\n//# sourceMappingURL=chunk-R7NOGUZG.js.map","import { handleError } from './chunk-CY4TP3FK.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/scores.ts\nvar scores_exports = {};\n__export(scores_exports, {\n  getScorerHandler: () => getScorerHandler,\n  getScorersHandler: () => getScorersHandler,\n  getScoresByEntityIdHandler: () => getScoresByEntityIdHandler,\n  getScoresByRunIdHandler: () => getScoresByRunIdHandler,\n  getScoresByScorerIdHandler: () => getScoresByScorerIdHandler,\n  saveScoreHandler: () => saveScoreHandler\n});\nasync function getScorersFromSystem({\n  mastra,\n  runtimeContext\n}) {\n  const agents = mastra.getAgents();\n  const workflows = mastra.getWorkflows();\n  const scorersMap = /* @__PURE__ */ new Map();\n  for (const [_agentId, agent] of Object.entries(agents)) {\n    const scorers = await agent.getScorers({\n      runtimeContext\n    }) || {};\n    if (Object.keys(scorers).length > 0) {\n      for (const [scorerId, scorer] of Object.entries(scorers)) {\n        if (scorersMap.has(scorerId)) {\n          scorersMap.get(scorerId)?.agentIds.push(agent.name);\n        } else {\n          scorersMap.set(scorerId, {\n            workflowIds: [],\n            ...scorer,\n            agentIds: [agent.name]\n          });\n        }\n      }\n    }\n  }\n  for (const [workflowId, workflow] of Object.entries(workflows)) {\n    const scorers = await workflow.getScorers({\n      runtimeContext\n    }) || {};\n    if (Object.keys(scorers).length > 0) {\n      for (const [scorerId, scorer] of Object.entries(scorers)) {\n        if (scorersMap.has(scorerId)) {\n          scorersMap.get(scorerId)?.workflowIds.push(workflowId);\n        } else {\n          scorersMap.set(scorerId, {\n            agentIds: [],\n            ...scorer,\n            workflowIds: [workflowId]\n          });\n        }\n      }\n    }\n  }\n  return Object.fromEntries(scorersMap.entries());\n}\nasync function getScorersHandler({ mastra, runtimeContext }) {\n  const scorers = await getScorersFromSystem({\n    mastra,\n    runtimeContext\n  });\n  return scorers;\n}\nasync function getScorerHandler({\n  mastra,\n  scorerId,\n  runtimeContext\n}) {\n  const scorers = await getScorersFromSystem({\n    mastra,\n    runtimeContext\n  });\n  const scorer = scorers[scorerId];\n  if (!scorer) {\n    return null;\n  }\n  return scorer;\n}\nasync function getScoresByRunIdHandler({\n  mastra,\n  runId,\n  pagination\n}) {\n  try {\n    const scores = await mastra.getStorage()?.getScoresByRunId?.({\n      runId,\n      pagination\n    }) || [];\n    return scores;\n  } catch (error) {\n    return handleError(error, \"Error getting scores by run id\");\n  }\n}\nasync function getScoresByScorerIdHandler({\n  mastra,\n  scorerId,\n  pagination,\n  entityId,\n  entityType\n}) {\n  try {\n    const scores = await mastra.getStorage()?.getScoresByScorerId?.({\n      scorerId,\n      pagination,\n      entityId,\n      entityType\n    }) || [];\n    return scores;\n  } catch (error) {\n    return handleError(error, \"Error getting scores by scorer id\");\n  }\n}\nasync function getScoresByEntityIdHandler({\n  mastra,\n  entityId,\n  entityType,\n  pagination\n}) {\n  try {\n    let entityIdToUse = entityId;\n    if (entityType === \"AGENT\") {\n      const agent = mastra.getAgentById(entityId);\n      entityIdToUse = agent.id;\n    } else if (entityType === \"WORKFLOW\") {\n      const workflow = mastra.getWorkflowById(entityId);\n      entityIdToUse = workflow.id;\n    }\n    const scores = await mastra.getStorage()?.getScoresByEntityId?.({\n      entityId: entityIdToUse,\n      entityType,\n      pagination\n    }) || [];\n    return scores;\n  } catch (error) {\n    return handleError(error, \"Error getting scores by entity id\");\n  }\n}\nasync function saveScoreHandler({ mastra, score }) {\n  try {\n    const scores = await mastra.getStorage()?.saveScore?.(score) || [];\n    return scores;\n  } catch (error) {\n    return handleError(error, \"Error saving score\");\n  }\n}\n\nexport { getScorerHandler, getScorersHandler, getScoresByEntityIdHandler, getScoresByRunIdHandler, getScoresByScorerIdHandler, saveScoreHandler, scores_exports };\n//# sourceMappingURL=chunk-7QEJ5QG5.js.map\n//# sourceMappingURL=chunk-7QEJ5QG5.js.map","import { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/telemetry.ts\nvar telemetry_exports = {};\n__export(telemetry_exports, {\n  collectParentSpanIds: () => collectParentSpanIds,\n  getTelemetryHandler: () => getTelemetryHandler,\n  storeTelemetryHandler: () => storeTelemetryHandler\n});\nasync function getTelemetryHandler({ mastra, body }) {\n  try {\n    const telemetry = mastra.getTelemetry();\n    const storage = mastra.getStorage();\n    if (!telemetry) {\n      throw new HTTPException(400, { message: \"Telemetry is not initialized\" });\n    }\n    if (!storage) {\n      return [];\n    }\n    if (!body) {\n      throw new HTTPException(400, { message: \"Body is required\" });\n    }\n    const { name, scope, page, perPage, attribute, fromDate, toDate } = body;\n    const attributes = attribute ? Object.fromEntries(\n      (Array.isArray(attribute) ? attribute : [attribute]).map((attr) => {\n        const [key, value] = attr.split(\":\");\n        return [key, value];\n      })\n    ) : void 0;\n    const traces = await storage.getTraces({\n      name,\n      scope,\n      page: Number(page ?? 0),\n      perPage: Number(perPage ?? 100),\n      attributes,\n      fromDate: fromDate ? new Date(fromDate) : void 0,\n      toDate: toDate ? new Date(toDate) : void 0\n    });\n    return traces;\n  } catch (error2) {\n    return handleError(error2, \"Error getting telemetry\");\n  }\n}\nasync function storeTelemetryHandler({ mastra, body }) {\n  try {\n    const storage = mastra.getStorage();\n    const logger = mastra.getLogger();\n    if (!storage) {\n      return {\n        status: \"error\",\n        message: \"Storage is not initialized\"\n      };\n    }\n    const now = /* @__PURE__ */ new Date();\n    const items = body?.resourceSpans?.[0]?.scopeSpans;\n    logger.debug(\"[Telemetry Handler] Received spans:\", {\n      totalSpans: items?.reduce((acc, scope) => acc + scope.spans.length, 0) || 0,\n      timestamp: now.toISOString()\n    });\n    if (!items?.length) {\n      return {\n        status: \"success\",\n        message: \"No spans to process\",\n        traceCount: 0\n      };\n    }\n    const parentSpanIds = collectParentSpanIds(items);\n    const allSpans = items.reduce((acc, scopedSpans) => {\n      const { scope, spans } = scopedSpans;\n      if (scope.name === \"@opentelemetry/instrumentation-http\") {\n        return acc;\n      }\n      for (const span of spans) {\n        const {\n          spanId,\n          parentSpanId,\n          traceId,\n          name,\n          kind,\n          attributes,\n          status,\n          events,\n          links,\n          startTimeUnixNano,\n          endTimeUnixNano,\n          ...rest\n        } = span;\n        const startTime = Number(BigInt(startTimeUnixNano) / 1000n);\n        const endTime = Number(BigInt(endTimeUnixNano) / 1000n);\n        acc.push({\n          id: spanId,\n          parentSpanId: parentSpanIds.has(parentSpanId) ? null : parentSpanId,\n          traceId,\n          name,\n          scope: scope.name,\n          kind,\n          status: JSON.stringify(status),\n          events: JSON.stringify(events),\n          links: JSON.stringify(links),\n          attributes: JSON.stringify(\n            attributes.reduce((acc2, attr) => {\n              const valueKey = Object.keys(attr.value)[0];\n              if (valueKey) {\n                acc2[attr.key] = attr.value[valueKey];\n              }\n              return acc2;\n            }, {})\n          ),\n          startTime,\n          endTime,\n          other: JSON.stringify(rest),\n          createdAt: now\n        });\n      }\n      return acc;\n    }, []);\n    return storage.batchTraceInsert({\n      records: allSpans\n    }).then(() => {\n      return {\n        status: \"success\",\n        message: \"Traces received and processed successfully\",\n        traceCount: body.resourceSpans?.length || 0\n      };\n    }).catch(() => {\n      return {\n        status: \"error\",\n        message: \"Failed to process traces\",\n        // @ts-ignore\n        error: error.message\n      };\n    });\n  } catch (error2) {\n    console.error(\"Error processing traces:\", error2);\n    return {\n      status: \"error\",\n      message: \"Failed to process traces\",\n      // @ts-ignore\n      error: error2.message\n    };\n  }\n}\nvar collectParentSpanIds = (items) => {\n  const result = /* @__PURE__ */ new Set();\n  for (const { scope, spans } of items) {\n    if (scope.name !== \"@opentelemetry/instrumentation-http\") {\n      continue;\n    }\n    for (const span of spans) {\n      result.add(span.spanId);\n    }\n  }\n  return result;\n};\n\nexport { collectParentSpanIds, getTelemetryHandler, storeTelemetryHandler, telemetry_exports };\n//# sourceMappingURL=chunk-KV6VHX4V.js.map\n//# sourceMappingURL=chunk-KV6VHX4V.js.map","import { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\n\n// src/server/handlers/vector.ts\nvar vector_exports = {};\n__export(vector_exports, {\n  createIndex: () => createIndex,\n  deleteIndex: () => deleteIndex,\n  describeIndex: () => describeIndex,\n  listIndexes: () => listIndexes,\n  queryVectors: () => queryVectors,\n  upsertVectors: () => upsertVectors\n});\nfunction getVector(mastra, vectorName) {\n  if (!vectorName) {\n    throw new HTTPException(400, { message: \"Vector name is required\" });\n  }\n  const vector = mastra.getVector(vectorName);\n  if (!vector) {\n    throw new HTTPException(404, { message: `Vector store ${vectorName} not found` });\n  }\n  return vector;\n}\nasync function upsertVectors({ mastra, vectorName, index }) {\n  try {\n    if (!index?.indexName || !index?.vectors || !Array.isArray(index.vectors)) {\n      throw new HTTPException(400, { message: \"Invalid request index. indexName and vectors array are required.\" });\n    }\n    const vector = getVector(mastra, vectorName);\n    const result = await vector.upsert(index);\n    return { ids: result };\n  } catch (error) {\n    return handleError(error, \"Error upserting vectors\");\n  }\n}\nasync function createIndex({\n  mastra,\n  vectorName,\n  index\n}) {\n  try {\n    const { indexName, dimension, metric } = index;\n    if (!indexName || typeof dimension !== \"number\" || dimension <= 0) {\n      throw new HTTPException(400, {\n        message: \"Invalid request index, indexName and positive dimension number are required.\"\n      });\n    }\n    if (metric && ![\"cosine\", \"euclidean\", \"dotproduct\"].includes(metric)) {\n      throw new HTTPException(400, { message: \"Invalid metric. Must be one of: cosine, euclidean, dotproduct\" });\n    }\n    const vector = getVector(mastra, vectorName);\n    await vector.createIndex({ indexName, dimension, metric });\n    return { success: true };\n  } catch (error) {\n    return handleError(error, \"Error creating index\");\n  }\n}\nasync function queryVectors({\n  mastra,\n  vectorName,\n  query\n}) {\n  try {\n    if (!query?.indexName || !query?.queryVector || !Array.isArray(query.queryVector)) {\n      throw new HTTPException(400, { message: \"Invalid request query. indexName and queryVector array are required.\" });\n    }\n    const vector = getVector(mastra, vectorName);\n    const results = await vector.query(query);\n    return results;\n  } catch (error) {\n    return handleError(error, \"Error querying vectors\");\n  }\n}\nasync function listIndexes({ mastra, vectorName }) {\n  try {\n    const vector = getVector(mastra, vectorName);\n    const indexes = await vector.listIndexes();\n    return indexes.filter(Boolean);\n  } catch (error) {\n    return handleError(error, \"Error listing indexes\");\n  }\n}\nasync function describeIndex({\n  mastra,\n  vectorName,\n  indexName\n}) {\n  try {\n    if (!indexName) {\n      throw new HTTPException(400, { message: \"Index name is required\" });\n    }\n    const vector = getVector(mastra, vectorName);\n    const stats = await vector.describeIndex({ indexName });\n    return {\n      dimension: stats.dimension,\n      count: stats.count,\n      metric: stats.metric?.toLowerCase()\n    };\n  } catch (error) {\n    return handleError(error, \"Error describing index\");\n  }\n}\nasync function deleteIndex({\n  mastra,\n  vectorName,\n  indexName\n}) {\n  try {\n    if (!indexName) {\n      throw new HTTPException(400, { message: \"Index name is required\" });\n    }\n    const vector = getVector(mastra, vectorName);\n    await vector.deleteIndex({ indexName });\n    return { success: true };\n  } catch (error) {\n    return handleError(error, \"Error deleting index\");\n  }\n}\n\nexport { createIndex, deleteIndex, describeIndex, listIndexes, queryVectors, upsertVectors, vector_exports };\n//# sourceMappingURL=chunk-WHN4VX55.js.map\n//# sourceMappingURL=chunk-WHN4VX55.js.map","import { stringify } from './chunk-LF2ZLOFP.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\nimport { ReadableStream } from 'stream/web';\nimport { zodToJsonSchema } from '@mastra/core/utils/zod-to-json';\n\n// src/server/handlers/workflows.ts\nvar workflows_exports = {};\n__export(workflows_exports, {\n  cancelWorkflowRunHandler: () => cancelWorkflowRunHandler,\n  createWorkflowRunHandler: () => createWorkflowRunHandler,\n  getWorkflowByIdHandler: () => getWorkflowByIdHandler,\n  getWorkflowRunByIdHandler: () => getWorkflowRunByIdHandler,\n  getWorkflowRunExecutionResultHandler: () => getWorkflowRunExecutionResultHandler,\n  getWorkflowRunsHandler: () => getWorkflowRunsHandler,\n  getWorkflowsHandler: () => getWorkflowsHandler,\n  resumeAsyncWorkflowHandler: () => resumeAsyncWorkflowHandler,\n  resumeWorkflowHandler: () => resumeWorkflowHandler,\n  sendWorkflowRunEventHandler: () => sendWorkflowRunEventHandler,\n  startAsyncWorkflowHandler: () => startAsyncWorkflowHandler,\n  startWorkflowRunHandler: () => startWorkflowRunHandler,\n  streamVNextWorkflowHandler: () => streamVNextWorkflowHandler,\n  streamWorkflowHandler: () => streamWorkflowHandler,\n  watchWorkflowHandler: () => watchWorkflowHandler\n});\nfunction getSteps(steps, path) {\n  return Object.entries(steps).reduce((acc, [key, step]) => {\n    const fullKey = path ? `${path}.${key}` : key;\n    acc[fullKey] = {\n      id: step.id,\n      description: step.description,\n      inputSchema: step.inputSchema ? stringify(zodToJsonSchema(step.inputSchema)) : void 0,\n      outputSchema: step.outputSchema ? stringify(zodToJsonSchema(step.outputSchema)) : void 0,\n      resumeSchema: step.resumeSchema ? stringify(zodToJsonSchema(step.resumeSchema)) : void 0,\n      suspendSchema: step.suspendSchema ? stringify(zodToJsonSchema(step.suspendSchema)) : void 0,\n      isWorkflow: step.component === \"WORKFLOW\"\n    };\n    if (step.component === \"WORKFLOW\" && step.steps) {\n      const nestedSteps = getSteps(step.steps, fullKey) || {};\n      acc = { ...acc, ...nestedSteps };\n    }\n    return acc;\n  }, {});\n}\nfunction getWorkflowInfo(workflow) {\n  return {\n    name: workflow.name,\n    description: workflow.description,\n    steps: Object.entries(workflow.steps).reduce((acc, [key, step]) => {\n      acc[key] = {\n        id: step.id,\n        description: step.description,\n        inputSchema: step.inputSchema ? stringify(zodToJsonSchema(step.inputSchema)) : void 0,\n        outputSchema: step.outputSchema ? stringify(zodToJsonSchema(step.outputSchema)) : void 0,\n        resumeSchema: step.resumeSchema ? stringify(zodToJsonSchema(step.resumeSchema)) : void 0,\n        suspendSchema: step.suspendSchema ? stringify(zodToJsonSchema(step.suspendSchema)) : void 0\n      };\n      return acc;\n    }, {}),\n    allSteps: getSteps(workflow.steps) || {},\n    stepGraph: workflow.serializedStepGraph,\n    inputSchema: workflow.inputSchema ? stringify(zodToJsonSchema(workflow.inputSchema)) : void 0,\n    outputSchema: workflow.outputSchema ? stringify(zodToJsonSchema(workflow.outputSchema)) : void 0\n  };\n}\nvar WorkflowRegistry = class {\n  static additionalWorkflows = {};\n  /**\n   * Register a workflow temporarily\n   */\n  static registerTemporaryWorkflow(id, workflow) {\n    this.additionalWorkflows[id] = workflow;\n  }\n  /**\n   * Register all workflows from map\n   */\n  static registerTemporaryWorkflows(workflows) {\n    for (const [id, workflow] of Object.entries(workflows)) {\n      this.additionalWorkflows[id] = workflow;\n    }\n  }\n  /**\n   * Get a workflow by ID from the registry (returns undefined if not found)\n   */\n  static getWorkflow(workflowId) {\n    return this.additionalWorkflows[workflowId];\n  }\n  /**\n   * Get all workflows from the registry\n   */\n  static getAllWorkflows() {\n    return { ...this.additionalWorkflows };\n  }\n  /**\n   * Clean up a temporary workflow\n   */\n  static cleanupTemporaryWorkflow(workflowId) {\n    delete this.additionalWorkflows[workflowId];\n  }\n  /**\n   * Clean up all registered workflows\n   */\n  static cleanup() {\n    this.additionalWorkflows = {};\n  }\n  /**\n   * Check if a workflow ID is a valid agent-builder workflow\n   */\n  static isAgentBuilderWorkflow(workflowId) {\n    return workflowId in this.additionalWorkflows;\n  }\n  /**\n   * Get all registered temporary workflow IDs (for debugging)\n   */\n  static getRegisteredWorkflowIds() {\n    return Object.keys(this.additionalWorkflows);\n  }\n};\n\n// src/server/handlers/workflows.ts\nasync function getWorkflowsHandler({ mastra }) {\n  try {\n    const workflows = mastra.getWorkflows({ serialized: false });\n    const _workflows = Object.entries(workflows).reduce((acc, [key, workflow]) => {\n      acc[key] = getWorkflowInfo(workflow);\n      return acc;\n    }, {});\n    return _workflows;\n  } catch (error) {\n    return handleError(error, \"Error getting workflows\");\n  }\n}\nasync function getWorkflowsFromSystem({ mastra, workflowId }) {\n  const logger = mastra.getLogger();\n  if (!workflowId) {\n    throw new HTTPException(400, { message: \"Workflow ID is required\" });\n  }\n  let workflow;\n  workflow = WorkflowRegistry.getWorkflow(workflowId);\n  if (!workflow) {\n    try {\n      workflow = mastra.getWorkflow(workflowId);\n    } catch (error) {\n      logger.debug(\"Error getting workflow, searching agents for workflow\", error);\n    }\n  }\n  if (!workflow) {\n    logger.debug(\"Workflow not found, searching agents for workflow\", { workflowId });\n    const agents = mastra.getAgents();\n    if (Object.keys(agents || {}).length) {\n      for (const [_, agent] of Object.entries(agents)) {\n        try {\n          const workflows = await agent.getWorkflows();\n          if (workflows[workflowId]) {\n            workflow = workflows[workflowId];\n            break;\n          }\n          break;\n        } catch (error) {\n          logger.debug(\"Error getting workflow from agent\", error);\n        }\n      }\n    }\n  }\n  if (!workflow) {\n    throw new HTTPException(404, { message: \"Workflow not found\" });\n  }\n  return { workflow };\n}\nasync function getWorkflowByIdHandler({ mastra, workflowId }) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    return getWorkflowInfo(workflow);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow\");\n  }\n}\nasync function getWorkflowRunByIdHandler({\n  mastra,\n  workflowId,\n  runId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"Run ID is required\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    return run;\n  } catch (error) {\n    return handleError(error, \"Error getting workflow run\");\n  }\n}\nasync function getWorkflowRunExecutionResultHandler({\n  mastra,\n  workflowId,\n  runId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"Run ID is required\" });\n    }\n    const workflow = mastra.getWorkflow(workflowId);\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const executionResult = await workflow.getWorkflowRunExecutionResult(runId);\n    if (!executionResult) {\n      throw new HTTPException(404, { message: \"Workflow run execution result not found\" });\n    }\n    return executionResult;\n  } catch (error) {\n    return handleError(error, \"Error getting workflow run execution result\");\n  }\n}\nasync function createWorkflowRunHandler({\n  mastra,\n  workflowId,\n  runId: prevRunId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.createRunAsync({ runId: prevRunId });\n    return { runId: run.runId };\n  } catch (error) {\n    return handleError(error, \"Error creating workflow run\");\n  }\n}\nasync function startAsyncWorkflowHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  inputData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    const result = await _run.start({\n      inputData,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error starting async workflow\");\n  }\n}\nasync function startWorkflowRunHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  inputData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to start run\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    void _run.start({\n      inputData,\n      runtimeContext\n    });\n    return { message: \"Workflow run started\" };\n  } catch (e) {\n    return handleError(e, \"Error starting workflow run\");\n  }\n}\nasync function watchWorkflowHandler({\n  mastra,\n  workflowId,\n  runId,\n  eventType = \"watch\"\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to watch workflow\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    let unwatch;\n    let asyncRef = null;\n    const stream = new ReadableStream({\n      start(controller) {\n        unwatch = _run.watch((event) => {\n          const { type, payload, eventTimestamp } = event;\n          controller.enqueue(JSON.stringify({ type, payload, eventTimestamp, runId }));\n          if (asyncRef) {\n            clearImmediate(asyncRef);\n            asyncRef = null;\n          }\n          asyncRef = setImmediate(async () => {\n            const runDone = eventType === \"watch\" ? payload.workflowState.status !== \"running\" : type === \"finish\";\n            if (runDone) {\n              controller.close();\n              unwatch?.();\n            }\n          });\n        }, eventType);\n      },\n      cancel() {\n        if (asyncRef) {\n          clearImmediate(asyncRef);\n          asyncRef = null;\n        }\n        unwatch?.();\n      }\n    });\n    return stream;\n  } catch (error) {\n    return handleError(error, \"Error watching workflow\");\n  }\n}\nasync function streamWorkflowHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  inputData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.createRunAsync({ runId });\n    const result = run.stream({\n      inputData,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error executing workflow\");\n  }\n}\nasync function streamVNextWorkflowHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  inputData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to stream workflow\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.createRunAsync({ runId });\n    const result = run.streamVNext({\n      inputData,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error streaming workflow\");\n  }\n}\nasync function resumeAsyncWorkflowHandler({\n  mastra,\n  workflowId,\n  runId,\n  body,\n  runtimeContext\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    if (!body.step) {\n      throw new HTTPException(400, { message: \"step required to resume workflow\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    const result = await _run.resume({\n      step: body.step,\n      resumeData: body.resumeData,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow step\");\n  }\n}\nasync function resumeWorkflowHandler({\n  mastra,\n  workflowId,\n  runId,\n  body,\n  runtimeContext\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    if (!body.step) {\n      throw new HTTPException(400, { message: \"step required to resume workflow\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    void _run.resume({\n      step: body.step,\n      resumeData: body.resumeData,\n      runtimeContext\n    });\n    return { message: \"Workflow run resumed\" };\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow\");\n  }\n}\nasync function getWorkflowRunsHandler({\n  mastra,\n  workflowId,\n  fromDate,\n  toDate,\n  limit,\n  offset,\n  resourceId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const workflowRuns = await workflow.getWorkflowRuns({ fromDate, toDate, limit, offset, resourceId }) || {\n      runs: [],\n      total: 0\n    };\n    return workflowRuns;\n  } catch (error) {\n    return handleError(error, \"Error getting workflow runs\");\n  }\n}\nasync function cancelWorkflowRunHandler({\n  mastra,\n  workflowId,\n  runId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to cancel workflow run\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    await _run.cancel();\n    return { message: \"Workflow run cancelled\" };\n  } catch (error) {\n    return handleError(error, \"Error canceling workflow run\");\n  }\n}\nasync function sendWorkflowRunEventHandler({\n  mastra,\n  workflowId,\n  runId,\n  event,\n  data\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to send workflow run event\" });\n    }\n    const { workflow } = await getWorkflowsFromSystem({ mastra, workflowId });\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getWorkflowRunById(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const _run = await workflow.createRunAsync({ runId });\n    await _run.sendEvent(event, data);\n    return { message: \"Workflow run event sent\" };\n  } catch (error) {\n    return handleError(error, \"Error sending workflow run event\");\n  }\n}\n\nexport { WorkflowRegistry, cancelWorkflowRunHandler, createWorkflowRunHandler, getWorkflowByIdHandler, getWorkflowInfo, getWorkflowRunByIdHandler, getWorkflowRunExecutionResultHandler, getWorkflowRunsHandler, getWorkflowsHandler, resumeAsyncWorkflowHandler, resumeWorkflowHandler, sendWorkflowRunEventHandler, startAsyncWorkflowHandler, startWorkflowRunHandler, streamVNextWorkflowHandler, streamWorkflowHandler, watchWorkflowHandler, workflows_exports };\n//# sourceMappingURL=chunk-OZGPYA7A.js.map\n//# sourceMappingURL=chunk-OZGPYA7A.js.map","import { stringify } from './chunk-LF2ZLOFP.js';\nimport { handleError } from './chunk-CY4TP3FK.js';\nimport { HTTPException } from './chunk-MMROOK5J.js';\nimport { __export } from './chunk-G3PMV62Z.js';\nimport { ReadableStream } from 'stream/web';\nimport { zodToJsonSchema } from '@mastra/core/utils/zod-to-json';\n\n// src/server/handlers/legacyWorkflows.ts\nvar legacyWorkflows_exports = {};\n__export(legacyWorkflows_exports, {\n  createLegacyWorkflowRunHandler: () => createLegacyWorkflowRunHandler,\n  getLegacyWorkflowByIdHandler: () => getLegacyWorkflowByIdHandler,\n  getLegacyWorkflowRunHandler: () => getLegacyWorkflowRunHandler,\n  getLegacyWorkflowRunsHandler: () => getLegacyWorkflowRunsHandler,\n  getLegacyWorkflowsHandler: () => getLegacyWorkflowsHandler,\n  resumeAsyncLegacyWorkflowHandler: () => resumeAsyncLegacyWorkflowHandler,\n  resumeLegacyWorkflowHandler: () => resumeLegacyWorkflowHandler,\n  startAsyncLegacyWorkflowHandler: () => startAsyncLegacyWorkflowHandler,\n  startLegacyWorkflowRunHandler: () => startLegacyWorkflowRunHandler,\n  watchLegacyWorkflowHandler: () => watchLegacyWorkflowHandler\n});\nasync function getLegacyWorkflowsHandler({ mastra }) {\n  try {\n    const workflows = mastra.legacy_getWorkflows({ serialized: false });\n    const _workflows = Object.entries(workflows).reduce((acc, [key, workflow]) => {\n      if (workflow.isNested) return acc;\n      acc[key] = {\n        stepGraph: workflow.stepGraph,\n        stepSubscriberGraph: workflow.stepSubscriberGraph,\n        serializedStepGraph: workflow.serializedStepGraph,\n        serializedStepSubscriberGraph: workflow.serializedStepSubscriberGraph,\n        name: workflow.name,\n        triggerSchema: workflow.triggerSchema ? stringify(zodToJsonSchema(workflow.triggerSchema)) : void 0,\n        steps: Object.entries(workflow.steps).reduce((acc2, [key2, step]) => {\n          const _step = step;\n          acc2[key2] = {\n            id: _step.id,\n            description: _step.description,\n            workflowId: _step.workflowId,\n            inputSchema: _step.inputSchema ? stringify(zodToJsonSchema(_step.inputSchema)) : void 0,\n            outputSchema: _step.outputSchema ? stringify(zodToJsonSchema(_step.outputSchema)) : void 0\n          };\n          return acc2;\n        }, {})\n      };\n      return acc;\n    }, {});\n    return _workflows;\n  } catch (error) {\n    return handleError(error, \"error getting workflows\");\n  }\n}\nasync function getLegacyWorkflowByIdHandler({ mastra, workflowId }) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    return {\n      stepGraph: workflow.stepGraph,\n      stepSubscriberGraph: workflow.stepSubscriberGraph,\n      serializedStepGraph: workflow.serializedStepGraph,\n      serializedStepSubscriberGraph: workflow.serializedStepSubscriberGraph,\n      name: workflow.name,\n      triggerSchema: workflow.triggerSchema ? stringify(zodToJsonSchema(workflow.triggerSchema)) : void 0,\n      steps: Object.entries(workflow.steps).reduce((acc, [key, step]) => {\n        const _step = step;\n        acc[key] = {\n          id: _step.id,\n          description: _step.description,\n          workflowId: _step.workflowId,\n          inputSchema: _step.inputSchema ? stringify(zodToJsonSchema(_step.inputSchema)) : void 0,\n          outputSchema: _step.outputSchema ? stringify(zodToJsonSchema(_step.outputSchema)) : void 0\n        };\n        return acc;\n      }, {})\n    };\n  } catch (error) {\n    return handleError(error, \"error getting workflow by id\");\n  }\n}\nasync function startAsyncLegacyWorkflowHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  triggerData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    if (!runId) {\n      const newRun = workflow.createRun();\n      const result2 = await newRun.start({\n        triggerData,\n        runtimeContext\n      });\n      return result2;\n    }\n    const run = workflow.getMemoryRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const result = await run.start({\n      triggerData,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"error starting workflow\");\n  }\n}\nasync function getLegacyWorkflowRunHandler({\n  mastra,\n  workflowId,\n  runId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"Run ID is required\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const run = await workflow.getRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    return run;\n  } catch (error) {\n    return handleError(error, \"error getting workflow run\");\n  }\n}\nasync function createLegacyWorkflowRunHandler({\n  mastra,\n  workflowId,\n  runId: prevRunId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    if (!workflow) {\n      throw new HTTPException(404, { message: \"Workflow not found\" });\n    }\n    const newRun = workflow.createRun({ runId: prevRunId });\n    return { runId: newRun.runId };\n  } catch (error) {\n    return handleError(error, \"error creating workflow run\");\n  }\n}\nasync function startLegacyWorkflowRunHandler({\n  mastra,\n  runtimeContext,\n  workflowId,\n  runId,\n  triggerData\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to start run\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    const run = workflow.getMemoryRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    void run.start({\n      triggerData,\n      runtimeContext\n    });\n    return { message: \"Workflow run started\" };\n  } catch (e) {\n    return handleError(e, \"Error starting workflow run\");\n  }\n}\nasync function watchLegacyWorkflowHandler({\n  mastra,\n  workflowId,\n  runId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to watch workflow\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    const run = workflow.getMemoryRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    let unwatch;\n    let asyncRef = null;\n    const stream = new ReadableStream({\n      start(controller) {\n        unwatch = run.watch(({ activePaths, runId: runId2, timestamp, results }) => {\n          const activePathsObj = Object.fromEntries(activePaths);\n          controller.enqueue(JSON.stringify({ activePaths: activePathsObj, runId: runId2, timestamp, results }));\n          if (asyncRef) {\n            clearImmediate(asyncRef);\n            asyncRef = null;\n          }\n          asyncRef = setImmediate(() => {\n            const runDone = Object.values(activePathsObj).every((value) => value.status !== \"executing\");\n            if (runDone) {\n              controller.close();\n              unwatch?.();\n            }\n          });\n        });\n      },\n      cancel() {\n        unwatch?.();\n      }\n    });\n    return stream;\n  } catch (error) {\n    return handleError(error, \"Error watching workflow\");\n  }\n}\nasync function resumeAsyncLegacyWorkflowHandler({\n  mastra,\n  workflowId,\n  runId,\n  body,\n  runtimeContext\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    const run = workflow.getMemoryRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    const result = await run.resume({\n      stepId: body.stepId,\n      context: body.context,\n      runtimeContext\n    });\n    return result;\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow step\");\n  }\n}\nasync function resumeLegacyWorkflowHandler({\n  mastra,\n  workflowId,\n  runId,\n  body,\n  runtimeContext\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    const run = workflow.getMemoryRun(runId);\n    if (!run) {\n      throw new HTTPException(404, { message: \"Workflow run not found\" });\n    }\n    void run.resume({\n      stepId: body.stepId,\n      context: body.context,\n      runtimeContext\n    });\n    return { message: \"Workflow run resumed\" };\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow\");\n  }\n}\nasync function getLegacyWorkflowRunsHandler({\n  mastra,\n  workflowId,\n  fromDate,\n  toDate,\n  limit,\n  offset,\n  resourceId\n}) {\n  try {\n    if (!workflowId) {\n      throw new HTTPException(400, { message: \"Workflow ID is required\" });\n    }\n    const workflow = mastra.legacy_getWorkflow(workflowId);\n    const workflowRuns = await workflow.getWorkflowRuns({ fromDate, toDate, limit, offset, resourceId }) || {\n      runs: [],\n      total: 0\n    };\n    return workflowRuns;\n  } catch (error) {\n    return handleError(error, \"Error getting workflow runs\");\n  }\n}\n\nexport { createLegacyWorkflowRunHandler, getLegacyWorkflowByIdHandler, getLegacyWorkflowRunHandler, getLegacyWorkflowRunsHandler, getLegacyWorkflowsHandler, legacyWorkflows_exports, resumeAsyncLegacyWorkflowHandler, resumeLegacyWorkflowHandler, startAsyncLegacyWorkflowHandler, startLegacyWorkflowRunHandler, watchLegacyWorkflowHandler };\n//# sourceMappingURL=chunk-TTHEEIZ3.js.map\n//# sourceMappingURL=chunk-TTHEEIZ3.js.map","import crypto, { randomUUID } from 'crypto';\nimport { readFile } from 'fs/promises';\nimport { join } from 'path/posix';\nimport { createServer } from 'http';\nimport { Http2ServerRequest } from 'http2';\nimport { Writable, Readable } from 'stream';\nimport { getMimeType } from 'hono/utils/mime';\nimport { createReadStream, lstatSync } from 'fs';\nimport { join as join$1 } from 'path';\nimport { html } from 'hono/html';\nimport { RuntimeContext } from '@mastra/core/runtime-context';\nimport { Telemetry } from '@mastra/core/telemetry';\nimport { Tool } from '@mastra/core/tools';\nimport { InMemoryTaskStore } from '@mastra/server/a2a/store';\nimport { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { logger } from 'hono/logger';\nimport { timeout } from 'hono/timeout';\nimport { HTTPException } from 'hono/http-exception';\nimport { getAgentCardByIdHandler as getAgentCardByIdHandler$1, getAgentExecutionHandler as getAgentExecutionHandler$1 } from '@mastra/server/handlers/a2a';\nimport { stream } from 'hono/streaming';\nimport { getAgentsHandler as getAgentsHandler$1, getAgentByIdHandler as getAgentByIdHandler$1, getEvalsByAgentIdHandler as getEvalsByAgentIdHandler$1, getLiveEvalsByAgentIdHandler as getLiveEvalsByAgentIdHandler$1, generateLegacyHandler as generateLegacyHandler$1, generateHandler as generateHandler$1, generateVNextHandler as generateVNextHandler$1, streamVNextGenerateHandler as streamVNextGenerateHandler$1, streamGenerateLegacyHandler as streamGenerateLegacyHandler$1, streamGenerateHandler as streamGenerateHandler$1, streamVNextUIMessageHandler as streamVNextUIMessageHandler$1, updateAgentModelHandler as updateAgentModelHandler$1 } from '@mastra/server/handlers/agents';\nimport { bodyLimit } from 'hono/body-limit';\nimport { Agent } from '@mastra/core/agent';\nimport { z } from 'zod';\nimport { getAgentToolHandler as getAgentToolHandler$1, executeAgentToolHandler as executeAgentToolHandler$1, getToolsHandler as getToolsHandler$1, getToolByIdHandler as getToolByIdHandler$1, executeToolHandler as executeToolHandler$1 } from '@mastra/server/handlers/tools';\nimport { getSpeakersHandler as getSpeakersHandler$1, generateSpeechHandler, getListenerHandler as getListenerHandler$1, transcribeSpeechHandler } from '@mastra/server/handlers/voice';\nimport { getLogsHandler as getLogsHandler$1, getLogTransports as getLogTransports$1, getLogsByRunIdHandler as getLogsByRunIdHandler$1 } from '@mastra/server/handlers/logs';\nimport util from 'util';\nimport { Buffer as Buffer$1 } from 'buffer';\nimport { getMemoryStatusHandler as getMemoryStatusHandler$1, getThreadsHandler as getThreadsHandler$1, getThreadByIdHandler as getThreadByIdHandler$1, getMessagesHandler as getMessagesHandler$1, createThreadHandler as createThreadHandler$1, updateThreadHandler as updateThreadHandler$1, deleteThreadHandler as deleteThreadHandler$1, saveMessagesHandler as saveMessagesHandler$1, deleteMessagesHandler as deleteMessagesHandler$1, getMemoryConfigHandler as getMemoryConfigHandler$1, getThreadsPaginatedHandler as getThreadsPaginatedHandler$1, getMessagesPaginatedHandler as getMessagesPaginatedHandler$1, searchMemoryHandler as searchMemoryHandler$1, getWorkingMemoryHandler as getWorkingMemoryHandler$1, updateWorkingMemoryHandler as updateWorkingMemoryHandler$1 } from '@mastra/server/handlers/memory';\nimport { getNetworksHandler as getNetworksHandler$1, getNetworkByIdHandler as getNetworkByIdHandler$1, generateHandler as generateHandler$2, streamGenerateHandler as streamGenerateHandler$2 } from '@mastra/server/handlers/network';\nimport { getVNextNetworksHandler as getVNextNetworksHandler$1, getVNextNetworkByIdHandler as getVNextNetworkByIdHandler$1, generateVNextNetworkHandler as generateVNextNetworkHandler$1, loopVNextNetworkHandler as loopVNextNetworkHandler$1, loopStreamVNextNetworkHandler as loopStreamVNextNetworkHandler$1, streamGenerateVNextNetworkHandler as streamGenerateVNextNetworkHandler$1 } from '@mastra/server/handlers/vNextNetwork';\nimport { AISpanType } from '@mastra/core/ai-tracing';\nimport { getAITracesPaginatedHandler as getAITracesPaginatedHandler$1, getAITraceHandler as getAITraceHandler$1 } from '@mastra/server/handlers/observability';\nimport { getScorersHandler as getScorersHandler$1, getScorerHandler as getScorerHandler$1, getScoresByRunIdHandler as getScoresByRunIdHandler$1, getScoresByScorerIdHandler as getScoresByScorerIdHandler$1, getScoresByEntityIdHandler as getScoresByEntityIdHandler$1, saveScoreHandler as saveScoreHandler$1 } from '@mastra/server/handlers/scores';\nimport { getTelemetryHandler as getTelemetryHandler$1, storeTelemetryHandler as storeTelemetryHandler$1 } from '@mastra/server/handlers/telemetry';\nimport { upsertVectors as upsertVectors$1, createIndex as createIndex$1, queryVectors as queryVectors$1, listIndexes as listIndexes$1, describeIndex as describeIndex$1, deleteIndex as deleteIndex$1 } from '@mastra/server/handlers/vector';\nimport { getWorkflowsHandler as getWorkflowsHandler$1, getWorkflowByIdHandler as getWorkflowByIdHandler$1, getWorkflowRunsHandler as getWorkflowRunsHandler$1, getWorkflowRunExecutionResultHandler as getWorkflowRunExecutionResultHandler$1, getWorkflowRunByIdHandler as getWorkflowRunByIdHandler$1, resumeWorkflowHandler as resumeWorkflowHandler$1, resumeAsyncWorkflowHandler as resumeAsyncWorkflowHandler$1, streamWorkflowHandler as streamWorkflowHandler$1, streamVNextWorkflowHandler as streamVNextWorkflowHandler$1, createWorkflowRunHandler as createWorkflowRunHandler$1, startAsyncWorkflowHandler as startAsyncWorkflowHandler$1, startWorkflowRunHandler as startWorkflowRunHandler$1, watchWorkflowHandler as watchWorkflowHandler$1, cancelWorkflowRunHandler as cancelWorkflowRunHandler$1, sendWorkflowRunEventHandler as sendWorkflowRunEventHandler$1 } from '@mastra/server/handlers/workflows';\nimport { getLegacyWorkflowsHandler as getLegacyWorkflowsHandler$1, getLegacyWorkflowByIdHandler as getLegacyWorkflowByIdHandler$1, getLegacyWorkflowRunsHandler as getLegacyWorkflowRunsHandler$1, resumeLegacyWorkflowHandler as resumeLegacyWorkflowHandler$1, resumeAsyncLegacyWorkflowHandler as resumeAsyncLegacyWorkflowHandler$1, createLegacyWorkflowRunHandler as createLegacyWorkflowRunHandler$1, startAsyncLegacyWorkflowHandler as startAsyncLegacyWorkflowHandler$1, startLegacyWorkflowRunHandler as startLegacyWorkflowRunHandler$1, watchLegacyWorkflowHandler as watchLegacyWorkflowHandler$1 } from '@mastra/server/handlers/legacyWorkflows';\n\n// src/server/index.ts\nvar RequestError = class extends Error {\n  constructor(message, options) {\n    super(message, options);\n    this.name = \"RequestError\";\n  }\n};\nvar toRequestError = (e2) => {\n  if (e2 instanceof RequestError) {\n    return e2;\n  }\n  return new RequestError(e2.message, { cause: e2 });\n};\nvar GlobalRequest = global.Request;\nvar Request = class extends GlobalRequest {\n  constructor(input, options) {\n    if (typeof input === \"object\" && getRequestCache in input) {\n      input = input[getRequestCache]();\n    }\n    if (typeof options?.body?.getReader !== \"undefined\") {\n      options.duplex ??= \"half\";\n    }\n    super(input, options);\n  }\n};\nvar wrapBodyStream = Symbol(\"wrapBodyStream\");\nvar newRequestFromIncoming = (method, url, incoming, abortController) => {\n  const headerRecord = [];\n  const rawHeaders = incoming.rawHeaders;\n  for (let i2 = 0; i2 < rawHeaders.length; i2 += 2) {\n    const { [i2]: key, [i2 + 1]: value } = rawHeaders;\n    if (key.charCodeAt(0) !== /*:*/\n    58) {\n      headerRecord.push([key, value]);\n    }\n  }\n  const init = {\n    method,\n    headers: headerRecord,\n    signal: abortController.signal\n  };\n  if (method === \"TRACE\") {\n    init.method = \"GET\";\n    const req = new Request(url, init);\n    Object.defineProperty(req, \"method\", {\n      get() {\n        return \"TRACE\";\n      }\n    });\n    return req;\n  }\n  if (!(method === \"GET\" || method === \"HEAD\")) {\n    if (\"rawBody\" in incoming && incoming.rawBody instanceof Buffer) {\n      init.body = new ReadableStream({\n        start(controller) {\n          controller.enqueue(incoming.rawBody);\n          controller.close();\n        }\n      });\n    } else if (incoming[wrapBodyStream]) {\n      let reader;\n      init.body = new ReadableStream({\n        async pull(controller) {\n          try {\n            reader ||= Readable.toWeb(incoming).getReader();\n            const { done, value } = await reader.read();\n            if (done) {\n              controller.close();\n            } else {\n              controller.enqueue(value);\n            }\n          } catch (error) {\n            controller.error(error);\n          }\n        }\n      });\n    } else {\n      init.body = Readable.toWeb(incoming);\n    }\n  }\n  return new Request(url, init);\n};\nvar getRequestCache = Symbol(\"getRequestCache\");\nvar requestCache = Symbol(\"requestCache\");\nvar incomingKey = Symbol(\"incomingKey\");\nvar urlKey = Symbol(\"urlKey\");\nvar abortControllerKey = Symbol(\"abortControllerKey\");\nvar getAbortController = Symbol(\"getAbortController\");\nvar requestPrototype = {\n  get method() {\n    return this[incomingKey].method || \"GET\";\n  },\n  get url() {\n    return this[urlKey];\n  },\n  [getAbortController]() {\n    this[getRequestCache]();\n    return this[abortControllerKey];\n  },\n  [getRequestCache]() {\n    this[abortControllerKey] ||= new AbortController();\n    return this[requestCache] ||= newRequestFromIncoming(\n      this.method,\n      this[urlKey],\n      this[incomingKey],\n      this[abortControllerKey]\n    );\n  }\n};\n[\n  \"body\",\n  \"bodyUsed\",\n  \"cache\",\n  \"credentials\",\n  \"destination\",\n  \"headers\",\n  \"integrity\",\n  \"mode\",\n  \"redirect\",\n  \"referrer\",\n  \"referrerPolicy\",\n  \"signal\",\n  \"keepalive\"\n].forEach((k) => {\n  Object.defineProperty(requestPrototype, k, {\n    get() {\n      return this[getRequestCache]()[k];\n    }\n  });\n});\n[\"arrayBuffer\", \"blob\", \"clone\", \"formData\", \"json\", \"text\"].forEach((k) => {\n  Object.defineProperty(requestPrototype, k, {\n    value: function() {\n      return this[getRequestCache]()[k]();\n    }\n  });\n});\nObject.setPrototypeOf(requestPrototype, Request.prototype);\nvar newRequest = (incoming, defaultHostname) => {\n  const req = Object.create(requestPrototype);\n  req[incomingKey] = incoming;\n  const incomingUrl = incoming.url || \"\";\n  if (incomingUrl[0] !== \"/\" && // short-circuit for performance. most requests are relative URL.\n  (incomingUrl.startsWith(\"http://\") || incomingUrl.startsWith(\"https://\"))) {\n    if (incoming instanceof Http2ServerRequest) {\n      throw new RequestError(\"Absolute URL for :path is not allowed in HTTP/2\");\n    }\n    try {\n      const url2 = new URL(incomingUrl);\n      req[urlKey] = url2.href;\n    } catch (e2) {\n      throw new RequestError(\"Invalid absolute URL\", { cause: e2 });\n    }\n    return req;\n  }\n  const host = (incoming instanceof Http2ServerRequest ? incoming.authority : incoming.headers.host) || defaultHostname;\n  if (!host) {\n    throw new RequestError(\"Missing host header\");\n  }\n  let scheme;\n  if (incoming instanceof Http2ServerRequest) {\n    scheme = incoming.scheme;\n    if (!(scheme === \"http\" || scheme === \"https\")) {\n      throw new RequestError(\"Unsupported scheme\");\n    }\n  } else {\n    scheme = incoming.socket && incoming.socket.encrypted ? \"https\" : \"http\";\n  }\n  const url = new URL(`${scheme}://${host}${incomingUrl}`);\n  if (url.hostname.length !== host.length && url.hostname !== host.replace(/:\\d+$/, \"\")) {\n    throw new RequestError(\"Invalid host header\");\n  }\n  req[urlKey] = url.href;\n  return req;\n};\nvar responseCache = Symbol(\"responseCache\");\nvar getResponseCache = Symbol(\"getResponseCache\");\nvar cacheKey = Symbol(\"cache\");\nvar GlobalResponse = global.Response;\nvar Response2 = class _Response {\n  #body;\n  #init;\n  [getResponseCache]() {\n    delete this[cacheKey];\n    return this[responseCache] ||= new GlobalResponse(this.#body, this.#init);\n  }\n  constructor(body, init) {\n    let headers;\n    this.#body = body;\n    if (init instanceof _Response) {\n      const cachedGlobalResponse = init[responseCache];\n      if (cachedGlobalResponse) {\n        this.#init = cachedGlobalResponse;\n        this[getResponseCache]();\n        return;\n      } else {\n        this.#init = init.#init;\n        headers = new Headers(init.#init.headers);\n      }\n    } else {\n      this.#init = init;\n    }\n    if (typeof body === \"string\" || typeof body?.getReader !== \"undefined\" || body instanceof Blob || body instanceof Uint8Array) {\n      headers ||= init?.headers || { \"content-type\": \"text/plain; charset=UTF-8\" };\n      this[cacheKey] = [init?.status || 200, body, headers];\n    }\n  }\n  get headers() {\n    const cache = this[cacheKey];\n    if (cache) {\n      if (!(cache[2] instanceof Headers)) {\n        cache[2] = new Headers(cache[2]);\n      }\n      return cache[2];\n    }\n    return this[getResponseCache]().headers;\n  }\n  get status() {\n    return this[cacheKey]?.[0] ?? this[getResponseCache]().status;\n  }\n  get ok() {\n    const status = this.status;\n    return status >= 200 && status < 300;\n  }\n};\n[\"body\", \"bodyUsed\", \"redirected\", \"statusText\", \"trailers\", \"type\", \"url\"].forEach((k) => {\n  Object.defineProperty(Response2.prototype, k, {\n    get() {\n      return this[getResponseCache]()[k];\n    }\n  });\n});\n[\"arrayBuffer\", \"blob\", \"clone\", \"formData\", \"json\", \"text\"].forEach((k) => {\n  Object.defineProperty(Response2.prototype, k, {\n    value: function() {\n      return this[getResponseCache]()[k]();\n    }\n  });\n});\nObject.setPrototypeOf(Response2, GlobalResponse);\nObject.setPrototypeOf(Response2.prototype, GlobalResponse.prototype);\nfunction writeFromReadableStream(stream6, writable) {\n  if (stream6.locked) {\n    throw new TypeError(\"ReadableStream is locked.\");\n  } else if (writable.destroyed) {\n    return;\n  }\n  const reader = stream6.getReader();\n  const handleError2 = () => {\n  };\n  writable.on(\"error\", handleError2);\n  reader.read().then(flow, handleStreamError);\n  return reader.closed.finally(() => {\n    writable.off(\"error\", handleError2);\n  });\n  function handleStreamError(error) {\n    if (error) {\n      writable.destroy(error);\n    }\n  }\n  function onDrain() {\n    reader.read().then(flow, handleStreamError);\n  }\n  function flow({ done, value }) {\n    try {\n      if (done) {\n        writable.end();\n      } else if (!writable.write(value)) {\n        writable.once(\"drain\", onDrain);\n      } else {\n        return reader.read().then(flow, handleStreamError);\n      }\n    } catch (e2) {\n      handleStreamError(e2);\n    }\n  }\n}\nvar buildOutgoingHttpHeaders = (headers) => {\n  const res = {};\n  if (!(headers instanceof Headers)) {\n    headers = new Headers(headers ?? void 0);\n  }\n  const cookies = [];\n  for (const [k, v] of headers) {\n    if (k === \"set-cookie\") {\n      cookies.push(v);\n    } else {\n      res[k] = v;\n    }\n  }\n  if (cookies.length > 0) {\n    res[\"set-cookie\"] = cookies;\n  }\n  res[\"content-type\"] ??= \"text/plain; charset=UTF-8\";\n  return res;\n};\nvar X_ALREADY_SENT = \"x-hono-already-sent\";\nvar webFetch = global.fetch;\nif (typeof global.crypto === \"undefined\") {\n  global.crypto = crypto;\n}\nglobal.fetch = (info, init) => {\n  init = {\n    // Disable compression handling so people can return the result of a fetch\n    // directly in the loader without messing with the Content-Encoding header.\n    compress: false,\n    ...init\n  };\n  return webFetch(info, init);\n};\nvar outgoingEnded = Symbol(\"outgoingEnded\");\nvar regBuffer = /^no$/i;\nvar regContentType = /^(application\\/json\\b|text\\/(?!event-stream\\b))/i;\nvar handleRequestError = () => new Response(null, {\n  status: 400\n});\nvar handleFetchError = (e2) => new Response(null, {\n  status: e2 instanceof Error && (e2.name === \"TimeoutError\" || e2.constructor.name === \"TimeoutError\") ? 504 : 500\n});\nvar handleResponseError = (e2, outgoing) => {\n  const err = e2 instanceof Error ? e2 : new Error(\"unknown error\", { cause: e2 });\n  if (err.code === \"ERR_STREAM_PREMATURE_CLOSE\") {\n    console.info(\"The user aborted a request.\");\n  } else {\n    console.error(e2);\n    if (!outgoing.headersSent) {\n      outgoing.writeHead(500, { \"Content-Type\": \"text/plain\" });\n    }\n    outgoing.end(`Error: ${err.message}`);\n    outgoing.destroy(err);\n  }\n};\nvar flushHeaders = (outgoing) => {\n  if (\"flushHeaders\" in outgoing && outgoing.writable) {\n    outgoing.flushHeaders();\n  }\n};\nvar responseViaCache = async (res, outgoing) => {\n  let [status, body, header] = res[cacheKey];\n  if (header instanceof Headers) {\n    header = buildOutgoingHttpHeaders(header);\n  }\n  if (typeof body === \"string\") {\n    header[\"Content-Length\"] = Buffer.byteLength(body);\n  } else if (body instanceof Uint8Array) {\n    header[\"Content-Length\"] = body.byteLength;\n  } else if (body instanceof Blob) {\n    header[\"Content-Length\"] = body.size;\n  }\n  outgoing.writeHead(status, header);\n  if (typeof body === \"string\" || body instanceof Uint8Array) {\n    outgoing.end(body);\n  } else if (body instanceof Blob) {\n    outgoing.end(new Uint8Array(await body.arrayBuffer()));\n  } else {\n    flushHeaders(outgoing);\n    await writeFromReadableStream(body, outgoing)?.catch(\n      (e2) => handleResponseError(e2, outgoing)\n    );\n  }\n  outgoing[outgoingEnded]?.();\n};\nvar responseViaResponseObject = async (res, outgoing, options = {}) => {\n  if (res instanceof Promise) {\n    if (options.errorHandler) {\n      try {\n        res = await res;\n      } catch (err) {\n        const errRes = await options.errorHandler(err);\n        if (!errRes) {\n          return;\n        }\n        res = errRes;\n      }\n    } else {\n      res = await res.catch(handleFetchError);\n    }\n  }\n  if (cacheKey in res) {\n    return responseViaCache(res, outgoing);\n  }\n  const resHeaderRecord = buildOutgoingHttpHeaders(res.headers);\n  if (res.body) {\n    const {\n      \"transfer-encoding\": transferEncoding,\n      \"content-encoding\": contentEncoding,\n      \"content-length\": contentLength,\n      \"x-accel-buffering\": accelBuffering,\n      \"content-type\": contentType\n    } = resHeaderRecord;\n    if (transferEncoding || contentEncoding || contentLength || // nginx buffering variant\n    accelBuffering && regBuffer.test(accelBuffering) || !regContentType.test(contentType)) {\n      outgoing.writeHead(res.status, resHeaderRecord);\n      flushHeaders(outgoing);\n      await writeFromReadableStream(res.body, outgoing);\n    } else {\n      const buffer = await res.arrayBuffer();\n      resHeaderRecord[\"content-length\"] = buffer.byteLength;\n      outgoing.writeHead(res.status, resHeaderRecord);\n      outgoing.end(new Uint8Array(buffer));\n    }\n  } else if (resHeaderRecord[X_ALREADY_SENT]) ; else {\n    outgoing.writeHead(res.status, resHeaderRecord);\n    outgoing.end();\n  }\n  outgoing[outgoingEnded]?.();\n};\nvar getRequestListener = (fetchCallback, options = {}) => {\n  const autoCleanupIncoming = options.autoCleanupIncoming ?? true;\n  if (options.overrideGlobalObjects !== false && global.Request !== Request) {\n    Object.defineProperty(global, \"Request\", {\n      value: Request\n    });\n    Object.defineProperty(global, \"Response\", {\n      value: Response2\n    });\n  }\n  return async (incoming, outgoing) => {\n    let res, req;\n    try {\n      req = newRequest(incoming, options.hostname);\n      let incomingEnded = !autoCleanupIncoming || incoming.method === \"GET\" || incoming.method === \"HEAD\";\n      if (!incomingEnded) {\n        incoming[wrapBodyStream] = true;\n        incoming.on(\"end\", () => {\n          incomingEnded = true;\n        });\n        if (incoming instanceof Http2ServerRequest) {\n          outgoing[outgoingEnded] = () => {\n            if (!incomingEnded) {\n              setTimeout(() => {\n                if (!incomingEnded) {\n                  setTimeout(() => {\n                    incoming.destroy();\n                    outgoing.destroy();\n                  });\n                }\n              });\n            }\n          };\n        }\n      }\n      outgoing.on(\"close\", () => {\n        const abortController = req[abortControllerKey];\n        if (abortController) {\n          if (incoming.errored) {\n            req[abortControllerKey].abort(incoming.errored.toString());\n          } else if (!outgoing.writableFinished) {\n            req[abortControllerKey].abort(\"Client connection prematurely closed.\");\n          }\n        }\n        if (!incomingEnded) {\n          setTimeout(() => {\n            if (!incomingEnded) {\n              setTimeout(() => {\n                incoming.destroy();\n              });\n            }\n          });\n        }\n      });\n      res = fetchCallback(req, { incoming, outgoing });\n      if (cacheKey in res) {\n        return responseViaCache(res, outgoing);\n      }\n    } catch (e2) {\n      if (!res) {\n        if (options.errorHandler) {\n          res = await options.errorHandler(req ? e2 : toRequestError(e2));\n          if (!res) {\n            return;\n          }\n        } else if (!req) {\n          res = handleRequestError();\n        } else {\n          res = handleFetchError(e2);\n        }\n      } else {\n        return handleResponseError(e2, outgoing);\n      }\n    }\n    try {\n      return await responseViaResponseObject(res, outgoing, options);\n    } catch (e2) {\n      return handleResponseError(e2, outgoing);\n    }\n  };\n};\nvar createAdaptorServer = (options) => {\n  const fetchCallback = options.fetch;\n  const requestListener = getRequestListener(fetchCallback, {\n    hostname: options.hostname,\n    overrideGlobalObjects: options.overrideGlobalObjects,\n    autoCleanupIncoming: options.autoCleanupIncoming\n  });\n  const createServer$1 = options.createServer || createServer;\n  const server = createServer$1(options.serverOptions || {}, requestListener);\n  return server;\n};\nvar serve = (options, listeningListener) => {\n  const server = createAdaptorServer(options);\n  server.listen(options?.port ?? 3e3, options.hostname, () => {\n    const serverInfo = server.address();\n    listeningListener && listeningListener(serverInfo);\n  });\n  return server;\n};\nvar COMPRESSIBLE_CONTENT_TYPE_REGEX = /^\\s*(?:text\\/[^;\\s]+|application\\/(?:javascript|json|xml|xml-dtd|ecmascript|dart|postscript|rtf|tar|toml|vnd\\.dart|vnd\\.ms-fontobject|vnd\\.ms-opentype|wasm|x-httpd-php|x-javascript|x-ns-proxy-autoconfig|x-sh|x-tar|x-virtualbox-hdd|x-virtualbox-ova|x-virtualbox-ovf|x-virtualbox-vbox|x-virtualbox-vdi|x-virtualbox-vhd|x-virtualbox-vmdk|x-www-form-urlencoded)|font\\/(?:otf|ttf)|image\\/(?:bmp|vnd\\.adobe\\.photoshop|vnd\\.microsoft\\.icon|vnd\\.ms-dds|x-icon|x-ms-bmp)|message\\/rfc822|model\\/gltf-binary|x-shader\\/x-fragment|x-shader\\/x-vertex|[^;\\s]+?\\+(?:json|text|xml|yaml))(?:[;\\s]|$)/i;\nvar ENCODINGS = {\n  br: \".br\",\n  zstd: \".zst\",\n  gzip: \".gz\"\n};\nvar ENCODINGS_ORDERED_KEYS = Object.keys(ENCODINGS);\nvar createStreamBody = (stream6) => {\n  const body = new ReadableStream({\n    start(controller) {\n      stream6.on(\"data\", (chunk) => {\n        controller.enqueue(chunk);\n      });\n      stream6.on(\"end\", () => {\n        controller.close();\n      });\n    },\n    cancel() {\n      stream6.destroy();\n    }\n  });\n  return body;\n};\nvar getStats = (path) => {\n  let stats;\n  try {\n    stats = lstatSync(path);\n  } catch {\n  }\n  return stats;\n};\nvar serveStatic = (options = { root: \"\" }) => {\n  const root = options.root || \"\";\n  const optionPath = options.path;\n  return async (c2, next) => {\n    if (c2.finalized) {\n      return next();\n    }\n    let filename;\n    if (optionPath) {\n      filename = optionPath;\n    } else {\n      try {\n        filename = decodeURIComponent(c2.req.path);\n        if (/(?:^|[\\/\\\\])\\.\\.(?:$|[\\/\\\\])/.test(filename)) {\n          throw new Error();\n        }\n      } catch {\n        await options.onNotFound?.(c2.req.path, c2);\n        return next();\n      }\n    }\n    let path = join$1(\n      root,\n      !optionPath && options.rewriteRequestPath ? options.rewriteRequestPath(filename, c2) : filename\n    );\n    let stats = getStats(path);\n    if (stats && stats.isDirectory()) {\n      const indexFile = options.index ?? \"index.html\";\n      path = join$1(path, indexFile);\n      stats = getStats(path);\n    }\n    if (!stats) {\n      await options.onNotFound?.(path, c2);\n      return next();\n    }\n    await options.onFound?.(path, c2);\n    const mimeType = getMimeType(path);\n    c2.header(\"Content-Type\", mimeType || \"application/octet-stream\");\n    if (options.precompressed && (!mimeType || COMPRESSIBLE_CONTENT_TYPE_REGEX.test(mimeType))) {\n      const acceptEncodingSet = new Set(\n        c2.req.header(\"Accept-Encoding\")?.split(\",\").map((encoding) => encoding.trim())\n      );\n      for (const encoding of ENCODINGS_ORDERED_KEYS) {\n        if (!acceptEncodingSet.has(encoding)) {\n          continue;\n        }\n        const precompressedStats = getStats(path + ENCODINGS[encoding]);\n        if (precompressedStats) {\n          c2.header(\"Content-Encoding\", encoding);\n          c2.header(\"Vary\", \"Accept-Encoding\", { append: true });\n          stats = precompressedStats;\n          path = path + ENCODINGS[encoding];\n          break;\n        }\n      }\n    }\n    const size = stats.size;\n    if (c2.req.method == \"HEAD\" || c2.req.method == \"OPTIONS\") {\n      c2.header(\"Content-Length\", size.toString());\n      c2.status(200);\n      return c2.body(null);\n    }\n    const range = c2.req.header(\"range\") || \"\";\n    if (!range) {\n      c2.header(\"Content-Length\", size.toString());\n      return c2.body(createStreamBody(createReadStream(path)), 200);\n    }\n    c2.header(\"Accept-Ranges\", \"bytes\");\n    c2.header(\"Date\", stats.birthtime.toUTCString());\n    const parts = range.replace(/bytes=/, \"\").split(\"-\", 2);\n    const start = parts[0] ? parseInt(parts[0], 10) : 0;\n    let end = parts[1] ? parseInt(parts[1], 10) : stats.size - 1;\n    if (size < end - start + 1) {\n      end = size - 1;\n    }\n    const chunksize = end - start + 1;\n    const stream6 = createReadStream(path, { start, end });\n    c2.header(\"Content-Length\", chunksize.toString());\n    c2.header(\"Content-Range\", `bytes ${start}-${end}/${stats.size}`);\n    return c2.body(createStreamBody(stream6), 206);\n  };\n};\nvar RENDER_TYPE = {\n  STRING_ARRAY: \"string_array\",\n  STRING: \"string\",\n  JSON_STRING: \"json_string\",\n  RAW: \"raw\"\n};\nvar RENDER_TYPE_MAP = {\n  configUrl: RENDER_TYPE.STRING,\n  deepLinking: RENDER_TYPE.RAW,\n  presets: RENDER_TYPE.STRING_ARRAY,\n  plugins: RENDER_TYPE.STRING_ARRAY,\n  spec: RENDER_TYPE.JSON_STRING,\n  url: RENDER_TYPE.STRING,\n  urls: RENDER_TYPE.JSON_STRING,\n  layout: RENDER_TYPE.STRING,\n  docExpansion: RENDER_TYPE.STRING,\n  maxDisplayedTags: RENDER_TYPE.RAW,\n  operationsSorter: RENDER_TYPE.RAW,\n  requestInterceptor: RENDER_TYPE.RAW,\n  responseInterceptor: RENDER_TYPE.RAW,\n  persistAuthorization: RENDER_TYPE.RAW,\n  defaultModelsExpandDepth: RENDER_TYPE.RAW,\n  defaultModelExpandDepth: RENDER_TYPE.RAW,\n  defaultModelRendering: RENDER_TYPE.STRING,\n  displayRequestDuration: RENDER_TYPE.RAW,\n  filter: RENDER_TYPE.RAW,\n  showExtensions: RENDER_TYPE.RAW,\n  showCommonExtensions: RENDER_TYPE.RAW,\n  queryConfigEnabled: RENDER_TYPE.RAW,\n  displayOperationId: RENDER_TYPE.RAW,\n  tagsSorter: RENDER_TYPE.RAW,\n  onComplete: RENDER_TYPE.RAW,\n  syntaxHighlight: RENDER_TYPE.JSON_STRING,\n  tryItOutEnabled: RENDER_TYPE.RAW,\n  requestSnippetsEnabled: RENDER_TYPE.RAW,\n  requestSnippets: RENDER_TYPE.JSON_STRING,\n  oauth2RedirectUrl: RENDER_TYPE.STRING,\n  showMutabledRequest: RENDER_TYPE.RAW,\n  request: RENDER_TYPE.JSON_STRING,\n  supportedSubmitMethods: RENDER_TYPE.JSON_STRING,\n  validatorUrl: RENDER_TYPE.STRING,\n  withCredentials: RENDER_TYPE.RAW,\n  modelPropertyMacro: RENDER_TYPE.RAW,\n  parameterMacro: RENDER_TYPE.RAW\n};\nvar renderSwaggerUIOptions = (options) => {\n  const optionsStrings = Object.entries(options).map(([k, v]) => {\n    const key = k;\n    if (!RENDER_TYPE_MAP[key] || v === void 0) {\n      return \"\";\n    }\n    switch (RENDER_TYPE_MAP[key]) {\n      case RENDER_TYPE.STRING:\n        return `${key}: '${v}'`;\n      case RENDER_TYPE.STRING_ARRAY:\n        if (!Array.isArray(v)) {\n          return \"\";\n        }\n        return `${key}: [${v.map((ve) => `${ve}`).join(\",\")}]`;\n      case RENDER_TYPE.JSON_STRING:\n        return `${key}: ${JSON.stringify(v)}`;\n      case RENDER_TYPE.RAW:\n        return `${key}: ${v}`;\n      default:\n        return \"\";\n    }\n  }).filter((item) => item !== \"\").join(\",\");\n  return optionsStrings;\n};\nvar remoteAssets = ({ version }) => {\n  const url = `https://cdn.jsdelivr.net/npm/swagger-ui-dist${version !== void 0 ? `@${version}` : \"\"}`;\n  return {\n    css: [`${url}/swagger-ui.css`],\n    js: [`${url}/swagger-ui-bundle.js`]\n  };\n};\nvar SwaggerUI = (options) => {\n  const asset = remoteAssets({ version: options?.version });\n  delete options.version;\n  if (options.manuallySwaggerUIHtml) {\n    return options.manuallySwaggerUIHtml(asset);\n  }\n  const optionsStrings = renderSwaggerUIOptions(options);\n  return `\n    <div>\n      <div id=\"swagger-ui\"></div>\n      ${asset.css.map((url) => html`<link rel=\"stylesheet\" href=\"${url}\" />`)}\n      ${asset.js.map((url) => html`<script src=\"${url}\" crossorigin=\"anonymous\"></script>`)}\n      <script>\n        window.onload = () => {\n          window.ui = SwaggerUIBundle({\n            dom_id: '#swagger-ui',${optionsStrings},\n          })\n        }\n      </script>\n    </div>\n  `;\n};\nvar middleware = (options) => async (c2) => {\n  const title = options?.title ?? \"SwaggerUI\";\n  return c2.html(\n    /* html */\n    `\n      <html lang=\"en\">\n        <head>\n          <meta charset=\"utf-8\" />\n          <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n          <meta name=\"description\" content=\"SwaggerUI\" />\n          <title>${title}</title>\n        </head>\n        <body>\n          ${SwaggerUI(options)}\n        </body>\n      </html>\n    `\n  );\n};\n\n// ../../node_modules/.pnpm/hono-openapi@0.4.8_hono@4.8.12_openapi-types@12.1.3_zod@3.25.76/node_modules/hono-openapi/utils.js\nvar e = Symbol(\"openapi\");\nvar n = [\"GET\", \"PUT\", \"POST\", \"DELETE\", \"OPTIONS\", \"HEAD\", \"PATCH\", \"TRACE\"];\nvar s2 = (e2) => e2.charAt(0).toUpperCase() + e2.slice(1);\nvar o = /* @__PURE__ */ new Map();\nvar a = (e2, t2) => {\n  const n2 = `${e2}:${t2}`;\n  if (o.has(n2)) return o.get(n2);\n  let a2 = e2;\n  if (\"/\" === t2) return `${a2}Index`;\n  for (const e3 of t2.split(\"/\")) 123 === e3.charCodeAt(0) ? a2 += `By${s2(e3.slice(1, -1))}` : a2 += s2(e3);\n  return o.set(n2, a2), a2;\n};\nvar r = /* @__PURE__ */ new Map();\nfunction c(e2, t2, n2) {\n  return e2 && t2 in e2 ? e2[t2] ?? n2 : n2;\n}\nfunction i(...e2) {\n  return e2.reduce((e3, t2) => {\n    if (!t2) return e3;\n    let n2;\n    return (\"tags\" in e3 && e3.tags || \"tags\" in t2 && t2.tags) && (n2 = Array.from(/* @__PURE__ */ new Set([...c(e3, \"tags\", []), ...c(t2, \"tags\", [])]))), { ...e3, ...t2, tags: n2, responses: { ...c(e3, \"responses\", {}), ...c(t2, \"responses\", {}) }, parameters: m(e3.parameters, t2.parameters) };\n  }, {});\n}\nfunction p({ path: e2, method: t2, data: n2, schema: s3 }) {\n  e2 = ((e3) => e3.split(\"/\").map((e4) => {\n    let t3 = e4;\n    if (t3.startsWith(\":\")) {\n      const e5 = t3.match(/^:([^{?]+)(?:{(.+)})?(\\?)?$/);\n      e5 ? t3 = `{${e5[1]}}` : (t3 = t3.slice(1, t3.length), t3.endsWith(\"?\") && (t3 = t3.slice(0, -1)), t3 = `{${t3}}`);\n    }\n    return t3;\n  }).join(\"/\"))(e2);\n  const o2 = t2.toLowerCase();\n  if (\"all\" === o2) {\n    if (!n2) return;\n    if (r.has(e2)) {\n      const t3 = r.get(e2) ?? {};\n      r.set(e2, { ...t3, ...n2, parameters: m(t3.parameters, n2.parameters) });\n    } else r.set(e2, n2);\n  } else {\n    const t3 = function(e3) {\n      const t4 = Array.from(r.keys());\n      let n3 = {};\n      for (const s4 of t4) e3.match(s4) && (n3 = i(n3, r.get(s4) ?? {}));\n      return n3;\n    }(e2);\n    s3[e2] = { ...s3[e2] ? s3[e2] : {}, [o2]: { responses: {}, operationId: a(o2, e2), ...i(t3, s3[e2]?.[o2], n2) } };\n  }\n}\nvar f = (e2) => \"$ref\" in e2 ? e2.$ref : `${e2.in} ${e2.name}`;\nfunction m(...e2) {\n  const t2 = e2.flatMap((e3) => e3 ?? []).reduce((e3, t3) => (e3.set(f(t3), t3), e3), /* @__PURE__ */ new Map());\n  return Array.from(t2.values());\n}\nfunction l(e2, { excludeStaticFile: t2 = true, exclude: n2 = [] }) {\n  const s3 = {}, o2 = Array.isArray(n2) ? n2 : [n2];\n  for (const [n3, a2] of Object.entries(e2)) if (!o2.some((e3) => \"string\" == typeof e3 ? n3 === e3 : e3.test(n3)) && (!n3.includes(\"*\") || n3.includes(\"{\")) && (!t2 || (!n3.includes(\".\") || n3.includes(\"{\")))) {\n    for (const e3 of Object.keys(a2)) {\n      const t3 = a2[e3];\n      if (n3.includes(\"{\")) {\n        t3.parameters || (t3.parameters = []);\n        const e4 = n3.split(\"/\").filter((e5) => e5.startsWith(\"{\") && !t3.parameters.find((t4) => \"path\" === t4.in && t4.name === e5.slice(1, e5.length - 1)));\n        for (const n4 of e4) {\n          const e5 = n4.slice(1, n4.length - 1), s4 = t3.parameters.findIndex((t4) => \"param\" === t4.in && t4.name === e5);\n          -1 !== s4 ? t3.parameters[s4].in = \"path\" : t3.parameters.push({ schema: { type: \"string\" }, in: \"path\", name: e5, required: true });\n        }\n      }\n      t3.responses || (t3.responses = { 200: {} });\n    }\n    s3[n3] = a2;\n  }\n  return s3;\n}\nvar u = { documentation: {}, excludeStaticFile: true, exclude: [], excludeMethods: [\"OPTIONS\"], excludeTags: [] };\nvar d = { version: \"3.1.0\", components: {} };\nfunction h(e2, t2) {\n  const n2 = { version: \"3.1.0\", components: {} };\n  let s3;\n  return async (o2) => (s3 || (s3 = await y(e2, t2, n2, o2)), o2.json(s3));\n}\nasync function y(t2, s3 = u, o2 = d, a2) {\n  const r2 = { ...u, ...s3 }, c2 = { ...d, ...o2 }, i2 = r2.documentation ?? {}, f2 = await async function(t3, s4, o3) {\n    const a3 = {};\n    for (const r3 of t3.routes) {\n      if (!(e in r3.handler)) {\n        s4.includeEmptyPaths && p({ method: r3.method, path: r3.path, schema: a3 });\n        continue;\n      }\n      if (s4.excludeMethods.includes(r3.method)) continue;\n      if (false === n.includes(r3.method) && \"ALL\" !== r3.method) continue;\n      const { resolver: t4, metadata: c3 = {} } = r3.handler[e], i3 = s4.defaultOptions?.[r3.method], { docs: f3, components: m2 } = await t4({ ...o3, ...c3 }, i3);\n      o3.components = { ...o3.components, ...m2 ?? {} }, p({ method: r3.method, path: r3.path, data: f3, schema: a3 });\n    }\n    return a3;\n  }(t2, r2, c2);\n  for (const e2 in f2) for (const t3 in f2[e2]) {\n    const n2 = f2[e2][t3]?.hide;\n    if (n2) {\n      let s4 = false;\n      \"boolean\" == typeof n2 ? s4 = n2 : \"function\" == typeof n2 && (a2 ? s4 = n2(a2) : console.warn(`'c' is not defined, cannot evaluate hide function for ${t3} ${e2}`)), s4 && delete f2[e2][t3];\n    }\n  }\n  return { openapi: c2.version, ...{ ...i2, tags: i2.tags?.filter((e2) => !r2.excludeTags?.includes(e2?.name)), info: { title: \"Hono Documentation\", description: \"Development documentation\", version: \"0.0.0\", ...i2.info }, paths: { ...l(f2, r2), ...i2.paths }, components: { ...i2.components, schemas: { ...c2.components, ...i2.components?.schemas } } } };\n}\nfunction w(n2) {\n  const { validateResponse: s3, ...o2 } = n2;\n  return Object.assign(async (e2, o3) => {\n    if (await o3(), s3 && n2.responses) {\n      const o4 = e2.res.status, a2 = e2.res.headers.get(\"content-type\");\n      if (o4 && a2) {\n        const r2 = n2.responses[o4];\n        if (r2 && \"content\" in r2 && r2.content) {\n          const n3 = a2.split(\";\")[0], o5 = r2.content[n3];\n          if (o5?.schema && \"validator\" in o5.schema) try {\n            let t2;\n            const s4 = e2.res.clone();\n            if (\"application/json\" === n3 ? t2 = await s4.json() : \"text/plain\" === n3 && (t2 = await s4.text()), !t2) throw new Error(\"No data to validate!\");\n            await o5.schema.validator(t2);\n          } catch (e3) {\n            let n4 = { status: 500, message: \"Response validation failed!\" };\n            throw \"object\" == typeof s3 && (n4 = { ...n4, ...s3 }), new HTTPException(n4.status, { message: n4.message, cause: e3 });\n          }\n        }\n      }\n    }\n  }, { [e]: { resolver: (e2, t2) => x(e2, o2, t2) } });\n}\nasync function x(e2, t2, n2 = {}) {\n  let s3 = {};\n  const o2 = { ...n2, ...t2, responses: { ...n2?.responses, ...t2.responses } };\n  if (o2.responses) for (const t3 of Object.keys(o2.responses)) {\n    const n3 = o2.responses[t3];\n    if (n3 && \"content\" in n3) for (const t4 of Object.keys(n3.content ?? {})) {\n      const o3 = n3.content?.[t4];\n      if (o3 && (o3.schema && \"builder\" in o3.schema)) {\n        const t5 = await o3.schema.builder(e2);\n        o3.schema = t5.schema, t5.components && (s3 = { ...s3, ...t5.components });\n      }\n    }\n  }\n  return { docs: o2, components: s3 };\n}\nasync function getAgentCardByIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const agentId = c2.req.param(\"agentId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const result = await getAgentCardByIdHandler$1({\n    mastra,\n    agentId,\n    runtimeContext\n  });\n  return c2.json(result);\n}\nasync function getAgentExecutionHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const agentId = c2.req.param(\"agentId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const taskStore = c2.get(\"taskStore\");\n  const logger2 = mastra.getLogger();\n  const body = await c2.req.json();\n  if (![\"message/send\", \"message/stream\", \"tasks/get\", \"tasks/cancel\"].includes(body.method)) {\n    return c2.json({ error: { message: `Unsupported method: ${body.method}`, code: \"invalid_method\" } }, 400);\n  }\n  const result = await getAgentExecutionHandler$1({\n    mastra,\n    agentId,\n    runtimeContext,\n    requestId: randomUUID(),\n    method: body.method,\n    params: body.params,\n    taskStore,\n    logger: logger2\n  });\n  if (body.method === \"message/stream\") {\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          stream6.onAbort(() => {\n            if (!result.locked) {\n              return result.cancel();\n            }\n          });\n          for await (const chunk of result) {\n            await stream6.write(JSON.stringify(chunk) + \"\u001e\");\n          }\n        } catch (err) {\n          logger2.error(\"Error in message/stream stream: \" + err?.message);\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in message/stream stream: \" + err?.message);\n      }\n    );\n  }\n  return c2.json(result);\n}\n\n// src/server/handlers/auth/defaults.ts\nvar defaultAuthConfig = {\n  protected: [\"/api/*\"],\n  // Simple rule system\n  rules: [\n    // Admin users can do anything\n    {\n      condition: (user) => {\n        if (typeof user === \"object\" && user !== null) {\n          if (\"isAdmin\" in user) {\n            return !!user.isAdmin;\n          }\n          if (\"role\" in user) {\n            return user.role === \"admin\";\n          }\n        }\n        return false;\n      },\n      allow: true\n    }\n  ]\n};\n\n// src/server/handlers/auth/helpers.ts\nvar isDevPlaygroundRequest = (req) => {\n  return req.header(\"x-mastra-dev-playground\") === \"true\" && process.env.MASTRA_DEV === \"true\";\n};\nvar isProtectedPath = (path, method, authConfig) => {\n  const protectedAccess = [...defaultAuthConfig.protected || [], ...authConfig.protected || []];\n  return isAnyMatch(path, method, protectedAccess);\n};\nvar canAccessPublicly = (path, method, authConfig) => {\n  const publicAccess = [...defaultAuthConfig.public || [], ...authConfig.public || []];\n  return isAnyMatch(path, method, publicAccess);\n};\nvar isAnyMatch = (path, method, patterns) => {\n  if (!patterns) {\n    return false;\n  }\n  for (const patternPathOrMethod of patterns) {\n    if (patternPathOrMethod instanceof RegExp) {\n      if (patternPathOrMethod.test(path)) {\n        return true;\n      }\n    }\n    if (typeof patternPathOrMethod === \"string\" && pathMatchesPattern(path, patternPathOrMethod)) {\n      return true;\n    }\n    if (Array.isArray(patternPathOrMethod) && patternPathOrMethod.length === 2) {\n      const [pattern, methodOrMethods] = patternPathOrMethod;\n      if (pathMatchesPattern(path, pattern) && matchesOrIncludes(methodOrMethods, method)) {\n        return true;\n      }\n    }\n  }\n  return false;\n};\nvar pathMatchesPattern = (path, pattern) => {\n  if (pattern.endsWith(\"*\")) {\n    const prefix = pattern.slice(0, -1);\n    return path.startsWith(prefix);\n  }\n  return path === pattern;\n};\nvar pathMatchesRule = (path, rulePath) => {\n  if (!rulePath) return true;\n  if (typeof rulePath === \"string\") {\n    return pathMatchesPattern(path, rulePath);\n  }\n  if (rulePath instanceof RegExp) {\n    console.log(\"rulePath\", rulePath, path, rulePath.test(path));\n    return rulePath.test(path);\n  }\n  if (Array.isArray(rulePath)) {\n    return rulePath.some((p2) => pathMatchesPattern(path, p2));\n  }\n  return false;\n};\nvar matchesOrIncludes = (values, value) => {\n  if (typeof values === \"string\") {\n    return values === value;\n  }\n  if (Array.isArray(values)) {\n    return values.includes(value);\n  }\n  return false;\n};\nvar checkRules = async (rules, path, method, user) => {\n  for (const i2 in rules || []) {\n    const rule = rules?.[i2];\n    if (!pathMatchesRule(path, rule.path)) {\n      continue;\n    }\n    if (rule.methods && !matchesOrIncludes(rule.methods, method)) {\n      continue;\n    }\n    const condition = rule.condition;\n    if (typeof condition === \"function\") {\n      const allowed = await Promise.resolve().then(() => condition(user)).catch(() => false);\n      if (allowed) {\n        return true;\n      }\n    } else if (rule.allow) {\n      return true;\n    }\n  }\n  return false;\n};\n\n// src/server/handlers/auth/index.ts\nvar authenticationMiddleware = async (c2, next) => {\n  const mastra = c2.get(\"mastra\");\n  const authConfig = mastra.getServer()?.experimental_auth;\n  if (!authConfig) {\n    return next();\n  }\n  if (isDevPlaygroundRequest(c2.req)) {\n    return next();\n  }\n  if (!isProtectedPath(c2.req.path, c2.req.method, authConfig)) {\n    return next();\n  }\n  if (canAccessPublicly(c2.req.path, c2.req.method, authConfig)) {\n    return next();\n  }\n  const authHeader = c2.req.header(\"Authorization\");\n  let token = authHeader ? authHeader.replace(\"Bearer \", \"\") : null;\n  if (!token && c2.req.query(\"apiKey\")) {\n    token = c2.req.query(\"apiKey\") || null;\n  }\n  if (!token) {\n    return c2.json({ error: \"Authentication required\" }, 401);\n  }\n  try {\n    let user;\n    if (typeof authConfig.authenticateToken === \"function\") {\n      user = await authConfig.authenticateToken(token, c2.req);\n    } else {\n      throw new Error(\"No token verification method configured\");\n    }\n    if (!user) {\n      return c2.json({ error: \"Invalid or expired token\" }, 401);\n    }\n    c2.get(\"runtimeContext\").set(\"user\", user);\n    return next();\n  } catch (err) {\n    console.error(err);\n    return c2.json({ error: \"Invalid or expired token\" }, 401);\n  }\n};\nvar authorizationMiddleware = async (c2, next) => {\n  const mastra = c2.get(\"mastra\");\n  const authConfig = mastra.getServer()?.experimental_auth;\n  if (!authConfig) {\n    return next();\n  }\n  const path = c2.req.path;\n  const method = c2.req.method;\n  if (isDevPlaygroundRequest(c2.req)) {\n    return next();\n  }\n  if (!isProtectedPath(c2.req.path, c2.req.method, authConfig)) {\n    return next();\n  }\n  if (canAccessPublicly(path, method, authConfig)) {\n    return next();\n  }\n  const user = c2.get(\"runtimeContext\").get(\"user\");\n  if (\"authorizeUser\" in authConfig && typeof authConfig.authorizeUser === \"function\") {\n    try {\n      const isAuthorized = await authConfig.authorizeUser(user, c2.req);\n      if (isAuthorized) {\n        return next();\n      }\n      return c2.json({ error: \"Access denied\" }, 403);\n    } catch (err) {\n      console.error(err);\n      return c2.json({ error: \"Authorization error\" }, 500);\n    }\n  }\n  if (\"authorize\" in authConfig && typeof authConfig.authorize === \"function\") {\n    try {\n      const isAuthorized = await authConfig.authorize(path, method, user, c2);\n      if (isAuthorized) {\n        return next();\n      }\n      return c2.json({ error: \"Access denied\" }, 403);\n    } catch (err) {\n      console.error(err);\n      return c2.json({ error: \"Authorization error\" }, 500);\n    }\n  }\n  if (\"rules\" in authConfig && authConfig.rules && authConfig.rules.length > 0) {\n    const isAuthorized = await checkRules(authConfig.rules, path, method, user);\n    if (isAuthorized) {\n      return next();\n    }\n    return c2.json({ error: \"Access denied\" }, 403);\n  }\n  if (defaultAuthConfig.rules && defaultAuthConfig.rules.length > 0) {\n    const isAuthorized = await checkRules(defaultAuthConfig.rules, path, method, user);\n    if (isAuthorized) {\n      return next();\n    }\n  }\n  return c2.json({ error: \"Access denied\" }, 403);\n};\n\n// src/server/handlers/client.ts\nvar clients = /* @__PURE__ */ new Set();\nfunction handleClientsRefresh(c2) {\n  const stream6 = new ReadableStream({\n    start(controller) {\n      clients.add(controller);\n      controller.enqueue(\"data: connected\\n\\n\");\n      c2.req.raw.signal.addEventListener(\"abort\", () => {\n        clients.delete(controller);\n      });\n    }\n  });\n  return new Response(stream6, {\n    headers: {\n      \"Content-Type\": \"text/event-stream\",\n      \"Cache-Control\": \"no-cache\",\n      Connection: \"keep-alive\",\n      \"Access-Control-Allow-Origin\": \"*\"\n    }\n  });\n}\nfunction handleTriggerClientsRefresh(c2) {\n  clients.forEach((controller) => {\n    try {\n      controller.enqueue(\"data: refresh\\n\\n\");\n    } catch {\n      clients.delete(controller);\n    }\n  });\n  return c2.json({ success: true, clients: clients.size });\n}\nfunction handleError(error, defaultMessage) {\n  const apiError = error;\n  throw new HTTPException(apiError.status || 500, {\n    message: apiError.message || defaultMessage,\n    cause: apiError.cause\n  });\n}\nfunction errorHandler(err, c2, isDev) {\n  if (err instanceof HTTPException) {\n    if (isDev) {\n      return c2.json({ error: err.message, cause: err.cause, stack: err.stack }, err.status);\n    }\n    return c2.json({ error: err.message }, err.status);\n  }\n  console.error(err);\n  return c2.json({ error: \"Internal Server Error\" }, 500);\n}\n\n// src/server/handlers/root.ts\nasync function rootHandler(c2) {\n  return c2.text(\"Hello to the Mastra API!\");\n}\nvar AllowedProviderKeys = {\n  openai: \"OPENAI_API_KEY\",\n  xai: \"XAI_API_KEY\",\n  anthropic: \"ANTHROPIC_API_KEY\",\n  google: \"GOOGLE_GENERATIVE_AI_API_KEY\",\n  groq: \"GROQ_API_KEY\"\n};\n\n// src/server/handlers/routes/agents/handlers.ts\nvar vNextBodyOptions = {\n  messages: {\n    type: \"array\",\n    items: { type: \"object\" }\n  },\n  threadId: { type: \"string\" },\n  resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n  runId: { type: \"string\" },\n  output: { type: \"object\" },\n  instructions: { type: \"string\", description: \"Optional instructions to override the agent's default instructions\" },\n  context: {\n    type: \"array\",\n    items: { type: \"object\" },\n    description: \"Additional context messages to include\"\n  },\n  memory: {\n    type: \"object\",\n    properties: {\n      threadId: { type: \"string\" },\n      resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n      options: { type: \"object\", description: \"Memory configuration options\" }\n    },\n    description: \"Memory options for the conversation\"\n  },\n  savePerStep: { type: \"boolean\", description: \"Whether to save messages incrementally on step finish\" },\n  format: { type: \"string\", enum: [\"mastra\", \"aisdk\"], description: \"Response format\" },\n  toolChoice: {\n    oneOf: [\n      { type: \"string\", enum: [\"auto\", \"none\", \"required\"] },\n      { type: \"object\", properties: { type: { type: \"string\" }, toolName: { type: \"string\" } } }\n    ],\n    description: \"Controls how tools are selected during generation\"\n  },\n  modelSettings: {\n    type: \"object\",\n    properties: {\n      maxTokens: { type: \"number\", description: \"Maximum number of tokens to generate\" },\n      temperature: { type: \"number\", minimum: 0, maximum: 1, description: \"Temperature setting for randomness (0-1)\" },\n      topP: { type: \"number\", minimum: 0, maximum: 1, description: \"Nucleus sampling (0-1)\" },\n      topK: { type: \"number\", description: \"Only sample from the top K options for each subsequent token\" },\n      presencePenalty: { type: \"number\", minimum: -1, maximum: 1, description: \"Presence penalty (-1 to 1)\" },\n      frequencyPenalty: { type: \"number\", minimum: -1, maximum: 1, description: \"Frequency penalty (-1 to 1)\" },\n      stopSequences: { type: \"array\", items: { type: \"string\" }, description: \"Stop sequences for text generation\" },\n      seed: { type: \"number\", description: \"Seed for deterministic results\" },\n      maxRetries: { type: \"number\", description: \"Maximum number of retries\" },\n      headers: { type: \"object\", description: \"Additional HTTP headers\" }\n    },\n    description: \"Model settings for generation\"\n  }\n};\nasync function getAgentsHandler(c2) {\n  const serializedAgents = await getAgentsHandler$1({\n    mastra: c2.get(\"mastra\"),\n    runtimeContext: c2.get(\"runtimeContext\")\n  });\n  return c2.json(serializedAgents);\n}\nasync function getAgentByIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const agentId = c2.req.param(\"agentId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const isPlayground = c2.req.header(\"x-mastra-dev-playground\") === \"true\";\n  const result = await getAgentByIdHandler$1({\n    mastra,\n    agentId,\n    runtimeContext,\n    isPlayground\n  });\n  return c2.json(result);\n}\nasync function getEvalsByAgentIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const agentId = c2.req.param(\"agentId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const result = await getEvalsByAgentIdHandler$1({\n    mastra,\n    agentId,\n    runtimeContext\n  });\n  return c2.json(result);\n}\nasync function getLiveEvalsByAgentIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const agentId = c2.req.param(\"agentId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const result = await getLiveEvalsByAgentIdHandler$1({\n    mastra,\n    agentId,\n    runtimeContext\n  });\n  return c2.json(result);\n}\nasync function generateLegacyHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const result = await generateLegacyHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error generating from agent\");\n  }\n}\nasync function generateHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const result = await generateHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error generating from agent\");\n  }\n}\nasync function generateVNextHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const result = await generateVNextHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error generating vnext from agent\");\n  }\n}\nasync function streamGenerateLegacyHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const streamResponse = await streamGenerateLegacyHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"Error streaming from agent\");\n  }\n}\nasync function streamGenerateHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const streamResponse = await streamGenerateHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"Error streaming from agent\");\n  }\n}\nasync function streamVNextGenerateHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const logger2 = mastra.getLogger();\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const streamResponse = await streamVNextGenerateHandler$1({\n            mastra,\n            agentId,\n            runtimeContext,\n            body,\n            abortSignal: c2.req.raw.signal\n          });\n          const reader = streamResponse.fullStream.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(`data: ${JSON.stringify(chunkResult.value)}\n\n`);\n          }\n          await stream6.write(\"data: [DONE]\\n\\n\");\n        } catch (err) {\n          logger2.error(\"Error in streamVNext generate: \" + (err?.message ?? \"Unknown error\"));\n        }\n        await stream6.close();\n      },\n      async (err) => {\n        logger2.error(\"Error in watch stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error streaming from agent\");\n  }\n}\nasync function streamVNextUIMessageHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const streamResponse = await streamVNextUIMessageHandler$1({\n      mastra,\n      agentId,\n      runtimeContext,\n      body,\n      abortSignal: c2.req.raw.signal\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"Error streaming ui message from agent\");\n  }\n}\nasync function setAgentInstructionsHandler(c2) {\n  try {\n    const isPlayground = c2.get(\"playground\") === true;\n    if (!isPlayground) {\n      return c2.json({ error: \"This API is only available in the playground environment\" }, 403);\n    }\n    const agentId = c2.req.param(\"agentId\");\n    const { instructions } = await c2.req.json();\n    if (!agentId || !instructions) {\n      return c2.json({ error: \"Missing required fields\" }, 400);\n    }\n    const mastra = c2.get(\"mastra\");\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      return c2.json({ error: \"Agent not found\" }, 404);\n    }\n    agent.__updateInstructions(instructions);\n    return c2.json(\n      {\n        instructions\n      },\n      200\n    );\n  } catch (error) {\n    return handleError(error, \"Error setting agent instructions\");\n  }\n}\nasync function updateAgentModelHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const body = await c2.req.json();\n    const result = await updateAgentModelHandler$1({\n      mastra,\n      agentId,\n      body\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error updating agent model\");\n  }\n}\nasync function deprecatedStreamVNextHandler(c2) {\n  return c2.json(\n    {\n      error: \"This endpoint is deprecated\",\n      message: \"The /streamVNext endpoint has been deprecated. Please use an alternative streaming endpoint.\",\n      deprecated_endpoint: \"/api/agents/:agentId/streamVNext\",\n      replacement_endpoint: \"/api/agents/:agentId/stream/vnext\"\n    },\n    410\n    // 410 Gone status code for deprecated endpoints\n  );\n}\nasync function getModelProvidersHandler(c2) {\n  const isPlayground = c2.get(\"playground\") === true;\n  if (!isPlayground) {\n    return c2.json({ error: \"This API is only available in the playground environment\" }, 403);\n  }\n  const envVars = process.env;\n  const providers = Object.entries(AllowedProviderKeys);\n  const envKeys = Object.keys(envVars);\n  const availableProviders = providers.filter(([_, value]) => envKeys.includes(value) && !!envVars[value]);\n  const availableProvidersNames = availableProviders.map(([key]) => key);\n  return c2.json(availableProvidersNames);\n}\nasync function generateSystemPromptHandler(c2) {\n  try {\n    const agentId = c2.req.param(\"agentId\");\n    const isPlayground = c2.get(\"playground\") === true;\n    if (!isPlayground) {\n      return c2.json({ error: \"This API is only available in the playground environment\" }, 403);\n    }\n    const { instructions, comment } = await c2.req.json();\n    if (!instructions) {\n      return c2.json({ error: \"Missing instructions in request body\" }, 400);\n    }\n    const mastra = c2.get(\"mastra\");\n    const agent = mastra.getAgent(agentId);\n    if (!agent) {\n      return c2.json({ error: \"Agent not found\" }, 404);\n    }\n    let evalSummary = \"\";\n    try {\n      const testEvals = await mastra.getStorage()?.getEvalsByAgentName?.(agent.name, \"test\") || [];\n      const liveEvals = await mastra.getStorage()?.getEvalsByAgentName?.(agent.name, \"live\") || [];\n      const evalsMapped = [...testEvals, ...liveEvals].filter(\n        ({ instructions: evalInstructions }) => evalInstructions === instructions\n      );\n      evalSummary = evalsMapped.map(\n        ({ input, output, result: result2 }) => `\n          Input: ${input}\n\n          Output: ${output}\n\n          Result: ${JSON.stringify(result2)}\n\n        `\n      ).join(\"\");\n    } catch (error) {\n      mastra.getLogger().error(`Error fetching evals`, { error });\n    }\n    const ENHANCE_SYSTEM_PROMPT_INSTRUCTIONS = `\n            You are an expert system prompt engineer, specialized in analyzing and enhancing instructions to create clear, effective, and comprehensive system prompts. Your goal is to help users transform their basic instructions into well-structured system prompts that will guide AI behavior effectively.\n            Follow these steps to analyze and enhance the instructions:\n            1. ANALYSIS PHASE\n            - Identify the core purpose and goals\n            - Extract key constraints and requirements\n            - Recognize domain-specific terminology and concepts\n            - Note any implicit assumptions that should be made explicit\n            2. PROMPT STRUCTURE\n            Create a system prompt with these components:\n            a) ROLE DEFINITION\n                - Clear statement of the AI's role and purpose\n                - Key responsibilities and scope\n                - Primary stakeholders and users\n            b) CORE CAPABILITIES\n                - Main functions and abilities\n                - Specific domain knowledge required\n                - Tools and resources available\n            c) BEHAVIORAL GUIDELINES\n                - Communication style and tone\n                - Decision-making framework\n                - Error handling approach\n                - Ethical considerations\n            d) CONSTRAINTS & BOUNDARIES\n                - Explicit limitations\n                - Out-of-scope activities\n                - Security and privacy considerations\n            e) SUCCESS CRITERIA\n                - Quality standards\n                - Expected outcomes\n                - Performance metrics\n            3. QUALITY CHECKS\n            Ensure the prompt is:\n            - Clear and unambiguous\n            - Comprehensive yet concise\n            - Properly scoped\n            - Technically accurate\n            - Ethically sound\n            4. OUTPUT FORMAT\n            Return a structured response with:\n            - Enhanced system prompt\n            - Analysis of key components\n            - Identified goals and constraints\n            - Core domain concepts\n            Remember: A good system prompt should be specific enough to guide behavior but flexible enough to handle edge cases. \n            Focus on creating prompts that are clear, actionable, and aligned with the intended use case.\n        `;\n    const systemPromptAgent = new Agent({\n      name: \"system-prompt-enhancer\",\n      instructions: ENHANCE_SYSTEM_PROMPT_INSTRUCTIONS,\n      model: agent.llm?.getModel()\n    });\n    const result = await systemPromptAgent.generate(\n      `\n            We need to improve the system prompt. \n            Current: ${instructions}\n            ${comment ? `User feedback: ${comment}` : \"\"}\n            ${evalSummary ? `\nEvaluation Results:\n${evalSummary}` : \"\"}\n        `,\n      {\n        output: z.object({\n          new_prompt: z.string(),\n          explanation: z.string()\n        })\n      }\n    );\n    return c2.json(result?.object || {});\n  } catch (error) {\n    return handleError(error, \"Error generating system prompt\");\n  }\n}\nasync function getToolsHandler(c2) {\n  try {\n    const tools = c2.get(\"tools\");\n    const result = await getToolsHandler$1({\n      tools\n    });\n    return c2.json(result || {});\n  } catch (error) {\n    return handleError(error, \"Error getting tools\");\n  }\n}\nasync function getToolByIdHandler(c2) {\n  try {\n    const tools = c2.get(\"tools\");\n    const toolId = c2.req.param(\"toolId\");\n    const result = await getToolByIdHandler$1({\n      tools,\n      toolId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting tool\");\n  }\n}\nfunction executeToolHandler(tools) {\n  return async (c2) => {\n    try {\n      const mastra = c2.get(\"mastra\");\n      const runtimeContext = c2.get(\"runtimeContext\");\n      const toolId = decodeURIComponent(c2.req.param(\"toolId\"));\n      const runId = c2.req.query(\"runId\");\n      const { data } = await c2.req.json();\n      const result = await executeToolHandler$1(tools)({\n        mastra,\n        toolId,\n        data,\n        runtimeContext,\n        runId\n      });\n      return c2.json(result);\n    } catch (error) {\n      return handleError(error, \"Error executing tool\");\n    }\n  };\n}\nasync function getAgentToolHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const agentId = c2.req.param(\"agentId\");\n    const toolId = c2.req.param(\"toolId\");\n    const result = await getAgentToolHandler$1({\n      mastra,\n      agentId,\n      toolId,\n      runtimeContext\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting agent tool\");\n  }\n}\nasync function executeAgentToolHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const agentId = c2.req.param(\"agentId\");\n    const toolId = c2.req.param(\"toolId\");\n    const { data } = await c2.req.json();\n    const result = await executeAgentToolHandler$1({\n      mastra,\n      agentId,\n      toolId,\n      data,\n      runtimeContext\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error executing tool\");\n  }\n}\nasync function getSpeakersHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const speakers = await getSpeakersHandler$1({\n      mastra,\n      agentId\n    });\n    return c2.json(speakers);\n  } catch (error) {\n    return handleError(error, \"Error getting speakers\");\n  }\n}\nasync function speakHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const { input, options } = await c2.req.json();\n    const audioStream = await generateSpeechHandler({\n      mastra,\n      agentId,\n      body: { text: input, speakerId: options?.speakerId }\n    });\n    c2.header(\"Content-Type\", `audio/${options?.filetype ?? \"mp3\"}`);\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return c2.body(audioStream);\n  } catch (error) {\n    return handleError(error, \"Error generating speech\");\n  }\n}\nasync function getListenerHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const listeners = await getListenerHandler$1({\n      mastra,\n      agentId\n    });\n    return c2.json(listeners);\n  } catch (error) {\n    return handleError(error, \"Error getting listener\");\n  }\n}\nasync function listenHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.param(\"agentId\");\n    const formData = await c2.req.formData();\n    const audioFile = formData.get(\"audio\");\n    const options = formData.get(\"options\");\n    if (!audioFile || !(audioFile instanceof File)) {\n      throw new HTTPException(400, { message: \"Audio file is required\" });\n    }\n    const audioData = await audioFile.arrayBuffer();\n    let parsedOptions = {};\n    try {\n      parsedOptions = options ? JSON.parse(options) : {};\n    } catch {\n    }\n    const transcription = await transcribeSpeechHandler({\n      mastra,\n      agentId,\n      body: {\n        audioData: Buffer.from(audioData),\n        options: parsedOptions\n      }\n    });\n    return c2.json({ text: transcription?.text });\n  } catch (error) {\n    return handleError(error, \"Error transcribing speech\");\n  }\n}\n\n// src/server/handlers/routes/agents/router.ts\nfunction agentsRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all available agents\",\n      tags: [\"agents\"],\n      responses: {\n        200: {\n          description: \"List of all agents\"\n        }\n      }\n    }),\n    getAgentsHandler\n  );\n  router.get(\n    \"/:agentId\",\n    w({\n      description: \"Get agent by ID\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Agent details\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    getAgentByIdHandler\n  );\n  router.get(\n    \"/:agentId/evals/ci\",\n    w({\n      description: \"Get CI evals by agent ID\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of evals\"\n        }\n      }\n    }),\n    getEvalsByAgentIdHandler\n  );\n  router.get(\n    \"/:agentId/evals/live\",\n    w({\n      description: \"Get live evals by agent ID\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of evals\"\n        }\n      }\n    }),\n    getLiveEvalsByAgentIdHandler\n  );\n  router.post(\n    \"/:agentId/generate-legacy\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                threadId: { type: \"string\" },\n                resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n                resourceid: {\n                  type: \"string\",\n                  description: \"The resource ID for the conversation (deprecated, use resourceId instead)\",\n                  deprecated: true\n                },\n                runId: { type: \"string\" },\n                output: { type: \"object\" }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    generateLegacyHandler\n  );\n  router.post(\n    \"/:agentId/generate\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                threadId: { type: \"string\" },\n                resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n                resourceid: {\n                  type: \"string\",\n                  description: \"The resource ID for the conversation (deprecated, use resourceId instead)\",\n                  deprecated: true\n                },\n                runId: { type: \"string\" },\n                output: { type: \"object\" }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    generateHandler\n  );\n  router.post(\n    \"/:agentId/generate/vnext\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: vNextBodyOptions,\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    generateVNextHandler\n  );\n  router.post(\n    \"/:agentId/stream/vnext\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: vNextBodyOptions,\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    streamVNextGenerateHandler\n  );\n  router.post(\n    \"/:agentId/stream-legacy\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Stream a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                threadId: { type: \"string\" },\n                resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n                resourceid: {\n                  type: \"string\",\n                  description: \"The resource ID for the conversation (deprecated, use resourceId instead)\",\n                  deprecated: true\n                },\n                runId: { type: \"string\" },\n                output: { type: \"object\" }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Streamed response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    streamGenerateLegacyHandler\n  );\n  router.post(\n    \"/:agentId/stream\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Stream a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                threadId: { type: \"string\" },\n                resourceId: { type: \"string\", description: \"The resource ID for the conversation\" },\n                resourceid: {\n                  type: \"string\",\n                  description: \"The resource ID for the conversation (deprecated, use resourceId instead)\",\n                  deprecated: true\n                },\n                runId: { type: \"string\" },\n                output: { type: \"object\" }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Streamed response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    streamGenerateHandler\n  );\n  router.post(\n    \"/:agentId/streamVNext\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"[DEPRECATED] This endpoint is deprecated. Please use /stream instead.\",\n      tags: [\"agents\"],\n      deprecated: true,\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                runId: { type: \"string\" },\n                output: { type: \"object\" },\n                experimental_output: { type: \"object\" },\n                instructions: { type: \"string\" },\n                toolsets: { type: \"object\" },\n                clientTools: { type: \"object\" },\n                context: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                memory: {\n                  type: \"object\",\n                  properties: {\n                    threadId: { type: \"string\" },\n                    resourceId: { type: \"string\", description: \"The resource ID for the conversation\" }\n                  }\n                },\n                toolChoice: {\n                  oneOf: [\n                    { type: \"string\", enum: [\"auto\", \"none\", \"required\"] },\n                    { type: \"object\", properties: { type: { type: \"string\" }, toolName: { type: \"string\" } } }\n                  ]\n                }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        410: {\n          description: \"Endpoint deprecated\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  error: { type: \"string\" },\n                  message: { type: \"string\" },\n                  deprecated_endpoint: { type: \"string\" },\n                  replacement_endpoint: { type: \"string\" }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    deprecatedStreamVNextHandler\n  );\n  router.post(\n    \"/:agentId/stream/vnext/ui\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Stream a response from an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: vNextBodyOptions,\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Streamed response\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    streamVNextUIMessageHandler\n  );\n  router.post(\n    \"/:agentId/model\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Update the model for an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                modelId: {\n                  type: \"string\",\n                  description: \"The modelId to update the agent to\"\n                },\n                provider: {\n                  type: \"string\",\n                  enum: [\"openai\", \"anthropic\", \"groq\", \"xai\", \"google\"],\n                  description: \"The provider of the model to update the agent to\"\n                }\n              },\n              required: [\"modelId\", \"provider\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Model updated successfully\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    updateAgentModelHandler\n  );\n  router.get(\n    \"/:agentId/speakers\",\n    async (c2, next) => {\n      c2.header(\"Deprecation\", \"true\");\n      c2.header(\"Warning\", '299 - \"This endpoint is deprecated, use /api/agents/:agentId/voice/speakers instead\"');\n      c2.header(\"Link\", '</api/agents/:agentId/voice/speakers>; rel=\"successor-version\"');\n      return next();\n    },\n    w({\n      description: \"[DEPRECATED] Use /api/agents/:agentId/voice/speakers instead. Get available speakers for an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of available speakers\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"array\",\n                items: {\n                  type: \"object\",\n                  description: \"Speaker information depending on the voice provider\",\n                  properties: {\n                    voiceId: { type: \"string\" }\n                  },\n                  additionalProperties: true\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    getSpeakersHandler\n  );\n  router.get(\n    \"/:agentId/voice/speakers\",\n    w({\n      description: \"Get available speakers for an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of available speakers\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"array\",\n                items: {\n                  type: \"object\",\n                  description: \"Speaker information depending on the voice provider\",\n                  properties: {\n                    voiceId: { type: \"string\" }\n                  },\n                  additionalProperties: true\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    getSpeakersHandler\n  );\n  router.post(\n    \"/:agentId/speak\",\n    bodyLimit(bodyLimitOptions),\n    async (c2, next) => {\n      c2.header(\"Deprecation\", \"true\");\n      c2.header(\"Warning\", '299 - \"This endpoint is deprecated, use /api/agents/:agentId/voice/speak instead\"');\n      c2.header(\"Link\", '</api/agents/:agentId/voice/speak>; rel=\"successor-version\"');\n      return next();\n    },\n    w({\n      description: \"[DEPRECATED] Use /api/agents/:agentId/voice/speak instead. Convert text to speech using the agent's voice provider\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                text: {\n                  type: \"string\",\n                  description: \"Text to convert to speech\"\n                },\n                options: {\n                  type: \"object\",\n                  description: \"Provider-specific options for speech generation\",\n                  properties: {\n                    speaker: {\n                      type: \"string\",\n                      description: \"Speaker ID to use for speech generation\"\n                    }\n                  },\n                  additionalProperties: true\n                }\n              },\n              required: [\"text\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Audio stream\",\n          content: {\n            \"audio/mpeg\": {\n              schema: {\n                format: \"binary\",\n                description: \"Audio stream containing the generated speech\"\n              }\n            },\n            \"audio/*\": {\n              schema: {\n                format: \"binary\",\n                description: \"Audio stream depending on the provider\"\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities or invalid request\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    speakHandler\n  );\n  router.post(\n    \"/:agentId/voice/speak\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Convert text to speech using the agent's voice provider\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                input: {\n                  type: \"string\",\n                  description: \"Text to convert to speech\"\n                },\n                options: {\n                  type: \"object\",\n                  description: \"Provider-specific options for speech generation\",\n                  properties: {\n                    speaker: {\n                      type: \"string\",\n                      description: \"Speaker ID to use for speech generation\"\n                    },\n                    options: {\n                      type: \"object\",\n                      description: \"Provider-specific options for speech generation\",\n                      additionalProperties: true\n                    }\n                  },\n                  additionalProperties: true\n                }\n              },\n              required: [\"text\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Audio stream\",\n          content: {\n            \"audio/mpeg\": {\n              schema: {\n                format: \"binary\",\n                description: \"Audio stream containing the generated speech\"\n              }\n            },\n            \"audio/*\": {\n              schema: {\n                format: \"binary\",\n                description: \"Audio stream depending on the provider\"\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities or invalid request\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    speakHandler\n  );\n  router.get(\n    \"/:agentId/voice/listener\",\n    w({\n      description: \"Get available listener for an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Checks if listener is available for the agent\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                description: \"Listener information depending on the voice provider\",\n                properties: {\n                  enabled: { type: \"boolean\" }\n                },\n                additionalProperties: true\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    getListenerHandler\n  );\n  router.post(\n    \"/:agentId/listen\",\n    bodyLimit({\n      ...bodyLimitOptions,\n      maxSize: 10 * 1024 * 1024\n      // 10 MB for audio files\n    }),\n    async (c2, next) => {\n      c2.header(\"Deprecation\", \"true\");\n      c2.header(\"Warning\", '299 - \"This endpoint is deprecated, use /api/agents/:agentId/voice/listen instead\"');\n      c2.header(\"Link\", '</api/agents/:agentId/voice/listen>; rel=\"successor-version\"');\n      return next();\n    },\n    w({\n      description: \"[DEPRECATED] Use /api/agents/:agentId/voice/listen instead. Convert speech to text using the agent's voice provider. Additional provider-specific options can be passed as query parameters.\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"audio/mpeg\": {\n            schema: {\n              format: \"binary\",\n              description: \"Audio data stream to transcribe (supports various formats depending on provider like mp3, wav, webm, flac)\"\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Transcription result\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  text: {\n                    type: \"string\",\n                    description: \"Transcribed text\"\n                  }\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities or invalid request\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    listenHandler\n  );\n  router.post(\n    \"/:agentId/voice/listen\",\n    bodyLimit({\n      ...bodyLimitOptions,\n      maxSize: 10 * 1024 * 1024\n      // 10 MB for audio files\n    }),\n    w({\n      description: \"Convert speech to text using the agent's voice provider. Additional provider-specific options can be passed as query parameters.\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"multipart/form-data\": {\n            schema: {\n              type: \"object\",\n              required: [\"audio\"],\n              properties: {\n                audio: {\n                  type: \"string\",\n                  format: \"binary\",\n                  description: \"Audio data stream to transcribe (supports various formats depending on provider like mp3, wav, webm, flac)\"\n                },\n                options: {\n                  type: \"object\",\n                  description: \"Provider-specific options for speech-to-text\",\n                  additionalProperties: true\n                }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Transcription result\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  text: {\n                    type: \"string\",\n                    description: \"Transcribed text\"\n                  }\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Agent does not have voice capabilities or invalid request\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    listenHandler\n  );\n  router.get(\n    \"/:agentId/tools/:toolId\",\n    w({\n      description: \"Get agent tool by ID\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"toolId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Tool details\"\n        },\n        404: {\n          description: \"Tool or agent not found\"\n        }\n      }\n    }),\n    getAgentToolHandler\n  );\n  router.post(\n    \"/:agentId/tools/:toolId/execute\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Execute a tool through an agent\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"toolId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                data: { type: \"object\" },\n                runtimeContext: { type: \"object\" }\n              },\n              required: [\"data\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Tool execution result\"\n        },\n        404: {\n          description: \"Tool or agent not found\"\n        }\n      }\n    }),\n    executeAgentToolHandler\n  );\n  return router;\n}\nfunction agentsRouterDev(bodyLimitOptions) {\n  const router = new Hono();\n  router.post(\n    \"/:agentId/instructions\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Update an agent's instructions\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                instructions: {\n                  type: \"string\",\n                  description: \"New instructions for the agent\"\n                }\n              },\n              required: [\"instructions\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Instructions updated successfully\"\n        },\n        403: {\n          description: \"Not allowed in non-playground environment\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    setAgentInstructionsHandler\n  );\n  router.post(\n    \"/:agentId/instructions/enhance\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate an improved system prompt from instructions\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"ID of the agent whose model will be used for prompt generation\"\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                instructions: {\n                  type: \"string\",\n                  description: \"Instructions to generate a system prompt from\"\n                },\n                comment: {\n                  type: \"string\",\n                  description: \"Optional comment for the enhanced prompt\"\n                }\n              },\n              required: [\"instructions\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated system prompt and analysis\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  explanation: {\n                    type: \"string\",\n                    description: \"Detailed analysis of the instructions\"\n                  },\n                  new_prompt: {\n                    type: \"string\",\n                    description: \"The enhanced system prompt\"\n                  }\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Missing or invalid request parameters\"\n        },\n        404: {\n          description: \"Agent not found\"\n        },\n        500: {\n          description: \"Internal server error or model response parsing error\"\n        }\n      }\n    }),\n    generateSystemPromptHandler\n  );\n  return router;\n}\nasync function getLogsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const { transportId, fromDate, toDate, logLevel, page, perPage } = c2.req.query();\n    const filters = c2.req.queries(\"filters\");\n    const logs = await getLogsHandler$1({\n      mastra,\n      transportId,\n      params: {\n        fromDate: fromDate ? new Date(fromDate) : void 0,\n        toDate: toDate ? new Date(toDate) : void 0,\n        logLevel: logLevel ? logLevel : void 0,\n        filters,\n        page: page ? Number(page) : void 0,\n        perPage: perPage ? Number(perPage) : void 0\n      }\n    });\n    return c2.json(logs);\n  } catch (error) {\n    return handleError(error, \"Error getting logs\");\n  }\n}\nasync function getLogsByRunIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runId = c2.req.param(\"runId\");\n    const { transportId, fromDate, toDate, logLevel, page, perPage } = c2.req.query();\n    const filters = c2.req.queries(\"filters\");\n    const logs = await getLogsByRunIdHandler$1({\n      mastra,\n      runId,\n      transportId,\n      params: {\n        fromDate: fromDate ? new Date(fromDate) : void 0,\n        toDate: toDate ? new Date(toDate) : void 0,\n        logLevel: logLevel ? logLevel : void 0,\n        filters,\n        page: page ? Number(page) : void 0,\n        perPage: perPage ? Number(perPage) : void 0\n      }\n    });\n    return c2.json(logs);\n  } catch (error) {\n    return handleError(error, \"Error getting logs by run ID\");\n  }\n}\nasync function getLogTransports(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const result = await getLogTransports$1({\n      mastra\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting log Transports\");\n  }\n}\n\n// src/server/handlers/routes/logs/router.ts\nfunction logsRouter() {\n  const router = new Hono();\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all logs\",\n      tags: [\"logs\"],\n      parameters: [\n        {\n          name: \"transportId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"fromDate\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"toDate\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"logLevel\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"filters\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" }\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of all logs\"\n        }\n      }\n    }),\n    getLogsHandler\n  );\n  router.get(\n    \"/transports\",\n    w({\n      description: \"List of all log transports\",\n      tags: [\"logs\"],\n      responses: {\n        200: {\n          description: \"List of all log transports\"\n        }\n      }\n    }),\n    getLogTransports\n  );\n  router.get(\n    \"/:runId\",\n    w({\n      description: \"Get logs by run ID\",\n      tags: [\"logs\"],\n      parameters: [\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"transportId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"fromDate\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"toDate\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"logLevel\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"filters\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" }\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of logs for run ID\"\n        }\n      }\n    }),\n    getLogsByRunIdHandler\n  );\n  return router;\n}\nvar classRegExp = /^([A-Z][a-z0-9]*)+$/;\nvar kTypes = [\n  \"string\",\n  \"function\",\n  \"number\",\n  \"object\",\n  // Accept 'Function' and 'Object' as alternative to the lower cased version.\n  \"Function\",\n  \"Object\",\n  \"boolean\",\n  \"bigint\",\n  \"symbol\"\n];\nfunction determineSpecificType(value) {\n  if (value == null) {\n    return \"\" + value;\n  }\n  if (typeof value === \"function\" && value.name) {\n    return `function ${value.name}`;\n  }\n  if (typeof value === \"object\") {\n    if (value.constructor?.name) {\n      return `an instance of ${value.constructor.name}`;\n    }\n    return `${util.inspect(value, { depth: -1 })}`;\n  }\n  let inspected = util.inspect(value, { colors: false });\n  if (inspected.length > 28) {\n    inspected = `${inspected.slice(0, 25)}...`;\n  }\n  return `type ${typeof value} (${inspected})`;\n}\nvar ERR_HTTP_BODY_NOT_ALLOWED = class extends Error {\n  constructor() {\n    super(\"Adding content for this request method or response status is not allowed.\");\n  }\n};\nvar ERR_HTTP_CONTENT_LENGTH_MISMATCH = class extends Error {\n  constructor(actual, expected) {\n    super(`Response body's content-length of ${actual} byte(s) does not match the content-length of ${expected} byte(s) set in header`);\n  }\n};\nvar ERR_HTTP_HEADERS_SENT = class extends Error {\n  constructor(arg) {\n    super(`Cannot ${arg} headers after they are sent to the client`);\n  }\n};\nvar ERR_INVALID_ARG_VALUE = class extends TypeError {\n  constructor(name, value, reason = \"is invalid\") {\n    let inspected = util.inspect(value);\n    if (inspected.length > 128) {\n      inspected = `${inspected.slice(0, 128)}...`;\n    }\n    const type = name.includes(\".\") ? \"property\" : \"argument\";\n    super(`The ${type} '${name}' ${reason}. Received ${inspected}`);\n  }\n};\nvar ERR_INVALID_CHAR = class extends TypeError {\n  constructor(name, field) {\n    let msg = `Invalid character in ${name}`;\n    if (field !== void 0) {\n      msg += ` [\"${field}\"]`;\n    }\n    super(msg);\n  }\n};\nvar ERR_HTTP_INVALID_HEADER_VALUE = class extends TypeError {\n  constructor(value, name) {\n    super(`Invalid value \"${value}\" for header \"${name}\"`);\n  }\n};\nvar ERR_HTTP_INVALID_STATUS_CODE = class extends RangeError {\n  originalStatusCode;\n  constructor(originalStatusCode) {\n    super(`Invalid status code: ${originalStatusCode}`);\n    this.originalStatusCode = originalStatusCode;\n  }\n};\nvar ERR_HTTP_TRAILER_INVALID = class extends Error {\n  constructor() {\n    super(`Trailers are invalid with this transfer encoding`);\n  }\n};\nvar ERR_INVALID_ARG_TYPE = class extends TypeError {\n  constructor(name, expected, actual) {\n    if (!Array.isArray(expected)) {\n      expected = [expected];\n    }\n    let msg = \"The \";\n    if (name.endsWith(\" argument\")) {\n      msg += `${name} `;\n    } else {\n      const type = name.includes(\".\") ? \"property\" : \"argument\";\n      msg += `\"${name}\" ${type} `;\n    }\n    msg += \"must be \";\n    const types = [];\n    const instances = [];\n    const other = [];\n    for (const value of expected) {\n      if (kTypes.includes(value)) {\n        types.push(value.toLowerCase());\n      } else if (classRegExp.exec(value) !== null) {\n        instances.push(value);\n      } else {\n        other.push(value);\n      }\n    }\n    if (instances.length > 0) {\n      const pos = types.indexOf(\"object\");\n      if (pos !== -1) {\n        types.splice(pos, 1);\n        instances.push(\"Object\");\n      }\n    }\n    if (types.length > 0) {\n      if (types.length > 2) {\n        const last = types.pop();\n        msg += `one of type ${types.join(\", \")}, or ${last}`;\n      } else if (types.length === 2) {\n        msg += `one of type ${types[0]} or ${types[1]}`;\n      } else {\n        msg += `of type ${types[0]}`;\n      }\n      if (instances.length > 0 || other.length > 0)\n        msg += \" or \";\n    }\n    if (instances.length > 0) {\n      if (instances.length > 2) {\n        const last = instances.pop();\n        msg += `an instance of ${instances.join(\", \")}, or ${last}`;\n      } else {\n        msg += `an instance of ${instances[0]}`;\n        if (instances.length === 2) {\n          msg += ` or ${instances[1]}`;\n        }\n      }\n      if (other.length > 0)\n        msg += \" or \";\n    }\n    if (other.length > 0) {\n      if (other.length > 2) {\n        const last = other.pop();\n        msg += `one of ${other.join(\", \")}, or ${last}`;\n      } else if (other.length === 2) {\n        msg += `one of ${other[0]} or ${other[1]}`;\n      } else {\n        if (other[0].toLowerCase() !== other[0])\n          msg += \"an \";\n        msg += `${other[0]}`;\n      }\n    }\n    msg += `. Received ${determineSpecificType(actual)}`;\n    super(msg);\n  }\n};\nvar ERR_INVALID_HTTP_TOKEN = class extends TypeError {\n  constructor(name, field) {\n    super(`${name} must be a valid HTTP token [\"${field}\"]`);\n  }\n};\nvar ERR_METHOD_NOT_IMPLEMENTED = class extends Error {\n  constructor(methodName) {\n    super(`The ${methodName} method is not implemented`);\n  }\n};\nvar ERR_STREAM_ALREADY_FINISHED = class extends Error {\n  constructor(methodName) {\n    super(`Cannot call ${methodName} after a stream was finished`);\n  }\n};\nvar ERR_STREAM_CANNOT_PIPE = class extends Error {\n  constructor() {\n    super(`Cannot pipe, not readable`);\n  }\n};\nvar ERR_STREAM_DESTROYED = class extends Error {\n  constructor(methodName) {\n    super(`Cannot call ${methodName} after a stream was destroyed`);\n  }\n};\nvar ERR_STREAM_NULL_VALUES = class extends TypeError {\n  constructor() {\n    super(`May not write null values to stream`);\n  }\n};\nvar ERR_STREAM_WRITE_AFTER_END = class extends Error {\n  constructor() {\n    super(`write after end`);\n  }\n};\n\n// ../../node_modules/.pnpm/fetch-to-node@2.1.0/node_modules/fetch-to-node/dist/fetch-to-node/http-incoming.js\nvar kHeaders = Symbol(\"kHeaders\");\nvar kHeadersDistinct = Symbol(\"kHeadersDistinct\");\nvar kHeadersCount = Symbol(\"kHeadersCount\");\nvar kTrailers = Symbol(\"kTrailers\");\nvar kTrailersDistinct = Symbol(\"kTrailersDistinct\");\nvar kTrailersCount = Symbol(\"kTrailersCount\");\nvar FetchIncomingMessage = class extends Readable {\n  get socket() {\n    return null;\n  }\n  set socket(_val) {\n    throw new ERR_METHOD_NOT_IMPLEMENTED(\"socket\");\n  }\n  httpVersionMajor;\n  httpVersionMinor;\n  httpVersion;\n  complete = false;\n  [kHeaders] = null;\n  [kHeadersDistinct] = null;\n  [kHeadersCount] = 0;\n  rawHeaders = [];\n  [kTrailers] = null;\n  [kTrailersDistinct] = null;\n  [kTrailersCount] = 0;\n  rawTrailers = [];\n  joinDuplicateHeaders = false;\n  aborted = false;\n  upgrade = false;\n  // request (server) only\n  url = \"\";\n  method;\n  // TODO: Support ClientRequest\n  // statusCode = null;\n  // statusMessage = null;\n  // client = socket;\n  _consuming;\n  _dumped;\n  // The underlying ReadableStream\n  _stream = null;\n  constructor() {\n    const streamOptions = {};\n    super(streamOptions);\n    this._readableState.readingMore = true;\n    this._consuming = false;\n    this._dumped = false;\n  }\n  get connection() {\n    return null;\n  }\n  set connection(_socket) {\n    console.error(\"No support for IncomingMessage.connection\");\n  }\n  get headers() {\n    if (!this[kHeaders]) {\n      this[kHeaders] = {};\n      const src = this.rawHeaders;\n      const dst = this[kHeaders];\n      for (let n2 = 0; n2 < this[kHeadersCount]; n2 += 2) {\n        this._addHeaderLine(src[n2], src[n2 + 1], dst);\n      }\n    }\n    return this[kHeaders];\n  }\n  set headers(val) {\n    this[kHeaders] = val;\n  }\n  get headersDistinct() {\n    if (!this[kHeadersDistinct]) {\n      this[kHeadersDistinct] = {};\n      const src = this.rawHeaders;\n      const dst = this[kHeadersDistinct];\n      for (let n2 = 0; n2 < this[kHeadersCount]; n2 += 2) {\n        this._addHeaderLineDistinct(src[n2], src[n2 + 1], dst);\n      }\n    }\n    return this[kHeadersDistinct];\n  }\n  set headersDistinct(val) {\n    this[kHeadersDistinct] = val;\n  }\n  get trailers() {\n    if (!this[kTrailers]) {\n      this[kTrailers] = {};\n      const src = this.rawTrailers;\n      const dst = this[kTrailers];\n      for (let n2 = 0; n2 < this[kTrailersCount]; n2 += 2) {\n        this._addHeaderLine(src[n2], src[n2 + 1], dst);\n      }\n    }\n    return this[kTrailers];\n  }\n  set trailers(val) {\n    this[kTrailers] = val;\n  }\n  get trailersDistinct() {\n    if (!this[kTrailersDistinct]) {\n      this[kTrailersDistinct] = {};\n      const src = this.rawTrailers;\n      const dst = this[kTrailersDistinct];\n      for (let n2 = 0; n2 < this[kTrailersCount]; n2 += 2) {\n        this._addHeaderLineDistinct(src[n2], src[n2 + 1], dst);\n      }\n    }\n    return this[kTrailersDistinct];\n  }\n  set trailersDistinct(val) {\n    this[kTrailersDistinct] = val;\n  }\n  setTimeout(msecs, callback) {\n    return this;\n  }\n  async _read(n2) {\n    if (!this._consuming) {\n      this._readableState.readingMore = false;\n      this._consuming = true;\n    }\n    if (this._stream == null) {\n      this.complete = true;\n      this.push(null);\n      return;\n    }\n    const reader = this._stream.getReader();\n    try {\n      const data = await reader.read();\n      if (data.done) {\n        this.complete = true;\n        this.push(null);\n      } else {\n        this.push(data.value);\n      }\n    } catch (e2) {\n      this.destroy(e2);\n    } finally {\n      reader.releaseLock();\n    }\n  }\n  _destroy(err, cb) {\n    if (!this.readableEnded || !this.complete) {\n      this.aborted = true;\n      this.emit(\"aborted\");\n    }\n    setTimeout(onError, 0, this, err, cb);\n  }\n  _addHeaderLines(headers, n2) {\n    if (headers?.length) {\n      let dest;\n      if (this.complete) {\n        this.rawTrailers = headers;\n        this[kTrailersCount] = n2;\n        dest = this[kTrailers];\n      } else {\n        this.rawHeaders = headers;\n        this[kHeadersCount] = n2;\n        dest = this[kHeaders];\n      }\n      if (dest) {\n        for (let i2 = 0; i2 < n2; i2 += 2) {\n          this._addHeaderLine(headers[i2], headers[i2 + 1], dest);\n        }\n      }\n    }\n  }\n  // Add the given (field, value) pair to the message\n  //\n  // Per RFC2616, section 4.2 it is acceptable to join multiple instances of the\n  // same header with a ', ' if the header in question supports specification of\n  // multiple values this way. The one exception to this is the Cookie header,\n  // which has multiple values joined with a '; ' instead. If a header's values\n  // cannot be joined in either of these ways, we declare the first instance the\n  // winner and drop the second. Extended header fields (those beginning with\n  // 'x-') are always joined.\n  _addHeaderLine(field, value, dest) {\n    field = matchKnownFields(field);\n    const flag = field.charCodeAt(0);\n    if (flag === 0 || flag === 2) {\n      field = field.slice(1);\n      if (typeof dest[field] === \"string\") {\n        dest[field] += (flag === 0 ? \", \" : \"; \") + value;\n      } else {\n        dest[field] = value;\n      }\n    } else if (flag === 1) {\n      if (dest[\"set-cookie\"] !== void 0) {\n        dest[\"set-cookie\"].push(value);\n      } else {\n        dest[\"set-cookie\"] = [value];\n      }\n    } else if (this.joinDuplicateHeaders) {\n      if (dest[field] === void 0) {\n        dest[field] = value;\n      } else {\n        dest[field] += \", \" + value;\n      }\n    } else if (dest[field] === void 0) {\n      dest[field] = value;\n    }\n  }\n  _addHeaderLineDistinct(field, value, dest) {\n    field = field.toLowerCase();\n    if (!dest[field]) {\n      dest[field] = [value];\n    } else {\n      dest[field].push(value);\n    }\n  }\n  // Call this instead of resume() if we want to just\n  // dump all the data to /dev/null\n  _dump() {\n    if (!this._dumped) {\n      this._dumped = true;\n      this.removeAllListeners(\"data\");\n      this.resume();\n    }\n  }\n};\nfunction matchKnownFields(field, lowercased = false) {\n  switch (field.length) {\n    case 3:\n      if (field === \"Age\" || field === \"age\")\n        return \"age\";\n      break;\n    case 4:\n      if (field === \"Host\" || field === \"host\")\n        return \"host\";\n      if (field === \"From\" || field === \"from\")\n        return \"from\";\n      if (field === \"ETag\" || field === \"etag\")\n        return \"etag\";\n      if (field === \"Date\" || field === \"date\")\n        return \"\\0date\";\n      if (field === \"Vary\" || field === \"vary\")\n        return \"\\0vary\";\n      break;\n    case 6:\n      if (field === \"Server\" || field === \"server\")\n        return \"server\";\n      if (field === \"Cookie\" || field === \"cookie\")\n        return \"\u0002cookie\";\n      if (field === \"Origin\" || field === \"origin\")\n        return \"\\0origin\";\n      if (field === \"Expect\" || field === \"expect\")\n        return \"\\0expect\";\n      if (field === \"Accept\" || field === \"accept\")\n        return \"\\0accept\";\n      break;\n    case 7:\n      if (field === \"Referer\" || field === \"referer\")\n        return \"referer\";\n      if (field === \"Expires\" || field === \"expires\")\n        return \"expires\";\n      if (field === \"Upgrade\" || field === \"upgrade\")\n        return \"\\0upgrade\";\n      break;\n    case 8:\n      if (field === \"Location\" || field === \"location\")\n        return \"location\";\n      if (field === \"If-Match\" || field === \"if-match\")\n        return \"\\0if-match\";\n      break;\n    case 10:\n      if (field === \"User-Agent\" || field === \"user-agent\")\n        return \"user-agent\";\n      if (field === \"Set-Cookie\" || field === \"set-cookie\")\n        return \"\u0001\";\n      if (field === \"Connection\" || field === \"connection\")\n        return \"\\0connection\";\n      break;\n    case 11:\n      if (field === \"Retry-After\" || field === \"retry-after\")\n        return \"retry-after\";\n      break;\n    case 12:\n      if (field === \"Content-Type\" || field === \"content-type\")\n        return \"content-type\";\n      if (field === \"Max-Forwards\" || field === \"max-forwards\")\n        return \"max-forwards\";\n      break;\n    case 13:\n      if (field === \"Authorization\" || field === \"authorization\")\n        return \"authorization\";\n      if (field === \"Last-Modified\" || field === \"last-modified\")\n        return \"last-modified\";\n      if (field === \"Cache-Control\" || field === \"cache-control\")\n        return \"\\0cache-control\";\n      if (field === \"If-None-Match\" || field === \"if-none-match\")\n        return \"\\0if-none-match\";\n      break;\n    case 14:\n      if (field === \"Content-Length\" || field === \"content-length\")\n        return \"content-length\";\n      break;\n    case 15:\n      if (field === \"Accept-Encoding\" || field === \"accept-encoding\")\n        return \"\\0accept-encoding\";\n      if (field === \"Accept-Language\" || field === \"accept-language\")\n        return \"\\0accept-language\";\n      if (field === \"X-Forwarded-For\" || field === \"x-forwarded-for\")\n        return \"\\0x-forwarded-for\";\n      break;\n    case 16:\n      if (field === \"Content-Encoding\" || field === \"content-encoding\")\n        return \"\\0content-encoding\";\n      if (field === \"X-Forwarded-Host\" || field === \"x-forwarded-host\")\n        return \"\\0x-forwarded-host\";\n      break;\n    case 17:\n      if (field === \"If-Modified-Since\" || field === \"if-modified-since\")\n        return \"if-modified-since\";\n      if (field === \"Transfer-Encoding\" || field === \"transfer-encoding\")\n        return \"\\0transfer-encoding\";\n      if (field === \"X-Forwarded-Proto\" || field === \"x-forwarded-proto\")\n        return \"\\0x-forwarded-proto\";\n      break;\n    case 19:\n      if (field === \"Proxy-Authorization\" || field === \"proxy-authorization\")\n        return \"proxy-authorization\";\n      if (field === \"If-Unmodified-Since\" || field === \"if-unmodified-since\")\n        return \"if-unmodified-since\";\n      break;\n  }\n  if (lowercased) {\n    return \"\\0\" + field;\n  }\n  return matchKnownFields(field.toLowerCase(), true);\n}\nfunction onError(self, error, cb) {\n  if (self.listenerCount(\"error\") === 0) {\n    cb();\n  } else {\n    cb(error);\n  }\n}\n\n// ../../node_modules/.pnpm/fetch-to-node@2.1.0/node_modules/fetch-to-node/dist/utils/types.js\nfunction validateString(value, name) {\n  if (typeof value !== \"string\")\n    throw new ERR_INVALID_ARG_TYPE(name, \"string\", value);\n}\nvar linkValueRegExp = /^(?:<[^>]*>)(?:\\s*;\\s*[^;\"\\s]+(?:=(\")?[^;\"\\s]*\\1)?)*$/;\nfunction validateLinkHeaderFormat(value, name) {\n  if (typeof value === \"undefined\" || !linkValueRegExp.exec(value)) {\n    throw new ERR_INVALID_ARG_VALUE(name, value, 'must be an array or string of format \"</styles.css>; rel=preload; as=style\"');\n  }\n}\nfunction validateLinkHeaderValue(hints) {\n  if (typeof hints === \"string\") {\n    validateLinkHeaderFormat(hints, \"hints\");\n    return hints;\n  } else if (Array.isArray(hints)) {\n    const hintsLength = hints.length;\n    let result = \"\";\n    if (hintsLength === 0) {\n      return result;\n    }\n    for (let i2 = 0; i2 < hintsLength; i2++) {\n      const link = hints[i2];\n      validateLinkHeaderFormat(link, \"hints\");\n      result += link;\n      if (i2 !== hintsLength - 1) {\n        result += \", \";\n      }\n    }\n    return result;\n  }\n  throw new ERR_INVALID_ARG_VALUE(\"hints\", hints, 'must be an array or string of format \"</styles.css>; rel=preload; as=style\"');\n}\nfunction isUint8Array(value) {\n  return value != null && value[Symbol.toStringTag] === \"Uint8Array\";\n}\n\n// ../../node_modules/.pnpm/fetch-to-node@2.1.0/node_modules/fetch-to-node/dist/fetch-to-node/internal-http.js\nvar kNeedDrain = Symbol(\"kNeedDrain\");\nvar kOutHeaders = Symbol(\"kOutHeaders\");\nfunction utcDate() {\n  return (/* @__PURE__ */ new Date()).toUTCString();\n}\n\n// ../../node_modules/.pnpm/fetch-to-node@2.1.0/node_modules/fetch-to-node/dist/fetch-to-node/internal-streams-state.js\nfunction getDefaultHighWaterMark(objectMode) {\n  return objectMode ? 16 : 64 * 1024;\n}\n\n// ../../node_modules/.pnpm/fetch-to-node@2.1.0/node_modules/fetch-to-node/dist/fetch-to-node/http-common.js\nvar tokenRegExp = /^[\\^_`a-zA-Z\\-0-9!#$%&'*+.|~]+$/;\nfunction checkIsHttpToken(val) {\n  return tokenRegExp.test(val);\n}\nvar headerCharRegex = /[^\\t\\x20-\\x7e\\x80-\\xff]/;\nfunction checkInvalidHeaderChar(val) {\n  return headerCharRegex.test(val);\n}\nvar chunkExpression = /(?:^|\\W)chunked(?:$|\\W)/i;\nvar kCorked = Symbol(\"corked\");\nvar kChunkedBuffer = Symbol(\"kChunkedBuffer\");\nvar kChunkedLength = Symbol(\"kChunkedLength\");\nvar kUniqueHeaders = Symbol(\"kUniqueHeaders\");\nvar kBytesWritten = Symbol(\"kBytesWritten\");\nvar kErrored = Symbol(\"errored\");\nvar kHighWaterMark = Symbol(\"kHighWaterMark\");\nvar kRejectNonStandardBodyWrites = Symbol(\"kRejectNonStandardBodyWrites\");\nvar nop = () => {\n};\nvar RE_CONN_CLOSE = /(?:^|\\W)close(?:$|\\W)/i;\nfunction isCookieField(s3) {\n  return s3.length === 6 && s3.toLowerCase() === \"cookie\";\n}\nfunction isContentDispositionField(s3) {\n  return s3.length === 19 && s3.toLowerCase() === \"content-disposition\";\n}\nvar WrittenDataBuffer = class {\n  [kCorked] = 0;\n  [kHighWaterMark] = getDefaultHighWaterMark();\n  entries = [];\n  onWrite;\n  constructor(params = {}) {\n    this.onWrite = params.onWrite;\n  }\n  write(data, encoding, callback) {\n    this.entries.push({\n      data,\n      length: data.length,\n      encoding,\n      callback,\n      written: false\n    });\n    this._flush();\n    return true;\n  }\n  cork() {\n    this[kCorked]++;\n  }\n  uncork() {\n    this[kCorked]--;\n    this._flush();\n  }\n  _flush() {\n    if (this[kCorked] <= 0) {\n      for (const [index, entry] of this.entries.entries()) {\n        if (!entry.written) {\n          entry.written = true;\n          if (this.onWrite != null) {\n            this.onWrite(index, entry);\n          }\n          if (entry.callback != null) {\n            entry.callback.call(void 0);\n          }\n        }\n      }\n    }\n  }\n  get writableLength() {\n    return this.entries.reduce((acc, entry) => {\n      return acc + (entry.written && entry.length ? entry.length : 0);\n    }, 0);\n  }\n  get writableHighWaterMark() {\n    return this[kHighWaterMark];\n  }\n  get writableCorked() {\n    return this[kCorked];\n  }\n};\nvar FetchOutgoingMessage = class extends Writable {\n  req;\n  outputData;\n  outputSize;\n  // Difference from Node.js -\n  // `writtenHeaderBytes` is the number of bytes the header has taken.\n  // Since Node.js writes both the headers and body into the same outgoing\n  // stream, it helps to keep track of this so that we can skip that many bytes\n  // from the beginning of the stream when providing the outgoing stream.\n  writtenHeaderBytes = 0;\n  _last;\n  chunkedEncoding;\n  shouldKeepAlive;\n  maxRequestsOnConnectionReached;\n  _defaultKeepAlive;\n  useChunkedEncodingByDefault;\n  sendDate;\n  _removedConnection;\n  _removedContLen;\n  _removedTE;\n  strictContentLength;\n  [kBytesWritten];\n  _contentLength;\n  _hasBody;\n  _trailer;\n  [kNeedDrain];\n  finished;\n  _headerSent;\n  [kCorked];\n  [kChunkedBuffer];\n  [kChunkedLength];\n  _closed;\n  // Difference from Node.js -\n  // In Node.js, this is a socket object.\n  // [kSocket]: null;\n  _header;\n  [kOutHeaders];\n  _keepAliveTimeout;\n  _maxRequestsPerSocket;\n  _onPendingData;\n  [kUniqueHeaders];\n  [kErrored];\n  [kHighWaterMark];\n  [kRejectNonStandardBodyWrites];\n  _writtenDataBuffer = new WrittenDataBuffer({\n    onWrite: this._onDataWritten.bind(this)\n  });\n  constructor(req, options) {\n    super();\n    this.req = req;\n    this.outputData = [];\n    this.outputSize = 0;\n    this.destroyed = false;\n    this._last = false;\n    this.chunkedEncoding = false;\n    this.shouldKeepAlive = true;\n    this.maxRequestsOnConnectionReached = false;\n    this._defaultKeepAlive = true;\n    this.useChunkedEncodingByDefault = true;\n    this.sendDate = false;\n    this._removedConnection = false;\n    this._removedContLen = false;\n    this._removedTE = false;\n    this.strictContentLength = false;\n    this[kBytesWritten] = 0;\n    this._contentLength = null;\n    this._hasBody = true;\n    this._trailer = \"\";\n    this[kNeedDrain] = false;\n    this.finished = false;\n    this._headerSent = false;\n    this[kCorked] = 0;\n    this[kChunkedBuffer] = [];\n    this[kChunkedLength] = 0;\n    this._closed = false;\n    this._header = null;\n    this[kOutHeaders] = null;\n    this._keepAliveTimeout = 0;\n    this._onPendingData = nop;\n    this[kErrored] = null;\n    this[kHighWaterMark] = options?.highWaterMark ?? getDefaultHighWaterMark();\n    this[kRejectNonStandardBodyWrites] = options?.rejectNonStandardBodyWrites ?? false;\n    this[kUniqueHeaders] = null;\n  }\n  _renderHeaders() {\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"render\");\n    }\n    const headersMap = this[kOutHeaders];\n    const headers = {};\n    if (headersMap !== null) {\n      const keys = Object.keys(headersMap);\n      for (let i2 = 0, l2 = keys.length; i2 < l2; i2++) {\n        const key = keys[i2];\n        headers[headersMap[key][0]] = headersMap[key][1];\n      }\n    }\n    return headers;\n  }\n  cork() {\n    this[kCorked]++;\n    if (this._writtenDataBuffer != null) {\n      this._writtenDataBuffer.cork();\n    }\n  }\n  uncork() {\n    this[kCorked]--;\n    if (this._writtenDataBuffer != null) {\n      this._writtenDataBuffer.uncork();\n    }\n    if (this[kCorked] || this[kChunkedBuffer].length === 0) {\n      return;\n    }\n    const buf = this[kChunkedBuffer];\n    for (const { data, encoding, callback } of buf) {\n      this._send(data ?? \"\", encoding, callback);\n    }\n    this[kChunkedBuffer].length = 0;\n    this[kChunkedLength] = 0;\n  }\n  setTimeout(msecs, callback) {\n    return this;\n  }\n  destroy(error) {\n    if (this.destroyed) {\n      return this;\n    }\n    this.destroyed = true;\n    this[kErrored] = error;\n    return this;\n  }\n  _send(data, encoding, callback, byteLength) {\n    if (!this._headerSent) {\n      const header = this._header;\n      if (typeof data === \"string\" && (encoding === \"utf8\" || encoding === \"latin1\" || !encoding)) {\n        data = header + data;\n      } else {\n        this.outputData.unshift({\n          data: header,\n          encoding: \"latin1\",\n          callback: void 0\n        });\n        this.outputSize += header.length;\n        this._onPendingData(header.length);\n      }\n      this._headerSent = true;\n      this.writtenHeaderBytes = header.length;\n      const [statusLine, ...headerLines] = this._header.split(\"\\r\\n\");\n      const STATUS_LINE_REGEXP = /^HTTP\\/1\\.1 (?<statusCode>\\d+) (?<statusMessage>.*)$/;\n      const statusLineResult = STATUS_LINE_REGEXP.exec(statusLine);\n      if (statusLineResult == null) {\n        throw new Error(\"Unexpected! Status line was \" + statusLine);\n      }\n      const { statusCode: statusCodeText, statusMessage } = statusLineResult.groups ?? {};\n      const statusCode = parseInt(statusCodeText, 10);\n      const headers = [];\n      for (const headerLine of headerLines) {\n        if (headerLine !== \"\") {\n          const pos = headerLine.indexOf(\": \");\n          const k = headerLine.slice(0, pos);\n          const v = headerLine.slice(pos + 2);\n          headers.push([k, v]);\n        }\n      }\n      const event = {\n        statusCode,\n        statusMessage,\n        headers\n      };\n      this.emit(\"_headersSent\", event);\n    }\n    return this._writeRaw(data, encoding, callback, byteLength);\n  }\n  _writeRaw(data, encoding, callback, size) {\n    if (typeof encoding === \"function\") {\n      callback = encoding;\n      encoding = null;\n    }\n    if (this._writtenDataBuffer != null) {\n      if (this.outputData.length) {\n        this._flushOutput(this._writtenDataBuffer);\n      }\n      return this._writtenDataBuffer.write(data, encoding, callback);\n    }\n    this.outputData.push({ data, encoding, callback });\n    this.outputSize += data.length;\n    this._onPendingData(data.length);\n    return this.outputSize < this[kHighWaterMark];\n  }\n  _onDataWritten(index, entry) {\n    const event = { index, entry };\n    this.emit(\"_dataWritten\", event);\n  }\n  _storeHeader(firstLine, headers) {\n    const state = {\n      connection: false,\n      contLen: false,\n      te: false,\n      date: false,\n      expect: false,\n      trailer: false,\n      header: firstLine\n    };\n    if (headers) {\n      if (headers === this[kOutHeaders]) {\n        for (const key in headers) {\n          const entry = headers[key];\n          processHeader(this, state, entry[0], entry[1], false);\n        }\n      } else if (Array.isArray(headers)) {\n        if (headers.length && Array.isArray(headers[0])) {\n          for (let i2 = 0; i2 < headers.length; i2++) {\n            const entry = headers[i2];\n            processHeader(this, state, entry[0], entry[1], true);\n          }\n        } else {\n          if (headers.length % 2 !== 0) {\n            throw new ERR_INVALID_ARG_VALUE(\"headers\", headers);\n          }\n          for (let n2 = 0; n2 < headers.length; n2 += 2) {\n            processHeader(this, state, headers[n2], headers[n2 + 1], true);\n          }\n        }\n      } else {\n        for (const key in headers) {\n          if (headers.hasOwnProperty(key)) {\n            const _headers = headers;\n            processHeader(this, state, key, _headers[key], true);\n          }\n        }\n      }\n    }\n    let { header } = state;\n    if (this.sendDate && !state.date) {\n      header += \"Date: \" + utcDate() + \"\\r\\n\";\n    }\n    if (this.chunkedEncoding && (this.statusCode === 204 || this.statusCode === 304)) {\n      this.chunkedEncoding = false;\n      this.shouldKeepAlive = false;\n    }\n    if (this._removedConnection) {\n      this._last = !this.shouldKeepAlive;\n    } else if (!state.connection) {\n      const shouldSendKeepAlive = this.shouldKeepAlive && (state.contLen || this.useChunkedEncodingByDefault);\n      if (shouldSendKeepAlive && this.maxRequestsOnConnectionReached) {\n        header += \"Connection: close\\r\\n\";\n      } else if (shouldSendKeepAlive) {\n        header += \"Connection: keep-alive\\r\\n\";\n        if (this._keepAliveTimeout && this._defaultKeepAlive) {\n          const timeoutSeconds = Math.floor(this._keepAliveTimeout / 1e3);\n          let max = \"\";\n          if (this._maxRequestsPerSocket && ~~this._maxRequestsPerSocket > 0) {\n            max = `, max=${this._maxRequestsPerSocket}`;\n          }\n          header += `Keep-Alive: timeout=${timeoutSeconds}${max}\\r\n`;\n        }\n      } else {\n        this._last = true;\n        header += \"Connection: close\\r\\n\";\n      }\n    }\n    if (!state.contLen && !state.te) {\n      if (!this._hasBody) {\n        this.chunkedEncoding = false;\n      } else if (!this.useChunkedEncodingByDefault) {\n        this._last = true;\n      } else if (!state.trailer && !this._removedContLen && typeof this._contentLength === \"number\") {\n        header += \"Content-Length: \" + this._contentLength + \"\\r\\n\";\n      } else if (!this._removedTE) {\n        header += \"Transfer-Encoding: chunked\\r\\n\";\n        this.chunkedEncoding = true;\n      } else {\n        this._last = true;\n      }\n    }\n    if (this.chunkedEncoding !== true && state.trailer) {\n      throw new ERR_HTTP_TRAILER_INVALID();\n    }\n    this._header = header + \"\\r\\n\";\n    this._headerSent = false;\n    if (state.expect) {\n      this._send(\"\");\n    }\n  }\n  get _headers() {\n    console.warn(\"DEP0066: OutgoingMessage.prototype._headers is deprecated\");\n    return this.getHeaders();\n  }\n  set _headers(val) {\n    console.warn(\"DEP0066: OutgoingMessage.prototype._headers is deprecated\");\n    if (val == null) {\n      this[kOutHeaders] = null;\n    } else if (typeof val === \"object\") {\n      const headers = this[kOutHeaders] = /* @__PURE__ */ Object.create(null);\n      const keys = Object.keys(val);\n      for (let i2 = 0; i2 < keys.length; ++i2) {\n        const name = keys[i2];\n        headers[name.toLowerCase()] = [name, val[name]];\n      }\n    }\n  }\n  get connection() {\n    return null;\n  }\n  set connection(_socket) {\n    console.error(\"No support for OutgoingMessage.connection\");\n  }\n  get socket() {\n    return null;\n  }\n  set socket(_socket) {\n    console.error(\"No support for OutgoingMessage.socket\");\n  }\n  get _headerNames() {\n    console.warn(\"DEP0066: OutgoingMessage.prototype._headerNames is deprecated\");\n    const headers = this[kOutHeaders];\n    if (headers !== null) {\n      const out = /* @__PURE__ */ Object.create(null);\n      const keys = Object.keys(headers);\n      for (let i2 = 0; i2 < keys.length; ++i2) {\n        const key = keys[i2];\n        const val = headers[key][0];\n        out[key] = val;\n      }\n      return out;\n    }\n    return null;\n  }\n  set _headerNames(val) {\n    console.warn(\"DEP0066: OutgoingMessage.prototype._headerNames is deprecated\");\n    if (typeof val === \"object\" && val !== null) {\n      const headers = this[kOutHeaders];\n      if (!headers)\n        return;\n      const keys = Object.keys(val);\n      for (let i2 = 0; i2 < keys.length; ++i2) {\n        const header = headers[keys[i2]];\n        if (header)\n          header[0] = val[keys[i2]];\n      }\n    }\n  }\n  setHeader(name, value) {\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"set\");\n    }\n    validateHeaderName(name);\n    validateHeaderValue(name, value);\n    let headers = this[kOutHeaders];\n    if (headers === null) {\n      this[kOutHeaders] = headers = { __proto__: null };\n    }\n    headers[name.toLowerCase()] = [name, value];\n    return this;\n  }\n  setHeaders(headers) {\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"set\");\n    }\n    if (!headers || Array.isArray(headers) || typeof headers.keys !== \"function\" || typeof headers.get !== \"function\") {\n      throw new ERR_INVALID_ARG_TYPE(\"headers\", [\"Headers\", \"Map\"], headers);\n    }\n    const cookies = [];\n    for (const { 0: key, 1: value } of headers) {\n      if (key === \"set-cookie\") {\n        if (Array.isArray(value)) {\n          cookies.push(...value);\n        } else {\n          cookies.push(value);\n        }\n        continue;\n      }\n      this.setHeader(key, value);\n    }\n    if (cookies.length) {\n      this.setHeader(\"set-cookie\", cookies);\n    }\n    return this;\n  }\n  appendHeader(name, value) {\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"append\");\n    }\n    validateHeaderName(name);\n    validateHeaderValue(name, value);\n    const field = name.toLowerCase();\n    const headers = this[kOutHeaders];\n    if (headers === null || !headers[field]) {\n      return this.setHeader(name, value);\n    }\n    if (!Array.isArray(headers[field][1])) {\n      headers[field][1] = [headers[field][1]];\n    }\n    const existingValues = headers[field][1];\n    if (Array.isArray(value)) {\n      for (let i2 = 0, length = value.length; i2 < length; i2++) {\n        existingValues.push(value[i2]);\n      }\n    } else {\n      existingValues.push(value);\n    }\n    return this;\n  }\n  getHeader(name) {\n    validateString(name, \"name\");\n    const headers = this[kOutHeaders];\n    if (headers === null) {\n      return;\n    }\n    const entry = headers[name.toLowerCase()];\n    return entry?.[1];\n  }\n  getHeaderNames() {\n    return this[kOutHeaders] !== null ? Object.keys(this[kOutHeaders]) : [];\n  }\n  getRawHeaderNames() {\n    const headersMap = this[kOutHeaders];\n    if (headersMap === null)\n      return [];\n    const values = Object.values(headersMap);\n    const headers = Array(values.length);\n    for (let i2 = 0, l2 = values.length; i2 < l2; i2++) {\n      headers[i2] = values[i2][0];\n    }\n    return headers;\n  }\n  getHeaders() {\n    const headers = this[kOutHeaders];\n    const ret = { __proto__: null };\n    if (headers) {\n      const keys = Object.keys(headers);\n      for (let i2 = 0; i2 < keys.length; ++i2) {\n        const key = keys[i2];\n        const val = headers[key][1];\n        ret[key] = val;\n      }\n    }\n    return ret;\n  }\n  hasHeader(name) {\n    validateString(name, \"name\");\n    return this[kOutHeaders] !== null && !!this[kOutHeaders][name.toLowerCase()];\n  }\n  removeHeader(name) {\n    validateString(name, \"name\");\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"remove\");\n    }\n    const key = name.toLowerCase();\n    switch (key) {\n      case \"connection\":\n        this._removedConnection = true;\n        break;\n      case \"content-length\":\n        this._removedContLen = true;\n        break;\n      case \"transfer-encoding\":\n        this._removedTE = true;\n        break;\n      case \"date\":\n        this.sendDate = false;\n        break;\n    }\n    if (this[kOutHeaders] !== null) {\n      delete this[kOutHeaders][key];\n    }\n  }\n  _implicitHeader() {\n    throw new ERR_METHOD_NOT_IMPLEMENTED(\"_implicitHeader()\");\n  }\n  get headersSent() {\n    return !!this._header;\n  }\n  write(chunk, encoding, callback) {\n    if (typeof encoding === \"function\") {\n      callback = encoding;\n      encoding = null;\n    }\n    const ret = write_(this, chunk, encoding, callback, false);\n    if (!ret) {\n      this[kNeedDrain] = true;\n    }\n    return ret;\n  }\n  addTrailers(headers) {\n    this._trailer = \"\";\n    const isArray = Array.isArray(headers);\n    const keys = isArray ? [...headers.keys()] : Object.keys(headers);\n    for (let i2 = 0, l2 = keys.length; i2 < l2; i2++) {\n      let field, value;\n      if (isArray) {\n        const _headers = headers;\n        const key = keys[i2];\n        field = _headers[key][0];\n        value = _headers[key][1];\n      } else {\n        const _headers = headers;\n        const key = keys[i2];\n        field = key;\n        value = _headers[key];\n      }\n      validateHeaderName(field, \"Trailer name\");\n      if (Array.isArray(value) && value.length > 1 && (!this[kUniqueHeaders] || !this[kUniqueHeaders].has(field.toLowerCase()))) {\n        for (let j = 0, l3 = value.length; j < l3; j++) {\n          if (checkInvalidHeaderChar(value[j])) {\n            throw new ERR_INVALID_CHAR(\"trailer content\", field);\n          }\n          this._trailer += field + \": \" + value[j] + \"\\r\\n\";\n        }\n      } else {\n        if (Array.isArray(value)) {\n          value = value.join(\"; \");\n        }\n        if (checkInvalidHeaderChar(String(value))) {\n          throw new ERR_INVALID_CHAR(\"trailer content\", field);\n        }\n        this._trailer += field + \": \" + value + \"\\r\\n\";\n      }\n    }\n  }\n  end(chunk, encoding, callback) {\n    if (typeof chunk === \"function\") {\n      callback = chunk;\n      chunk = null;\n      encoding = null;\n    } else if (typeof encoding === \"function\") {\n      callback = encoding;\n      encoding = null;\n    }\n    if (chunk) {\n      if (this.finished) {\n        onError2(this, new ERR_STREAM_WRITE_AFTER_END(), typeof callback !== \"function\" ? nop : callback);\n        return this;\n      }\n      if (this._writtenDataBuffer != null) {\n        this._writtenDataBuffer.cork();\n      }\n      write_(this, chunk, encoding, null, true);\n    } else if (this.finished) {\n      if (typeof callback === \"function\") {\n        if (!this.writableFinished) {\n          this.on(\"finish\", callback);\n        } else {\n          callback(new ERR_STREAM_ALREADY_FINISHED(\"end\"));\n        }\n      }\n      return this;\n    } else if (!this._header) {\n      if (this._writtenDataBuffer != null) {\n        this._writtenDataBuffer.cork();\n      }\n      this._contentLength = 0;\n      this._implicitHeader();\n    }\n    if (typeof callback === \"function\")\n      this.once(\"finish\", callback);\n    if (strictContentLength(this) && this[kBytesWritten] !== this._contentLength) {\n      throw new ERR_HTTP_CONTENT_LENGTH_MISMATCH(this[kBytesWritten], this._contentLength);\n    }\n    const finish = onFinish.bind(void 0, this);\n    if (this._hasBody && this.chunkedEncoding) {\n      this._send(\"\", \"latin1\", finish);\n    } else if (!this._headerSent || this.writableLength || chunk) {\n      this._send(\"\", \"latin1\", finish);\n    } else {\n      setTimeout(finish, 0);\n    }\n    if (this._writtenDataBuffer != null) {\n      this._writtenDataBuffer.uncork();\n    }\n    this[kCorked] = 1;\n    this.uncork();\n    this.finished = true;\n    if (this.outputData.length === 0 && this._writtenDataBuffer != null) {\n      this._finish();\n    }\n    return this;\n  }\n  _finish() {\n    this.emit(\"prefinish\");\n  }\n  // No _flush() implementation?\n  _flush() {\n    if (this._writtenDataBuffer != null) {\n      const ret = this._flushOutput(this._writtenDataBuffer);\n      if (this.finished) {\n        this._finish();\n      } else if (ret && this[kNeedDrain]) {\n        this[kNeedDrain] = false;\n        this.emit(\"drain\");\n      }\n    }\n  }\n  _flushOutput(dataBuffer) {\n    while (this[kCorked]) {\n      this[kCorked]--;\n      dataBuffer.cork();\n    }\n    const outputLength = this.outputData.length;\n    if (outputLength <= 0) {\n      return void 0;\n    }\n    const outputData = this.outputData;\n    dataBuffer.cork();\n    let ret;\n    for (let i2 = 0; i2 < outputLength; i2++) {\n      const { data, encoding, callback } = outputData[i2];\n      outputData[i2].data = null;\n      ret = dataBuffer.write(data ?? \"\", encoding, callback);\n    }\n    dataBuffer.uncork();\n    this.outputData = [];\n    this._onPendingData(-this.outputSize);\n    this.outputSize = 0;\n    return ret;\n  }\n  flushHeaders() {\n    if (!this._header) {\n      this._implicitHeader();\n    }\n    this._send(\"\");\n  }\n  pipe(destination) {\n    this.emit(\"error\", new ERR_STREAM_CANNOT_PIPE());\n    return destination;\n  }\n};\nfunction processHeader(self, state, key, value, validate) {\n  if (validate) {\n    validateHeaderName(key);\n  }\n  if (isContentDispositionField(key) && self._contentLength) {\n    if (Array.isArray(value)) {\n      for (let i2 = 0; i2 < value.length; i2++) {\n        value[i2] = String(Buffer$1.from(String(value[i2]), \"latin1\"));\n      }\n    } else {\n      value = String(Buffer$1.from(String(value), \"latin1\"));\n    }\n  }\n  if (Array.isArray(value)) {\n    if ((value.length < 2 || !isCookieField(key)) && (!self[kUniqueHeaders] || !self[kUniqueHeaders].has(key.toLowerCase()))) {\n      for (let i2 = 0; i2 < value.length; i2++) {\n        storeHeader(self, state, key, value[i2], validate);\n      }\n      return;\n    }\n    value = value.join(\"; \");\n  }\n  storeHeader(self, state, key, String(value), validate);\n}\nfunction storeHeader(self, state, key, value, validate) {\n  if (validate) {\n    validateHeaderValue(key, value);\n  }\n  state.header += key + \": \" + value + \"\\r\\n\";\n  matchHeader(self, state, key, value);\n}\nfunction validateHeaderName(name, label) {\n  if (typeof name !== \"string\" || !name || !checkIsHttpToken(name)) {\n    throw new ERR_INVALID_HTTP_TOKEN(label || \"Header name\", name);\n  }\n}\nfunction validateHeaderValue(name, value) {\n  if (value === void 0) {\n    throw new ERR_HTTP_INVALID_HEADER_VALUE(String(value), name);\n  }\n  if (checkInvalidHeaderChar(String(value))) {\n    throw new ERR_INVALID_CHAR(\"header content\", name);\n  }\n}\nfunction matchHeader(self, state, field, value) {\n  if (field.length < 4 || field.length > 17)\n    return;\n  field = field.toLowerCase();\n  switch (field) {\n    case \"connection\":\n      state.connection = true;\n      self._removedConnection = false;\n      if (RE_CONN_CLOSE.exec(value) !== null)\n        self._last = true;\n      else\n        self.shouldKeepAlive = true;\n      break;\n    case \"transfer-encoding\":\n      state.te = true;\n      self._removedTE = false;\n      if (chunkExpression.exec(value) !== null)\n        self.chunkedEncoding = true;\n      break;\n    case \"content-length\":\n      state.contLen = true;\n      self._contentLength = +value;\n      self._removedContLen = false;\n      break;\n    case \"date\":\n    case \"expect\":\n    case \"trailer\":\n      state[field] = true;\n      break;\n    case \"keep-alive\":\n      self._defaultKeepAlive = false;\n      break;\n  }\n}\nfunction onError2(msg, err, callback) {\n  if (msg.destroyed) {\n    return;\n  }\n  setTimeout(emitErrorNt, 0, msg, err, callback);\n}\nfunction emitErrorNt(msg, err, callback) {\n  callback(err);\n  if (typeof msg.emit === \"function\" && !msg.destroyed) {\n    msg.emit(\"error\", err);\n  }\n}\nfunction strictContentLength(msg) {\n  return msg.strictContentLength && msg._contentLength != null && msg._hasBody && !msg._removedContLen && !msg.chunkedEncoding && !msg.hasHeader(\"transfer-encoding\");\n}\nfunction write_(msg, chunk, encoding, callback, fromEnd) {\n  if (typeof callback !== \"function\") {\n    callback = nop;\n  }\n  if (chunk === null) {\n    throw new ERR_STREAM_NULL_VALUES();\n  } else if (typeof chunk !== \"string\" && !isUint8Array(chunk)) {\n    throw new ERR_INVALID_ARG_TYPE(\"chunk\", [\"string\", \"Buffer\", \"Uint8Array\"], chunk);\n  }\n  let err = void 0;\n  if (msg.finished) {\n    err = new ERR_STREAM_WRITE_AFTER_END();\n  } else if (msg.destroyed) {\n    err = new ERR_STREAM_DESTROYED(\"write\");\n  }\n  if (err) {\n    if (!msg.destroyed) {\n      onError2(msg, err, callback);\n    } else {\n      setTimeout(callback, 0, err);\n    }\n    return false;\n  }\n  let len = void 0;\n  if (msg.strictContentLength) {\n    len ??= typeof chunk === \"string\" ? Buffer$1.byteLength(chunk, encoding ?? void 0) : chunk.byteLength;\n    if (strictContentLength(msg) && (fromEnd ? msg[kBytesWritten] + len !== msg._contentLength : msg[kBytesWritten] + len > (msg._contentLength ?? 0))) {\n      throw new ERR_HTTP_CONTENT_LENGTH_MISMATCH(len + msg[kBytesWritten], msg._contentLength);\n    }\n    msg[kBytesWritten] += len;\n  }\n  if (!msg._header) {\n    if (fromEnd) {\n      len ??= typeof chunk === \"string\" ? Buffer$1.byteLength(chunk, encoding ?? void 0) : chunk.byteLength;\n      msg._contentLength = len;\n    }\n    msg._implicitHeader();\n  }\n  if (!msg._hasBody) {\n    if (msg[kRejectNonStandardBodyWrites]) {\n      throw new ERR_HTTP_BODY_NOT_ALLOWED();\n    } else {\n      setTimeout(callback, 0);\n      return true;\n    }\n  }\n  if (!fromEnd && msg._writtenDataBuffer != null && !msg._writtenDataBuffer.writableCorked) {\n    msg._writtenDataBuffer.cork();\n    setTimeout(connectionCorkNT, 0, msg._writtenDataBuffer);\n  }\n  let ret;\n  if (msg.chunkedEncoding && chunk.length !== 0) {\n    len ??= typeof chunk === \"string\" ? Buffer$1.byteLength(chunk, encoding ?? void 0) : chunk.byteLength;\n    if (msg[kCorked] && msg._headerSent) {\n      msg[kChunkedBuffer].push({ data: chunk, encoding, callback });\n      msg[kChunkedLength] += len;\n      ret = msg[kChunkedLength] < msg[kHighWaterMark];\n    } else {\n      ret = msg._send(chunk, encoding, callback, len);\n    }\n  } else {\n    ret = msg._send(chunk, encoding, callback, len);\n  }\n  return ret;\n}\nfunction connectionCorkNT(dataBuffer) {\n  dataBuffer.uncork();\n}\nfunction onFinish(outmsg) {\n  outmsg.emit(\"finish\");\n}\nObject.defineProperties(FetchOutgoingMessage.prototype, {\n  errored: {\n    get() {\n      return this[kErrored];\n    }\n  },\n  closed: {\n    get() {\n      return this._closed;\n    }\n  },\n  writableFinished: {\n    get() {\n      return this.finished && this.outputSize === 0 && (this._writtenDataBuffer == null || this._writtenDataBuffer.writableLength === 0);\n    }\n  },\n  writableObjectMode: {\n    get() {\n      return false;\n    }\n  },\n  writableLength: {\n    get() {\n      return this.outputSize + this[kChunkedLength] + (this._writtenDataBuffer != null ? this._writtenDataBuffer.writableLength : 0);\n    }\n  },\n  writableHighWaterMark: {\n    get() {\n      return this._writtenDataBuffer != null ? this._writtenDataBuffer.writableHighWaterMark : this[kHighWaterMark];\n    }\n  },\n  writableCorked: {\n    get() {\n      return this[kCorked];\n    }\n  },\n  writableEnded: {\n    get() {\n      return this.finished;\n    }\n  },\n  writableNeedDrain: {\n    get() {\n      return !this.destroyed && !this.finished && this[kNeedDrain];\n    }\n  }\n});\nvar headerCharRegex2 = /[^\\t\\x20-\\x7e\\x80-\\xff]/;\nfunction checkInvalidHeaderChar2(val) {\n  return headerCharRegex2.test(val);\n}\nvar STATUS_CODES = {\n  100: \"Continue\",\n  // RFC 7231 6.2.1\n  101: \"Switching Protocols\",\n  // RFC 7231 6.2.2\n  102: \"Processing\",\n  // RFC 2518 10.1 (obsoleted by RFC 4918)\n  103: \"Early Hints\",\n  // RFC 8297 2\n  200: \"OK\",\n  // RFC 7231 6.3.1\n  201: \"Created\",\n  // RFC 7231 6.3.2\n  202: \"Accepted\",\n  // RFC 7231 6.3.3\n  203: \"Non-Authoritative Information\",\n  // RFC 7231 6.3.4\n  204: \"No Content\",\n  // RFC 7231 6.3.5\n  205: \"Reset Content\",\n  // RFC 7231 6.3.6\n  206: \"Partial Content\",\n  // RFC 7233 4.1\n  207: \"Multi-Status\",\n  // RFC 4918 11.1\n  208: \"Already Reported\",\n  // RFC 5842 7.1\n  226: \"IM Used\",\n  // RFC 3229 10.4.1\n  300: \"Multiple Choices\",\n  // RFC 7231 6.4.1\n  301: \"Moved Permanently\",\n  // RFC 7231 6.4.2\n  302: \"Found\",\n  // RFC 7231 6.4.3\n  303: \"See Other\",\n  // RFC 7231 6.4.4\n  304: \"Not Modified\",\n  // RFC 7232 4.1\n  305: \"Use Proxy\",\n  // RFC 7231 6.4.5\n  307: \"Temporary Redirect\",\n  // RFC 7231 6.4.7\n  308: \"Permanent Redirect\",\n  // RFC 7238 3\n  400: \"Bad Request\",\n  // RFC 7231 6.5.1\n  401: \"Unauthorized\",\n  // RFC 7235 3.1\n  402: \"Payment Required\",\n  // RFC 7231 6.5.2\n  403: \"Forbidden\",\n  // RFC 7231 6.5.3\n  404: \"Not Found\",\n  // RFC 7231 6.5.4\n  405: \"Method Not Allowed\",\n  // RFC 7231 6.5.5\n  406: \"Not Acceptable\",\n  // RFC 7231 6.5.6\n  407: \"Proxy Authentication Required\",\n  // RFC 7235 3.2\n  408: \"Request Timeout\",\n  // RFC 7231 6.5.7\n  409: \"Conflict\",\n  // RFC 7231 6.5.8\n  410: \"Gone\",\n  // RFC 7231 6.5.9\n  411: \"Length Required\",\n  // RFC 7231 6.5.10\n  412: \"Precondition Failed\",\n  // RFC 7232 4.2\n  413: \"Payload Too Large\",\n  // RFC 7231 6.5.11\n  414: \"URI Too Long\",\n  // RFC 7231 6.5.12\n  415: \"Unsupported Media Type\",\n  // RFC 7231 6.5.13\n  416: \"Range Not Satisfiable\",\n  // RFC 7233 4.4\n  417: \"Expectation Failed\",\n  // RFC 7231 6.5.14\n  418: \"I'm a Teapot\",\n  // RFC 7168 2.3.3\n  421: \"Misdirected Request\",\n  // RFC 7540 9.1.2\n  422: \"Unprocessable Entity\",\n  // RFC 4918 11.2\n  423: \"Locked\",\n  // RFC 4918 11.3\n  424: \"Failed Dependency\",\n  // RFC 4918 11.4\n  425: \"Too Early\",\n  // RFC 8470 5.2\n  426: \"Upgrade Required\",\n  // RFC 2817 and RFC 7231 6.5.15\n  428: \"Precondition Required\",\n  // RFC 6585 3\n  429: \"Too Many Requests\",\n  // RFC 6585 4\n  431: \"Request Header Fields Too Large\",\n  // RFC 6585 5\n  451: \"Unavailable For Legal Reasons\",\n  // RFC 7725 3\n  500: \"Internal Server Error\",\n  // RFC 7231 6.6.1\n  501: \"Not Implemented\",\n  // RFC 7231 6.6.2\n  502: \"Bad Gateway\",\n  // RFC 7231 6.6.3\n  503: \"Service Unavailable\",\n  // RFC 7231 6.6.4\n  504: \"Gateway Timeout\",\n  // RFC 7231 6.6.5\n  505: \"HTTP Version Not Supported\",\n  // RFC 7231 6.6.6\n  506: \"Variant Also Negotiates\",\n  // RFC 2295 8.1\n  507: \"Insufficient Storage\",\n  // RFC 4918 11.5\n  508: \"Loop Detected\",\n  // RFC 5842 7.2\n  509: \"Bandwidth Limit Exceeded\",\n  510: \"Not Extended\",\n  // RFC 2774 7\n  511: \"Network Authentication Required\"\n  // RFC 6585 6\n};\nvar FetchServerResponse = class _FetchServerResponse extends FetchOutgoingMessage {\n  static encoder = new TextEncoder();\n  statusCode = 200;\n  statusMessage;\n  _sent100;\n  _expect_continue;\n  [kOutHeaders] = null;\n  constructor(req, options) {\n    super(req, options);\n    if (req.method === \"HEAD\") {\n      this._hasBody = false;\n    }\n    this.sendDate = true;\n    this._sent100 = false;\n    this._expect_continue = false;\n    if (req.httpVersionMajor < 1 || req.httpVersionMinor < 1) {\n      this.useChunkedEncodingByDefault = chunkExpression.exec(String(req.headers.te)) !== null;\n      this.shouldKeepAlive = false;\n    }\n    this.fetchResponse = new Promise((resolve) => {\n      let finished = false;\n      this.on(\"finish\", () => {\n        finished = true;\n      });\n      const initialDataChunks = [];\n      const initialDataWrittenHandler = (e2) => {\n        if (finished) {\n          return;\n        }\n        initialDataChunks[e2.index] = this.dataFromDataWrittenEvent(e2);\n      };\n      this.on(\"_dataWritten\", initialDataWrittenHandler);\n      this.on(\"_headersSent\", (e2) => {\n        this.off(\"_dataWritten\", initialDataWrittenHandler);\n        const { statusCode, statusMessage, headers } = e2;\n        resolve(this._toFetchResponse(statusCode, statusMessage, headers, initialDataChunks, finished));\n      });\n    });\n  }\n  dataFromDataWrittenEvent(e2) {\n    const { index, entry } = e2;\n    let { data, encoding } = entry;\n    if (index === 0) {\n      if (typeof data !== \"string\") {\n        console.error(\"First chunk should be string, not sure what happened.\");\n        throw new ERR_INVALID_ARG_TYPE(\"packet.data\", [\"string\", \"Buffer\", \"Uint8Array\"], data);\n      }\n      data = data.slice(this.writtenHeaderBytes);\n    }\n    if (typeof data === \"string\") {\n      if (encoding === void 0 || encoding === \"utf8\" || encoding === \"utf-8\") {\n        data = _FetchServerResponse.encoder.encode(data);\n      } else {\n        data = Buffer$1.from(data, encoding ?? void 0);\n      }\n    }\n    return data ?? Buffer$1.from([]);\n  }\n  _finish() {\n    super._finish();\n  }\n  assignSocket(socket) {\n    throw new ERR_METHOD_NOT_IMPLEMENTED(\"assignSocket\");\n  }\n  detachSocket(socket) {\n    throw new ERR_METHOD_NOT_IMPLEMENTED(\"detachSocket\");\n  }\n  writeContinue(callback) {\n    this._writeRaw(\"HTTP/1.1 100 Continue\\r\\n\\r\\n\", \"ascii\", callback);\n    this._sent100 = true;\n  }\n  writeProcessing(callback) {\n    this._writeRaw(\"HTTP/1.1 102 Processing\\r\\n\\r\\n\", \"ascii\", callback);\n  }\n  writeEarlyHints(hints, callback) {\n    let head = \"HTTP/1.1 103 Early Hints\\r\\n\";\n    if (hints.link === null || hints.link === void 0) {\n      return;\n    }\n    const link = validateLinkHeaderValue(hints.link);\n    if (link.length === 0) {\n      return;\n    }\n    head += \"Link: \" + link + \"\\r\\n\";\n    for (const key of Object.keys(hints)) {\n      if (key !== \"link\") {\n        head += key + \": \" + hints[key] + \"\\r\\n\";\n      }\n    }\n    head += \"\\r\\n\";\n    this._writeRaw(head, \"ascii\", callback);\n  }\n  _implicitHeader() {\n    this.writeHead(this.statusCode);\n  }\n  writeHead(statusCode, reason, obj) {\n    if (this._header) {\n      throw new ERR_HTTP_HEADERS_SENT(\"write\");\n    }\n    const originalStatusCode = statusCode;\n    statusCode |= 0;\n    if (statusCode < 100 || statusCode > 999) {\n      throw new ERR_HTTP_INVALID_STATUS_CODE(originalStatusCode);\n    }\n    if (typeof reason === \"string\") {\n      this.statusMessage = reason;\n    } else {\n      this.statusMessage ||= STATUS_CODES[statusCode] || \"unknown\";\n      obj ??= reason;\n    }\n    this.statusCode = statusCode;\n    let headers;\n    if (this[kOutHeaders]) {\n      let k;\n      if (Array.isArray(obj)) {\n        if (obj.length % 2 !== 0) {\n          throw new ERR_INVALID_ARG_VALUE(\"headers\", obj);\n        }\n        for (let n2 = 0; n2 < obj.length; n2 += 2) {\n          k = obj[n2 + 0];\n          this.removeHeader(String(k));\n        }\n        for (let n2 = 0; n2 < obj.length; n2 += 2) {\n          k = obj[n2];\n          if (k) {\n            this.appendHeader(String(k), obj[n2 + 1]);\n          }\n        }\n      } else if (obj) {\n        const keys = Object.keys(obj);\n        for (let i2 = 0; i2 < keys.length; i2++) {\n          k = keys[i2];\n          if (k) {\n            this.setHeader(k, obj[k]);\n          }\n        }\n      }\n      headers = this[kOutHeaders];\n    } else {\n      headers = obj;\n    }\n    if (checkInvalidHeaderChar2(this.statusMessage)) {\n      throw new ERR_INVALID_CHAR(\"statusMessage\");\n    }\n    const statusLine = `HTTP/1.1 ${statusCode} ${this.statusMessage}\\r\n`;\n    if (statusCode === 204 || statusCode === 304 || statusCode >= 100 && statusCode <= 199) {\n      this._hasBody = false;\n    }\n    if (this._expect_continue && !this._sent100) {\n      this.shouldKeepAlive = false;\n    }\n    const convertedHeaders = headers && !Array.isArray(headers) ? headers : headers;\n    this._storeHeader(statusLine, convertedHeaders ?? null);\n    return this;\n  }\n  // Docs-only deprecated: DEP0063\n  writeHeader = this.writeHead;\n  fetchResponse;\n  _toFetchResponse(status, statusText, sentHeaders, initialDataChunks, finished) {\n    const headers = new Headers();\n    for (const [header, value] of sentHeaders) {\n      headers.append(header, value);\n    }\n    const _this = this;\n    let body = this._hasBody ? new ReadableStream({\n      start(controller) {\n        for (const dataChunk of initialDataChunks) {\n          controller.enqueue(dataChunk);\n        }\n        if (finished) {\n          controller.close();\n        } else {\n          _this.on(\"finish\", () => {\n            finished = true;\n            controller.close();\n          });\n          _this.on(\"_dataWritten\", (e2) => {\n            if (finished) {\n              return;\n            }\n            const data = _this.dataFromDataWrittenEvent(e2);\n            controller.enqueue(data);\n          });\n        }\n      }\n    }) : null;\n    if (body != null && typeof FixedLengthStream !== \"undefined\") {\n      const contentLength = parseInt(headers.get(\"content-length\") ?? \"\", 10);\n      if (contentLength >= 0) {\n        body = body.pipeThrough(new FixedLengthStream(contentLength));\n      }\n    }\n    return new Response(body, {\n      status,\n      statusText,\n      headers\n    });\n  }\n};\nfunction toReqRes(req, options) {\n  const { createIncomingMessage = () => new FetchIncomingMessage(), createServerResponse = (incoming2) => new FetchServerResponse(incoming2), ctx } = {};\n  const incoming = createIncomingMessage(ctx);\n  const serverResponse = createServerResponse(incoming, ctx);\n  const reqUrl = new URL(req.url);\n  const versionMajor = 1;\n  const versionMinor = 1;\n  incoming.httpVersionMajor = versionMajor;\n  incoming.httpVersionMinor = versionMinor;\n  incoming.httpVersion = `${versionMajor}.${versionMinor}`;\n  incoming.url = reqUrl.pathname + reqUrl.search;\n  incoming.upgrade = false;\n  const headers = [];\n  for (const [headerName, headerValue] of req.headers) {\n    headers.push(headerName);\n    headers.push(headerValue);\n  }\n  incoming._addHeaderLines(headers, headers.length);\n  incoming.method = req.method;\n  incoming._stream = req.body;\n  return {\n    req: incoming,\n    res: serverResponse\n  };\n}\nfunction toFetchResponse(res) {\n  if (!(res instanceof FetchServerResponse)) {\n    throw new Error(\"toFetchResponse must be called on a ServerResponse generated by toReqRes\");\n  }\n  return res.fetchResponse;\n}\n\n// src/server/handlers/routes/mcp/handlers.ts\nvar getMastra = (c2) => c2.get(\"mastra\");\nvar getMcpServerMessageHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"serverId\");\n  const { req, res } = toReqRes(c2.req.raw);\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    res.writeHead(404, { \"Content-Type\": \"application/json\" });\n    res.end(JSON.stringify({ error: `MCP server '${serverId}' not found` }));\n    return;\n  }\n  try {\n    await server.startHTTP({\n      url: new URL(c2.req.url),\n      httpPath: `/api/mcp/${serverId}/mcp`,\n      req,\n      res\n    });\n    return await toFetchResponse(res);\n  } catch (error) {\n    if (!res.headersSent) {\n      res.writeHead(500, { \"Content-Type\": \"application/json\" });\n      res.end(\n        JSON.stringify({\n          jsonrpc: \"2.0\",\n          error: {\n            code: -32603,\n            message: \"Internal server error\"\n          },\n          id: null\n          // Cannot determine original request ID in catch\n        })\n      );\n    } else {\n      c2.get(\"logger\")?.error(\"Error after headers sent:\", error);\n    }\n  }\n};\nvar getMcpServerSseHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"serverId\");\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    return c2.json({ error: `MCP server '${serverId}' not found` }, 404);\n  }\n  const requestUrl = new URL(c2.req.url);\n  const sseConnectionPath = `/api/mcp/${serverId}/sse`;\n  const sseMessagePath = `/api/mcp/${serverId}/messages`;\n  try {\n    return await server.startHonoSSE({\n      url: requestUrl,\n      ssePath: sseConnectionPath,\n      messagePath: sseMessagePath,\n      context: c2\n    });\n  } catch (error) {\n    c2.get(\"logger\")?.error({ err: error, serverId, path: requestUrl.pathname }, \"Error in MCP SSE route handler\");\n    return handleError(error, \"Error handling MCP SSE request\");\n  }\n};\nvar listMcpRegistryServersHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  if (!mastra || typeof mastra.getMCPServers !== \"function\") {\n    c2.get(\"logger\")?.error(\"Mastra instance or getMCPServers method not available in listMcpRegistryServersHandler\");\n    return c2.json({ error: \"Mastra instance or getMCPServers method not available\" }, 500);\n  }\n  const mcpServersMap = mastra.getMCPServers();\n  if (!mcpServersMap) {\n    c2.get(\"logger\")?.warn(\"getMCPServers returned undefined or null in listMcpRegistryServersHandler\");\n    return c2.json({ servers: [], next: null, total_count: 0 });\n  }\n  const allServersArray = Array.from(\n    mcpServersMap instanceof Map ? mcpServersMap.values() : Object.values(mcpServersMap)\n  );\n  const limit = parseInt(c2.req.query(\"limit\") || \"50\", 10);\n  const offset = parseInt(c2.req.query(\"offset\") || \"0\", 10);\n  const paginatedServers = allServersArray.slice(offset, offset + limit);\n  const serverInfos = paginatedServers.map((server) => server.getServerInfo());\n  const total_count = allServersArray.length;\n  let next = null;\n  if (offset + limit < total_count) {\n    const nextOffset = offset + limit;\n    const currentUrl = new URL(c2.req.url);\n    currentUrl.searchParams.set(\"offset\", nextOffset.toString());\n    currentUrl.searchParams.set(\"limit\", limit.toString());\n    next = currentUrl.toString();\n  }\n  return c2.json({\n    servers: serverInfos,\n    next,\n    total_count\n  });\n};\nvar getMcpRegistryServerDetailHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"id\");\n  const requestedVersion = c2.req.query(\"version\");\n  if (!mastra || typeof mastra.getMCPServer !== \"function\") {\n    c2.get(\"logger\")?.error(\"Mastra instance or getMCPServer method not available in getMcpRegistryServerDetailHandler\");\n    return c2.json({ error: \"Mastra instance or getMCPServer method not available\" }, 500);\n  }\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    return c2.json({ error: `MCP server with ID '${serverId}' not found` }, 404);\n  }\n  const serverDetailInfo = server.getServerDetail();\n  if (requestedVersion && serverDetailInfo.version_detail.version !== requestedVersion) {\n    c2.get(\"logger\")?.info(\n      `MCP server with ID '${serverId}' found, but version '${serverDetailInfo.version_detail.version}' does not match requested version '${requestedVersion}'.`\n    );\n    return c2.json(\n      {\n        error: `MCP server with ID '${serverId}' found, but not version '${requestedVersion}'. Available version is '${serverDetailInfo.version_detail.version}'.`\n      },\n      404\n      // Return 404 as the specific version is not found\n    );\n  }\n  return c2.json(serverDetailInfo);\n};\nvar listMcpServerToolsHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"serverId\");\n  if (!mastra || typeof mastra.getMCPServer !== \"function\") {\n    c2.get(\"logger\")?.error(\"Mastra instance or getMCPServer method not available in listMcpServerToolsHandler\");\n    return c2.json({ error: \"Mastra instance or getMCPServer method not available\" }, 500);\n  }\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    return c2.json({ error: `MCP server with ID '${serverId}' not found` }, 404);\n  }\n  if (typeof server.getToolListInfo !== \"function\") {\n    c2.get(\"logger\")?.error(`MCPServer with ID '${serverId}' does not support getToolListInfo.`);\n    return c2.json({ error: `Server '${serverId}' cannot list tools in this way.` }, 501);\n  }\n  try {\n    const toolListInfo = server.getToolListInfo();\n    return c2.json(toolListInfo);\n  } catch (error) {\n    c2.get(\"logger\")?.error(`Error in listMcpServerToolsHandler for serverId '${serverId}':`, { error: error.message });\n    return handleError(error, `Error listing tools for MCP server '${serverId}'`);\n  }\n};\nvar getMcpServerToolDetailHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"serverId\");\n  const toolId = c2.req.param(\"toolId\");\n  if (!mastra || typeof mastra.getMCPServer !== \"function\") {\n    c2.get(\"logger\")?.error(\"Mastra instance or getMCPServer method not available in getMcpServerToolDetailHandler\");\n    return c2.json({ error: \"Mastra instance or getMCPServer method not available\" }, 500);\n  }\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    return c2.json({ error: `MCP server with ID '${serverId}' not found` }, 404);\n  }\n  if (typeof server.getToolInfo !== \"function\") {\n    c2.get(\"logger\")?.error(`MCPServer with ID '${serverId}' does not support getToolInfo.`);\n    return c2.json({ error: `Server '${serverId}' cannot provide tool details in this way.` }, 501);\n  }\n  try {\n    const toolInfo = server.getToolInfo(toolId);\n    if (!toolInfo) {\n      return c2.json({ error: `Tool with ID '${toolId}' not found on MCP server '${serverId}'` }, 404);\n    }\n    return c2.json(toolInfo);\n  } catch (error) {\n    c2.get(\"logger\")?.error(`Error in getMcpServerToolDetailHandler for serverId '${serverId}', toolId '${toolId}':`, {\n      error: error.message\n    });\n    return handleError(error, `Error getting tool '${toolId}' details for MCP server '${serverId}'`);\n  }\n};\nvar executeMcpServerToolHandler = async (c2) => {\n  const mastra = getMastra(c2);\n  const serverId = c2.req.param(\"serverId\");\n  const toolId = c2.req.param(\"toolId\");\n  if (!mastra || typeof mastra.getMCPServer !== \"function\") {\n    c2.get(\"logger\")?.error(\"Mastra instance or getMCPServer method not available in executeMcpServerToolHandler\");\n    return c2.json({ error: \"Mastra instance or getMCPServer method not available\" }, 500);\n  }\n  const server = mastra.getMCPServer(serverId);\n  if (!server) {\n    return c2.json({ error: `MCP server with ID '${serverId}' not found` }, 404);\n  }\n  if (typeof server.executeTool !== \"function\") {\n    c2.get(\"logger\")?.error(`MCPServer with ID '${serverId}' does not support executeTool.`);\n    return c2.json({ error: `Server '${serverId}' cannot execute tools in this way.` }, 501);\n  }\n  try {\n    const body = await c2.req.json();\n    const args = body?.data;\n    const runtimeContext = body?.runtimeContext;\n    const result = await server.executeTool(toolId, args, runtimeContext);\n    return c2.json({ result });\n  } catch (error) {\n    c2.get(\"logger\")?.error(`Error executing tool '${toolId}' on server '${serverId}':`, { error: error.message });\n    if (error.name === \"ZodError\") {\n      return c2.json({ error: \"Invalid tool arguments\", details: error.errors }, 400);\n    }\n    return handleError(error, `Error executing tool '${toolId}' on MCP server '${serverId}'`);\n  }\n};\n\n// src/server/handlers/routes/mcp/router.ts\nfunction mcpRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.post(\n    \"/:serverId/mcp\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Send a message to an MCP server using Streamable HTTP\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"serverId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        content: { \"application/json\": { schema: { type: \"object\" } } }\n      },\n      responses: {\n        200: {\n          description: \"Streamable HTTP connection processed\"\n        },\n        404: {\n          description: \"MCP server not found\"\n        }\n      }\n    }),\n    getMcpServerMessageHandler\n  );\n  router.get(\n    \"/:serverId/mcp\",\n    w({\n      description: \"Send a message to an MCP server using Streamable HTTP\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"serverId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Streamable HTTP connection processed\"\n        },\n        404: {\n          description: \"MCP server not found\"\n        }\n      }\n    }),\n    getMcpServerMessageHandler\n  );\n  const mcpSseBasePath = \"/:serverId/sse\";\n  const mcpSseMessagePath = \"/:serverId/messages\";\n  router.get(\n    mcpSseBasePath,\n    w({\n      description: \"Establish an MCP Server-Sent Events (SSE) connection with a server instance.\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"serverId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The ID of the MCP server instance.\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"SSE connection established. The client will receive events over this connection. (Content-Type: text/event-stream)\"\n        },\n        404: { description: \"MCP server instance not found.\" },\n        500: { description: \"Internal server error establishing SSE connection.\" }\n      }\n    }),\n    getMcpServerSseHandler\n  );\n  router.post(\n    mcpSseMessagePath,\n    bodyLimit(bodyLimitOptions),\n    // Apply body limit for messages\n    w({\n      description: \"Send a message to an MCP server over an established SSE connection.\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"serverId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The ID of the MCP server instance.\"\n        }\n      ],\n      requestBody: {\n        description: \"JSON-RPC message to send to the MCP server.\",\n        required: true,\n        content: { \"application/json\": { schema: { type: \"object\" } } }\n        // MCP messages are typically JSON\n      },\n      responses: {\n        200: {\n          description: \"Message received and is being processed by the MCP server. The actual result or error will be sent as an SSE event over the established connection.\"\n        },\n        400: { description: \"Bad request (e.g., invalid JSON payload or missing body).\" },\n        404: { description: \"MCP server instance not found or SSE connection path incorrect.\" },\n        503: { description: \"SSE connection not established with this server, or server unable to process message.\" }\n      }\n    }),\n    getMcpServerSseHandler\n  );\n  router.get(\n    \"/v0/servers\",\n    w({\n      description: \"List all available MCP server instances with basic information.\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"limit\",\n          in: \"query\",\n          description: \"Number of results per page.\",\n          required: false,\n          schema: { type: \"integer\", default: 50, minimum: 1, maximum: 5e3 }\n        },\n        {\n          name: \"offset\",\n          in: \"query\",\n          description: \"Number of results to skip for pagination.\",\n          required: false,\n          schema: { type: \"integer\", default: 0, minimum: 0 }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"A list of MCP server instances.\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  servers: {\n                    type: \"array\",\n                    items: {\n                      type: \"object\",\n                      properties: {\n                        id: { type: \"string\" },\n                        name: { type: \"string\" },\n                        description: { type: \"string\" },\n                        repository: {\n                          type: \"object\",\n                          properties: {\n                            url: { type: \"string\", description: \"The URL of the repository (e.g., a GitHub URL)\" },\n                            source: {\n                              type: \"string\",\n                              description: \"The source control platform (e.g., 'github', 'gitlab')\",\n                              enum: [\"github\", \"gitlab\"]\n                            },\n                            id: { type: \"string\", description: \"A unique identifier for the repository at the source\" }\n                          }\n                        },\n                        version_detail: {\n                          type: \"object\",\n                          properties: {\n                            version: { type: \"string\", description: 'The semantic version string (e.g., \"1.0.2\")' },\n                            release_date: {\n                              type: \"string\",\n                              description: \"The ISO 8601 date-time string when this version was released or registered\"\n                            },\n                            is_latest: {\n                              type: \"boolean\",\n                              description: \"Indicates if this version is the latest available\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  },\n                  next: { type: \"string\", format: \"uri\", nullable: true },\n                  total_count: { type: \"integer\" }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    listMcpRegistryServersHandler\n  );\n  router.get(\n    \"/v0/servers/:id\",\n    w({\n      description: \"Get detailed information about a specific MCP server instance.\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"id\",\n          in: \"path\",\n          required: true,\n          description: \"Unique ID of the MCP server instance.\",\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"version\",\n          in: \"query\",\n          required: false,\n          description: \"Desired MCP server version (currently informational, server returns its actual version).\",\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Detailed information about the MCP server instance.\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  id: { type: \"string\" },\n                  name: { type: \"string\" },\n                  description: { type: \"string\" },\n                  repository: {\n                    type: \"object\",\n                    properties: {\n                      url: { type: \"string\" },\n                      source: { type: \"string\" },\n                      id: { type: \"string\" }\n                    }\n                  },\n                  version_detail: {\n                    type: \"object\",\n                    properties: {\n                      version: { type: \"string\" },\n                      release_date: { type: \"string\" },\n                      is_latest: { type: \"boolean\" }\n                    }\n                  },\n                  package_canonical: { type: \"string\" },\n                  packages: {\n                    type: \"array\",\n                    items: {\n                      type: \"object\",\n                      properties: {\n                        registry_name: { type: \"string\" },\n                        name: { type: \"string\" },\n                        version: { type: \"string\" },\n                        command: {\n                          type: \"object\",\n                          properties: {\n                            name: { type: \"string\" },\n                            subcommands: {\n                              type: \"array\",\n                              items: {\n                                type: \"object\",\n                                properties: {\n                                  name: { type: \"string\" },\n                                  description: { type: \"string\" },\n                                  is_required: { type: \"boolean\" },\n                                  subcommands: {\n                                    type: \"array\",\n                                    items: { type: \"object\" }\n                                  },\n                                  positional_arguments: {\n                                    type: \"array\",\n                                    items: { type: \"object\" }\n                                  },\n                                  named_arguments: {\n                                    type: \"array\",\n                                    items: { type: \"object\" }\n                                  }\n                                }\n                              }\n                            },\n                            positional_arguments: {\n                              type: \"array\",\n                              items: { type: \"object\" }\n                            },\n                            named_arguments: {\n                              type: \"array\",\n                              items: { type: \"object\" }\n                            }\n                          }\n                        },\n                        environment_variables: {\n                          type: \"array\",\n                          items: {\n                            type: \"object\",\n                            properties: {\n                              name: { type: \"string\" },\n                              description: { type: \"string\" },\n                              required: { type: \"boolean\" },\n                              default_value: { type: \"string\" }\n                            }\n                          }\n                        }\n                      }\n                    }\n                  },\n                  remotes: {\n                    type: \"array\",\n                    items: {\n                      type: \"object\",\n                      properties: {\n                        transport_type: { type: \"string\" },\n                        url: { type: \"string\" }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        404: {\n          description: \"MCP server instance not found.\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  error: { type: \"string\" }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    getMcpRegistryServerDetailHandler\n  );\n  router.get(\n    \"/:serverId/tools\",\n    w({\n      description: \"List all tools available on a specific MCP server instance.\",\n      tags: [\"mcp\"],\n      parameters: [\n        {\n          name: \"serverId\",\n          in: \"path\",\n          required: true,\n          description: \"Unique ID of the MCP server instance.\",\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: { description: \"A list of tools for the MCP server.\" },\n        // Define schema if you have one for McpServerToolListResponse\n        404: { description: \"MCP server instance not found.\" },\n        501: { description: \"Server does not support listing tools.\" }\n      }\n    }),\n    listMcpServerToolsHandler\n  );\n  router.get(\n    \"/:serverId/tools/:toolId\",\n    w({\n      description: \"Get details for a specific tool on an MCP server.\",\n      tags: [\"mcp\"],\n      parameters: [\n        { name: \"serverId\", in: \"path\", required: true, schema: { type: \"string\" } },\n        { name: \"toolId\", in: \"path\", required: true, schema: { type: \"string\" } }\n      ],\n      responses: {\n        200: { description: \"Details of the specified tool.\" },\n        // Define schema for McpToolInfo\n        404: { description: \"MCP server or tool not found.\" },\n        501: { description: \"Server does not support getting tool details.\" }\n      }\n    }),\n    getMcpServerToolDetailHandler\n  );\n  router.post(\n    \"/:serverId/tools/:toolId/execute\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Execute a specific tool on an MCP server.\",\n      tags: [\"mcp\"],\n      parameters: [\n        { name: \"serverId\", in: \"path\", required: true, schema: { type: \"string\" } },\n        { name: \"toolId\", in: \"path\", required: true, schema: { type: \"string\" } }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                data: { type: \"object\" },\n                runtimeContext: { type: \"object\" }\n              }\n            }\n          }\n        }\n        // Simplified schema\n      },\n      responses: {\n        200: { description: \"Result of the tool execution.\" },\n        400: { description: \"Invalid tool arguments.\" },\n        404: { description: \"MCP server or tool not found.\" },\n        501: { description: \"Server does not support tool execution.\" }\n      }\n    }),\n    executeMcpServerToolHandler\n  );\n  return router;\n}\n\n// src/server/handlers/utils/query-parsers.ts\nfunction parseLimit(rawLimit) {\n  if (rawLimit === void 0) {\n    return void 0;\n  }\n  const n2 = Number(rawLimit);\n  if (Number.isFinite(n2) && Number.isInteger(n2) && n2 > 0) {\n    return n2;\n  }\n  return void 0;\n}\n\n// src/server/handlers/routes/memory/handlers.ts\nasync function getMemoryStatusHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const result = await getMemoryStatusHandler$1({\n      mastra,\n      agentId,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting memory status\");\n  }\n}\nasync function getMemoryConfigHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const result = await getMemoryConfigHandler$1({\n      mastra,\n      agentId,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting memory configuration\");\n  }\n}\nasync function getThreadsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const resourceId = c2.req.query(\"resourceid\");\n    const networkId = c2.req.query(\"networkId\");\n    const orderBy = c2.req.query(\"orderBy\");\n    const sortDirection = c2.req.query(\"sortDirection\");\n    const result = await getThreadsHandler$1({\n      mastra,\n      agentId,\n      resourceId,\n      networkId,\n      orderBy,\n      sortDirection\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting threads\");\n  }\n}\nasync function getThreadsPaginatedHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const resourceId = c2.req.query(\"resourceId\");\n    const networkId = c2.req.query(\"networkId\");\n    const page = parseInt(c2.req.query(\"page\") || \"0\", 10);\n    const perPage = parseInt(c2.req.query(\"perPage\") || \"100\", 10);\n    const orderBy = c2.req.query(\"orderBy\");\n    const sortDirection = c2.req.query(\"sortDirection\");\n    const result = await getThreadsPaginatedHandler$1({\n      mastra,\n      agentId,\n      resourceId,\n      networkId,\n      page,\n      perPage,\n      orderBy,\n      sortDirection\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting paginated threads\");\n  }\n}\nasync function getThreadByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const threadId = c2.req.param(\"threadId\");\n    const networkId = c2.req.query(\"networkId\");\n    const result = await getThreadByIdHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting thread\");\n  }\n}\nasync function saveMessagesHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const body = await c2.req.json();\n    const result = await saveMessagesHandler$1({\n      mastra,\n      agentId,\n      body,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error saving messages\");\n  }\n}\nasync function createThreadHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const body = await c2.req.json();\n    const result = await createThreadHandler$1({\n      mastra,\n      agentId,\n      body,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error saving thread to memory\");\n  }\n}\nasync function updateThreadHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const threadId = c2.req.param(\"threadId\");\n    const networkId = c2.req.query(\"networkId\");\n    const body = await c2.req.json();\n    const result = await updateThreadHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      body,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error updating thread\");\n  }\n}\nasync function deleteThreadHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const threadId = c2.req.param(\"threadId\");\n    const networkId = c2.req.query(\"networkId\");\n    const result = await deleteThreadHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error deleting thread\");\n  }\n}\nasync function getMessagesHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const threadId = c2.req.param(\"threadId\");\n    const limit = parseLimit(c2.req.query(\"limit\"));\n    const result = await getMessagesHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      networkId,\n      limit\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting messages\");\n  }\n}\nasync function getMessagesPaginatedHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const threadId = c2.req.param(\"threadId\");\n    const resourceId = c2.req.query(\"resourceId\");\n    const format = c2.req.query(\"format\") || \"v1\";\n    const selectByArgs = c2.req.query(\"selectBy\");\n    let selectBy = {};\n    if (selectByArgs) {\n      try {\n        selectBy = JSON.parse(selectByArgs);\n      } catch (_error) {\n      }\n    }\n    const result = await getMessagesPaginatedHandler$1({\n      mastra,\n      threadId,\n      resourceId,\n      format,\n      selectBy\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting messages\");\n  }\n}\nasync function updateWorkingMemoryHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const threadId = c2.req.param(\"threadId\");\n    const networkId = c2.req.query(\"networkId\");\n    const body = await c2.req.json();\n    const result = await updateWorkingMemoryHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      body,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error updating working memory\");\n  }\n}\nasync function getWorkingMemoryHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const threadId = c2.req.param(\"threadId\");\n    const resourceId = c2.req.query(\"resourceId\");\n    const networkId = c2.req.query(\"networkId\");\n    const result = await getWorkingMemoryHandler$1({\n      mastra,\n      agentId,\n      threadId,\n      resourceId,\n      networkId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting working memory\");\n  }\n}\nasync function searchMemoryHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const searchQuery = c2.req.query(\"searchQuery\");\n    const resourceId = c2.req.query(\"resourceId\");\n    const threadId = c2.req.query(\"threadId\");\n    const limit = parseLimit(c2.req.query(\"limit\"));\n    const memoryConfig = c2.req.query(\"memoryConfig\") ? JSON.parse(c2.req.query(\"memoryConfig\")) : void 0;\n    const networkId = c2.req.query(\"networkId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const result = await searchMemoryHandler$1({\n      mastra,\n      agentId,\n      searchQuery,\n      resourceId,\n      threadId,\n      limit,\n      memoryConfig,\n      networkId,\n      runtimeContext\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error searching memory\");\n  }\n}\nasync function deleteMessagesHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const agentId = c2.req.query(\"agentId\");\n    const networkId = c2.req.query(\"networkId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const body = await c2.req.json();\n    const messageIds = body?.messageIds;\n    const result = await deleteMessagesHandler$1({\n      mastra,\n      agentId,\n      messageIds,\n      networkId,\n      runtimeContext\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error deleting messages\");\n  }\n}\n\n// src/server/handlers/routes/memory/router.ts\nfunction memoryRoutes(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/network/status\",\n    w({\n      description: \"Get network memory status\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Memory status\"\n        }\n      }\n    }),\n    getMemoryStatusHandler\n  );\n  router.get(\n    \"/network/threads\",\n    w({\n      description: \"Get all threads\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"resourceid\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"orderBy\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"createdAt\", \"updatedAt\"],\n            default: \"createdAt\"\n          },\n          description: \"Field to sort by\"\n        },\n        {\n          name: \"sortDirection\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"ASC\", \"DESC\"],\n            default: \"DESC\"\n          },\n          description: \"Sort direction\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of all threads\"\n        }\n      }\n    }),\n    getThreadsHandler\n  );\n  router.get(\n    \"/network/threads/:threadId\",\n    w({\n      description: \"Get thread by ID\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Thread details\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    getThreadByIdHandler\n  );\n  router.get(\n    \"/network/threads/:threadId/messages\",\n    w({\n      description: \"Get messages for a thread\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"limit\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Limit the number of messages to retrieve (default: 40)\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of messages\"\n        }\n      }\n    }),\n    getMessagesHandler\n  );\n  router.post(\n    \"/network/threads\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Create a new thread\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                title: { type: \"string\" },\n                metadata: { type: \"object\" },\n                resourceId: { type: \"string\" },\n                threadId: { type: \"string\" }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Created thread\"\n        }\n      }\n    }),\n    createThreadHandler\n  );\n  router.patch(\n    \"/network/threads/:threadId\",\n    w({\n      description: \"Update a thread\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: { type: \"object\" }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Updated thread\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    updateThreadHandler\n  );\n  router.delete(\n    \"/network/threads/:threadId\",\n    w({\n      description: \"Delete a thread\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Thread deleted\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    deleteThreadHandler\n  );\n  router.post(\n    \"/network/save-messages\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Save messages\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  description: \"Array of messages in either v1 or v2 format\",\n                  items: {\n                    oneOf: [\n                      {\n                        type: \"object\",\n                        description: \"Mastra Message v1 format\",\n                        properties: {\n                          id: { type: \"string\" },\n                          content: { type: \"string\" },\n                          role: { type: \"string\", enum: [\"user\", \"assistant\", \"system\", \"tool\"] },\n                          type: { type: \"string\", enum: [\"text\", \"tool-call\", \"tool-result\"] },\n                          createdAt: { type: \"string\", format: \"date-time\" },\n                          threadId: { type: \"string\" },\n                          resourceId: { type: \"string\" }\n                        },\n                        required: [\"content\", \"role\", \"type\", \"threadId\", \"resourceId\"]\n                      },\n                      {\n                        type: \"object\",\n                        description: \"Mastra Message v2 format\",\n                        properties: {\n                          id: { type: \"string\" },\n                          role: { type: \"string\", enum: [\"user\", \"assistant\"] },\n                          createdAt: { type: \"string\", format: \"date-time\" },\n                          threadId: { type: \"string\" },\n                          resourceId: { type: \"string\" },\n                          content: {\n                            type: \"object\",\n                            properties: {\n                              format: { type: \"number\", enum: [2] },\n                              parts: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              },\n                              content: { type: \"string\" },\n                              toolInvocations: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              },\n                              experimental_attachments: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              }\n                            },\n                            required: [\"format\", \"parts\"]\n                          }\n                        },\n                        required: [\"role\", \"content\", \"threadId\", \"resourceId\"]\n                      }\n                    ]\n                  }\n                }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Messages saved\"\n        }\n      }\n    }),\n    saveMessagesHandler\n  );\n  router.post(\n    \"/network/messages/delete\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Delete one or more messages\",\n      tags: [\"networkMemory\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messageIds: {\n                  oneOf: [\n                    { type: \"string\" },\n                    {\n                      type: \"array\",\n                      items: { type: \"string\" }\n                    },\n                    {\n                      type: \"object\",\n                      properties: { id: { type: \"string\" } },\n                      required: [\"id\"]\n                    },\n                    {\n                      type: \"array\",\n                      items: {\n                        type: \"object\",\n                        properties: { id: { type: \"string\" } },\n                        required: [\"id\"]\n                      }\n                    }\n                  ]\n                }\n              },\n              required: [\"messageIds\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Messages deleted successfully\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  success: { type: \"boolean\" },\n                  message: { type: \"string\" }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    deleteMessagesHandler\n  );\n  router.get(\n    \"/status\",\n    w({\n      description: \"Get memory status\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Memory status\"\n        }\n      }\n    }),\n    getMemoryStatusHandler\n  );\n  router.get(\n    \"/config\",\n    w({\n      description: \"Get memory configuration\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Memory configuration\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  config: {\n                    type: \"object\",\n                    properties: {\n                      lastMessages: {\n                        oneOf: [{ type: \"number\" }, { type: \"boolean\" }]\n                      },\n                      semanticRecall: {\n                        oneOf: [\n                          { type: \"boolean\" },\n                          {\n                            type: \"object\",\n                            properties: {\n                              topK: { type: \"number\" },\n                              messageRange: {\n                                oneOf: [\n                                  { type: \"number\" },\n                                  {\n                                    type: \"object\",\n                                    properties: {\n                                      before: { type: \"number\" },\n                                      after: { type: \"number\" }\n                                    }\n                                  }\n                                ]\n                              },\n                              scope: { type: \"string\", enum: [\"thread\", \"resource\"] }\n                            }\n                          }\n                        ]\n                      },\n                      workingMemory: {\n                        type: \"object\",\n                        properties: {\n                          enabled: { type: \"boolean\" },\n                          scope: { type: \"string\", enum: [\"thread\", \"resource\"] },\n                          template: { type: \"string\" }\n                        }\n                      },\n                      threads: {\n                        type: \"object\",\n                        properties: {\n                          generateTitle: {\n                            oneOf: [{ type: \"boolean\" }, { type: \"object\" }]\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    getMemoryConfigHandler\n  );\n  router.get(\n    \"/threads\",\n    w({\n      description: \"Get all threads\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"resourceid\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"orderBy\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"createdAt\", \"updatedAt\"],\n            default: \"createdAt\"\n          },\n          description: \"Field to sort by\"\n        },\n        {\n          name: \"sortDirection\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"ASC\", \"DESC\"],\n            default: \"DESC\"\n          },\n          description: \"Sort direction\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of all threads\"\n        }\n      }\n    }),\n    getThreadsHandler\n  );\n  router.get(\n    \"/threads/paginated\",\n    w({\n      description: \"Get paginated threads\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"resourceId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\", default: 0 },\n          description: \"Page number\"\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\", default: 100 },\n          description: \"Number of threads per page\"\n        },\n        {\n          name: \"orderBy\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"createdAt\", \"updatedAt\"],\n            default: \"createdAt\"\n          }\n        },\n        {\n          name: \"sortDirection\",\n          in: \"query\",\n          required: false,\n          schema: {\n            type: \"string\",\n            enum: [\"ASC\", \"DESC\"],\n            default: \"DESC\"\n          }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of threads\"\n        }\n      }\n    }),\n    getThreadsPaginatedHandler\n  );\n  router.get(\n    \"/threads/:threadId\",\n    w({\n      description: \"Get thread by ID\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Thread details\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    getThreadByIdHandler\n  );\n  router.get(\n    \"/threads/:threadId/messages\",\n    async (c2, next) => {\n      c2.header(\"Deprecation\", \"true\");\n      c2.header(\n        \"Warning\",\n        '299 - \"This endpoint is deprecated, use /api/memory/threads/:threadId/messages/paginated instead\"'\n      );\n      c2.header(\"Link\", '</api/memory/threads/:threadId/messages/paginated>; rel=\"successor-version\"');\n      return next();\n    },\n    w({\n      description: \"Get messages for a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"limit\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Limit the number of messages to retrieve (default: 40)\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of messages\"\n        }\n      }\n    }),\n    getMessagesHandler\n  );\n  router.get(\n    \"/threads/:threadId/messages/paginated\",\n    w({\n      description: \"Get paginated messages for a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          description: \"The unique identifier of the thread\",\n          schema: {\n            type: \"string\"\n          }\n        },\n        {\n          name: \"resourceId\",\n          in: \"query\",\n          required: false,\n          description: \"Filter messages by resource ID\",\n          schema: {\n            type: \"string\"\n          }\n        },\n        {\n          name: \"format\",\n          in: \"query\",\n          required: false,\n          description: \"Message format to return\",\n          schema: {\n            type: \"string\",\n            enum: [\"v1\", \"v2\"],\n            default: \"v1\"\n          }\n        },\n        {\n          name: \"selectBy\",\n          in: \"query\",\n          required: false,\n          description: \"JSON string containing selection criteria for messages\",\n          schema: {\n            type: \"string\",\n            example: '{\"pagination\":{\"page\":0,\"perPage\":20,\"dateRange\":{\"start\":\"2024-01-01T00:00:00Z\",\"end\":\"2024-12-31T23:59:59Z\"}},\"include\":[{\"id\":\"msg-123\",\"withPreviousMessages\":5,\"withNextMessages\":3}]}'\n          }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of messages\"\n        }\n      }\n    }),\n    getMessagesPaginatedHandler\n  );\n  router.get(\n    \"/search\",\n    w({\n      description: \"Search messages in a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"searchQuery\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The text to search for\"\n        },\n        {\n          name: \"resourceId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The resource ID (user/org) to validate thread ownership\"\n        },\n        {\n          name: \"threadId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" },\n          description: \"The thread ID to search within (optional - searches all threads if not provided)\"\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The agent ID\"\n        },\n        {\n          name: \"limit\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Maximum number of results to return (default: 20)\"\n        },\n        {\n          name: \"memoryConfig\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" },\n          description: 'JSON-encoded memory configuration (e.g., {\"lastMessages\": 0} for semantic-only search)'\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Search results\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  results: {\n                    type: \"array\",\n                    items: {\n                      type: \"object\",\n                      properties: {\n                        id: { type: \"string\" },\n                        role: { type: \"string\" },\n                        content: { type: \"string\" },\n                        createdAt: { type: \"string\" }\n                      }\n                    }\n                  },\n                  count: { type: \"number\" },\n                  query: { type: \"string\" }\n                }\n              }\n            }\n          }\n        },\n        400: {\n          description: \"Bad request\"\n        },\n        403: {\n          description: \"Thread does not belong to the specified resource\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    searchMemoryHandler\n  );\n  router.get(\n    \"/threads/:threadId/working-memory\",\n    w({\n      description: \"Get working memory for a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"resourceId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Working memory details\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    getWorkingMemoryHandler\n  );\n  router.post(\n    \"/threads/:threadId/working-memory\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Update working memory for a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                workingMemory: { type: \"string\" },\n                resourceId: { type: \"string\" }\n              },\n              required: [\"workingMemory\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Working memory updated successfully\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    updateWorkingMemoryHandler\n  );\n  router.post(\n    \"/threads\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Create a new thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                title: { type: \"string\" },\n                metadata: { type: \"object\" },\n                resourceId: { type: \"string\" },\n                threadId: { type: \"string\" }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Created thread\"\n        }\n      }\n    }),\n    createThreadHandler\n  );\n  router.patch(\n    \"/threads/:threadId\",\n    w({\n      description: \"Update a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: { type: \"object\" }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Updated thread\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    updateThreadHandler\n  );\n  router.delete(\n    \"/threads/:threadId\",\n    w({\n      description: \"Delete a thread\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"threadId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Thread deleted\"\n        },\n        404: {\n          description: \"Thread not found\"\n        }\n      }\n    }),\n    deleteThreadHandler\n  );\n  router.post(\n    \"/save-messages\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Save messages\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messages: {\n                  type: \"array\",\n                  description: \"Array of messages in either v1 or v2 format\",\n                  items: {\n                    oneOf: [\n                      {\n                        type: \"object\",\n                        description: \"Mastra Message v1 format\",\n                        properties: {\n                          id: { type: \"string\" },\n                          content: { type: \"string\" },\n                          role: { type: \"string\", enum: [\"user\", \"assistant\", \"system\", \"tool\"] },\n                          type: { type: \"string\", enum: [\"text\", \"tool-call\", \"tool-result\"] },\n                          createdAt: { type: \"string\", format: \"date-time\" },\n                          threadId: { type: \"string\" },\n                          resourceId: { type: \"string\" }\n                        },\n                        required: [\"content\", \"role\", \"type\", \"threadId\", \"resourceId\"]\n                      },\n                      {\n                        type: \"object\",\n                        description: \"Mastra Message v2 format\",\n                        properties: {\n                          id: { type: \"string\" },\n                          role: { type: \"string\", enum: [\"user\", \"assistant\"] },\n                          createdAt: { type: \"string\", format: \"date-time\" },\n                          threadId: { type: \"string\" },\n                          resourceId: { type: \"string\" },\n                          content: {\n                            type: \"object\",\n                            properties: {\n                              format: { type: \"number\", enum: [2] },\n                              parts: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              },\n                              content: { type: \"string\" },\n                              toolInvocations: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              },\n                              experimental_attachments: {\n                                type: \"array\",\n                                items: { type: \"object\" }\n                              }\n                            },\n                            required: [\"format\", \"parts\"]\n                          }\n                        },\n                        required: [\"role\", \"content\", \"threadId\", \"resourceId\"]\n                      }\n                    ]\n                  }\n                }\n              },\n              required: [\"messages\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Messages saved\"\n        }\n      }\n    }),\n    saveMessagesHandler\n  );\n  router.post(\n    \"/messages/delete\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Delete one or more messages\",\n      tags: [\"memory\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                messageIds: {\n                  oneOf: [\n                    { type: \"string\" },\n                    {\n                      type: \"array\",\n                      items: { type: \"string\" }\n                    },\n                    {\n                      type: \"object\",\n                      properties: { id: { type: \"string\" } },\n                      required: [\"id\"]\n                    },\n                    {\n                      type: \"array\",\n                      items: {\n                        type: \"object\",\n                        properties: { id: { type: \"string\" } },\n                        required: [\"id\"]\n                      }\n                    }\n                  ]\n                }\n              },\n              required: [\"messageIds\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Messages deleted successfully\",\n          content: {\n            \"application/json\": {\n              schema: {\n                type: \"object\",\n                properties: {\n                  success: { type: \"boolean\" },\n                  message: { type: \"string\" }\n                }\n              }\n            }\n          }\n        }\n      }\n    }),\n    deleteMessagesHandler\n  );\n  return router;\n}\nasync function getNetworksHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networks = await getNetworksHandler$1({\n      mastra,\n      runtimeContext\n    });\n    return c2.json(networks);\n  } catch (error) {\n    return handleError(error, \"Error getting networks\");\n  }\n}\nasync function getNetworkByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const networkId = c2.req.param(\"networkId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const network = await getNetworkByIdHandler$1({\n      mastra,\n      networkId,\n      runtimeContext\n    });\n    return c2.json(network);\n  } catch (error) {\n    return handleError(error, \"Error getting network by ID\");\n  }\n}\nasync function generateHandler2(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    const result = await generateHandler$2({\n      mastra,\n      runtimeContext,\n      networkId,\n      body\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error generating from network\");\n  }\n}\nasync function streamGenerateHandler2(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    const streamResponse = await streamGenerateHandler$2({\n      mastra,\n      runtimeContext,\n      networkId,\n      body\n    });\n    return streamResponse;\n  } catch (error) {\n    return handleError(error, \"Error streaming from network\");\n  }\n}\nasync function getVNextNetworksHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networks = await getVNextNetworksHandler$1({\n      mastra,\n      runtimeContext\n    });\n    return c2.json(networks);\n  } catch (error) {\n    return handleError(error, \"Error getting networks\");\n  }\n}\nasync function getVNextNetworkByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const networkId = c2.req.param(\"networkId\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const network = await getVNextNetworkByIdHandler$1({\n      mastra,\n      networkId,\n      runtimeContext\n    });\n    return c2.json(network);\n  } catch (error) {\n    return handleError(error, \"Error getting network by ID\");\n  }\n}\nasync function generateVNextNetworkHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    const result = await generateVNextNetworkHandler$1({\n      mastra,\n      runtimeContext,\n      networkId,\n      body\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error generating from network\");\n  }\n}\nasync function streamGenerateVNextNetworkHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const logger2 = mastra.getLogger();\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await streamGenerateVNextNetworkHandler$1({\n            mastra,\n            runtimeContext,\n            networkId,\n            body\n          });\n          const reader = result.stream.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(JSON.stringify(chunkResult.value) + \"\u001e\");\n          }\n        } catch (err) {\n          mastra.getLogger().error(\"Error in network stream: \" + (err?.message ?? \"Unknown error\"));\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in network stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error streaming from network\");\n  }\n}\nasync function loopVNextNetworkHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    const result = await loopVNextNetworkHandler$1({\n      mastra,\n      runtimeContext,\n      networkId,\n      body\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error looping from network\");\n  }\n}\nasync function loopStreamVNextNetworkHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const logger2 = mastra.getLogger();\n    const networkId = c2.req.param(\"networkId\");\n    const body = await c2.req.json();\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await loopStreamVNextNetworkHandler$1({\n            mastra,\n            runtimeContext,\n            networkId,\n            body\n          });\n          const reader = result.stream.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(JSON.stringify(chunkResult.value) + \"\u001e\");\n          }\n        } catch (err) {\n          mastra.getLogger().error(\"Error in network loop stream: \" + (err?.message ?? \"Unknown error\"));\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in network loop stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error streaming network loop\");\n  }\n}\n\n// src/server/handlers/routes/networks/router.ts\nfunction vNextNetworksRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/v-next\",\n    w({\n      description: \"Get all available v-next networks\",\n      tags: [\"vNextNetworks\"],\n      responses: {\n        200: {\n          description: \"List of all v-next networks\"\n        }\n      }\n    }),\n    getVNextNetworksHandler\n  );\n  router.get(\n    \"/v-next/:networkId\",\n    w({\n      description: \"Get v-next network by ID\",\n      tags: [\"vNextNetworks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"v-next Network details\"\n        },\n        404: {\n          description: \"v-next Network not found\"\n        }\n      }\n    }),\n    getVNextNetworkByIdHandler\n  );\n  router.post(\n    \"/v-next/:networkId/generate\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from a v-next network\",\n      tags: [\"vNextNetworks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                message: {\n                  type: \"string\",\n                  description: \"Message for the v-next network\"\n                },\n                threadId: {\n                  type: \"string\",\n                  description: \"Thread Id of the conversation\"\n                },\n                resourceId: {\n                  type: \"string\",\n                  description: \"Resource Id of the conversation\"\n                }\n              },\n              required: [\"message\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"v-next Network not found\"\n        }\n      }\n    }),\n    generateVNextNetworkHandler\n  );\n  router.post(\n    \"/v-next/:networkId/loop\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Loop a v-next network\",\n      tags: [\"vNextNetworks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                message: {\n                  type: \"string\",\n                  description: \"Message for the v-next network\"\n                }\n              },\n              required: [\"message\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Looped response\"\n        },\n        404: {\n          description: \"v-next Network not found\"\n        }\n      }\n    }),\n    loopVNextNetworkHandler\n  );\n  router.post(\n    \"/v-next/:networkId/loop-stream\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Stream a v-next network loop\",\n      tags: [\"vNextNetworks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                message: {\n                  type: \"string\",\n                  description: \"Message for the v-next network\"\n                },\n                threadId: {\n                  type: \"string\",\n                  description: \"Thread Id of the conversation\"\n                },\n                resourceId: {\n                  type: \"string\",\n                  description: \"Resource Id of the conversation\"\n                },\n                maxIterations: {\n                  type: \"number\",\n                  description: \"Maximum number of iterations to run\"\n                }\n              },\n              required: [\"message\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Streamed response\"\n        },\n        404: {\n          description: \"v-next Network not found\"\n        }\n      }\n    }),\n    loopStreamVNextNetworkHandler\n  );\n  router.post(\n    \"/v-next/:networkId/stream\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Stream a response from a v-next network\",\n      tags: [\"vNextNetworks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                message: {\n                  type: \"string\",\n                  description: \"Message for the v-next network\"\n                },\n                threadId: {\n                  type: \"string\",\n                  description: \"Thread Id of the conversation\"\n                },\n                resourceId: {\n                  type: \"string\",\n                  description: \"Resource Id of the conversation\"\n                }\n              },\n              required: [\"message\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Streamed response\"\n        },\n        404: {\n          description: \"v-next Network not found\"\n        }\n      }\n    }),\n    streamGenerateVNextNetworkHandler\n  );\n  return router;\n}\nfunction networksRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all available networks\",\n      tags: [\"networks\"],\n      responses: {\n        200: {\n          description: \"List of all networks\"\n        }\n      }\n    }),\n    getNetworksHandler\n  );\n  router.get(\n    \"/:networkId\",\n    w({\n      description: \"Get network by ID\",\n      tags: [\"networks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Network details\"\n        },\n        404: {\n          description: \"Network not found\"\n        }\n      }\n    }),\n    getNetworkByIdHandler\n  );\n  router.post(\n    \"/:networkId/generate\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from a network\",\n      tags: [\"networks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                input: {\n                  oneOf: [\n                    { type: \"string\" },\n                    {\n                      type: \"array\",\n                      items: { type: \"object\" }\n                    }\n                  ],\n                  description: \"Input for the network, can be a string or an array of CoreMessage objects\"\n                }\n              },\n              required: [\"input\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Network not found\"\n        }\n      }\n    }),\n    generateHandler2\n  );\n  router.post(\n    \"/:networkId/stream\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Generate a response from a network\",\n      tags: [\"networks\"],\n      parameters: [\n        {\n          name: \"networkId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                input: {\n                  oneOf: [\n                    { type: \"string\" },\n                    {\n                      type: \"array\",\n                      items: { type: \"object\" }\n                    }\n                  ],\n                  description: \"Input for the network, can be a string or an array of CoreMessage objects\"\n                }\n              },\n              required: [\"input\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Generated response\"\n        },\n        404: {\n          description: \"Network not found\"\n        }\n      }\n    }),\n    streamGenerateHandler2\n  );\n  return router;\n}\nasync function getAITraceHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const traceId = c2.req.param(\"traceId\");\n    if (!traceId) {\n      return c2.json({ error: \"Trace ID is required\" }, 400);\n    }\n    const trace = await getAITraceHandler$1({\n      mastra,\n      traceId\n    });\n    return c2.json(trace);\n  } catch (error) {\n    return handleError(error, \"Error getting AI trace\");\n  }\n}\nasync function getAITracesPaginatedHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const { page, perPage, name, spanType, start, end } = c2.req.query();\n    const pagination = {\n      page: parseInt(page || \"0\"),\n      perPage: parseInt(perPage || \"10\")\n    };\n    const filters = {};\n    if (name) filters.name = name;\n    if (spanType) {\n      if (Object.values(AISpanType).includes(spanType)) {\n        filters.spanType = spanType;\n      } else {\n        return c2.json({ error: \"Invalid spanType\" }, 400);\n      }\n    }\n    const dateRange = {};\n    if (start) {\n      try {\n        dateRange.start = new Date(start);\n      } catch {\n        return c2.json({ error: \"Invalid start date\" }, 400);\n      }\n    }\n    if (end) {\n      try {\n        dateRange.end = new Date(end);\n      } catch {\n        return c2.json({ error: \"Invalid end date\" }, 400);\n      }\n    }\n    if (Object.keys(dateRange).length > 0) {\n      pagination.dateRange = dateRange;\n    }\n    const result = await getAITracesPaginatedHandler$1({\n      mastra,\n      body: {\n        pagination,\n        filters\n      }\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error getting AI traces paginated\");\n  }\n}\n\n// src/server/handlers/routes/observability/router.ts\nfunction observabilityRouter() {\n  const router = new Hono();\n  router.get(\n    \"/traces\",\n    w({\n      description: \"Get paginated list of AI traces\",\n      tags: [\"observability\"],\n      parameters: [\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Page number for pagination (default: 0)\"\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Number of items per page (default: 10)\"\n        },\n        {\n          name: \"name\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" },\n          description: \"Filter traces by name\"\n        },\n        {\n          name: \"spanType\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Filter traces by span type\"\n        },\n        {\n          name: \"dateRange\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" },\n          description: \"JSON string with start and end dates for filtering\"\n        },\n        {\n          name: \"attributes\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" },\n          description: \"JSON string with attributes to filter by\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of AI traces\"\n        },\n        400: {\n          description: \"Bad request - invalid parameters\"\n        }\n      }\n    }),\n    getAITracesPaginatedHandler\n  );\n  router.get(\n    \"/traces/:traceId\",\n    w({\n      description: \"Get a specific AI trace by ID\",\n      tags: [\"observability\"],\n      parameters: [\n        {\n          name: \"traceId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"The ID of the trace to retrieve\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"AI trace with all its spans\"\n        },\n        400: {\n          description: \"Bad request - missing trace ID\"\n        },\n        404: {\n          description: \"Trace not found\"\n        }\n      }\n    }),\n    getAITraceHandler\n  );\n  return router;\n}\nasync function getScorersHandler(c2) {\n  try {\n    const scorers = await getScorersHandler$1({\n      mastra: c2.get(\"mastra\"),\n      runtimeContext: c2.get(\"runtimeContext\")\n    });\n    return c2.json(scorers);\n  } catch (error) {\n    return handleError(error, \"Error getting scorers\");\n  }\n}\nasync function getScorerHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const scorerId = c2.req.param(\"scorerId\");\n  const runtimeContext = c2.get(\"runtimeContext\");\n  const scorer = await getScorerHandler$1({\n    mastra,\n    scorerId,\n    runtimeContext\n  });\n  return c2.json(scorer);\n}\nasync function getScoresByRunIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const runId = c2.req.param(\"runId\");\n  const page = parseInt(c2.req.query(\"page\") || \"0\");\n  const perPage = parseInt(c2.req.query(\"perPage\") || \"10\");\n  const pagination = { page, perPage };\n  try {\n    const scores = await getScoresByRunIdHandler$1({\n      mastra,\n      runId,\n      pagination\n    });\n    return c2.json(scores);\n  } catch (error) {\n    return handleError(error, \"Error getting scores by run id\");\n  }\n}\nasync function getScoresByScorerIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const scorerId = c2.req.param(\"scorerId\");\n  const page = parseInt(c2.req.query(\"page\") || \"0\");\n  const perPage = parseInt(c2.req.query(\"perPage\") || \"10\");\n  const entityId = c2.req.query(\"entityId\");\n  const entityType = c2.req.query(\"entityType\");\n  const pagination = { page, perPage };\n  try {\n    const scores = await getScoresByScorerIdHandler$1({\n      mastra,\n      scorerId,\n      pagination,\n      entityId,\n      entityType\n    });\n    return c2.json(scores);\n  } catch (error) {\n    return handleError(error, \"Error getting scores by scorer id\");\n  }\n}\nasync function getScoresByEntityIdHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const entityId = c2.req.param(\"entityId\");\n  const entityType = c2.req.param(\"entityType\");\n  const page = parseInt(c2.req.query(\"page\") || \"0\");\n  const perPage = parseInt(c2.req.query(\"perPage\") || \"10\");\n  const pagination = { page, perPage };\n  try {\n    const scores = await getScoresByEntityIdHandler$1({\n      mastra,\n      entityId,\n      entityType,\n      pagination\n    });\n    return c2.json(scores);\n  } catch (error) {\n    return handleError(error, \"Error getting scores by entity id\");\n  }\n}\nasync function saveScoreHandler(c2) {\n  const mastra = c2.get(\"mastra\");\n  const score = await c2.req.json();\n  try {\n    const result = await saveScoreHandler$1({\n      mastra,\n      score\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error saving score\");\n  }\n}\n\n// src/server/handlers/routes/scores/router.ts\nfunction scoresRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/scorers\",\n    w({\n      description: \"Get all scorers\",\n      tags: [\"scores\"],\n      responses: {\n        200: {\n          description: \"List of all scorers\"\n        }\n      }\n    }),\n    getScorersHandler\n  );\n  router.get(\n    \"/scorers/:scorerId\",\n    w({\n      description: \"Get a scorer by ID\",\n      tags: [\"scores\"],\n      responses: {\n        200: {\n          description: \"Scorer details\"\n        }\n      }\n    }),\n    getScorerHandler\n  );\n  router.get(\n    \"/run/:runId\",\n    w({\n      description: \"Get scores by run ID\",\n      tags: [\"scores\"],\n      parameters: [\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Page number for pagination (default: 0)\"\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Number of items per page (default: 10)\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of scores for run ID\"\n        }\n      }\n    }),\n    getScoresByRunIdHandler\n  );\n  router.get(\n    \"/scorer/:scorerId\",\n    w({\n      description: \"Get scores by scorer ID\",\n      tags: [\"scores\"],\n      parameters: [\n        {\n          name: \"scorerId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Page number for pagination (default: 0)\"\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Number of items per page (default: 10)\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of scores for run ID\"\n        }\n      }\n    }),\n    getScoresByScorerIdHandler\n  );\n  router.get(\n    \"/entity/:entityType/:entityId\",\n    w({\n      description: \"Get scores by entity ID and type\",\n      tags: [\"scores\"],\n      parameters: [\n        {\n          name: \"entityType\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"Type of entity (e.g., agent, workflow, tool)\"\n        },\n        {\n          name: \"entityId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" },\n          description: \"ID of the entity\"\n        },\n        {\n          name: \"page\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Page number for pagination (default: 0)\"\n        },\n        {\n          name: \"perPage\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"number\" },\n          description: \"Number of items per page (default: 10)\"\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Paginated list of scores for entity\"\n        }\n      }\n    }),\n    getScoresByEntityIdHandler\n  );\n  router.post(\n    \"/\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Save a score\",\n      tags: [\"scores\"],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                id: { type: \"string\" },\n                runId: { type: \"string\" },\n                scorer: { type: \"object\" },\n                result: { type: \"object\" },\n                input: { type: \"object\" },\n                output: { type: \"object\" },\n                source: { type: \"string\" },\n                entityType: { type: \"string\" },\n                entity: { type: \"object\" },\n                metadata: { type: \"object\" },\n                additionalLLMContext: { type: \"object\" },\n                runtimeContext: { type: \"object\" },\n                resourceId: { type: \"string\" },\n                threadId: { type: \"string\" },\n                traceId: { type: \"string\" }\n              },\n              required: [\"id\", \"runId\", \"scorer\", \"result\", \"input\", \"output\", \"source\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Score saved successfully\"\n        },\n        400: {\n          description: \"Invalid score data\"\n        }\n      }\n    }),\n    saveScoreHandler\n  );\n  return router;\n}\nasync function getTelemetryHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const { name, scope, page, perPage, fromDate, toDate } = c2.req.query();\n    const attribute = c2.req.queries(\"attribute\");\n    const traces = await getTelemetryHandler$1({\n      mastra,\n      body: {\n        name,\n        scope,\n        page: Number(page ?? 0),\n        perPage: Number(perPage ?? 100),\n        attribute,\n        fromDate: fromDate ? new Date(fromDate) : void 0,\n        toDate: toDate ? new Date(toDate) : void 0\n      }\n    });\n    return c2.json({ traces });\n  } catch (error) {\n    return handleError(error, \"Error getting telemetry traces\");\n  }\n}\nasync function storeTelemetryHandler(c2) {\n  try {\n    const body = await c2.req.json();\n    const mastra = c2.get(\"mastra\");\n    const result = await storeTelemetryHandler$1({ mastra, body });\n    if (result.status === \"error\") {\n      return c2.json(result, 500);\n    }\n    return c2.json(result, 200);\n  } catch (error) {\n    return handleError(error, \"Error storing telemetry traces\");\n  }\n}\n\n// src/server/handlers/routes/telemetry/router.ts\nfunction telemetryRouter() {\n  const router = new Hono();\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all traces\",\n      tags: [\"telemetry\"],\n      responses: {\n        200: {\n          description: \"List of all traces (paged)\"\n        }\n      }\n    }),\n    getTelemetryHandler\n  );\n  router.post(\n    \"/\",\n    w({\n      description: \"Store telemetry\",\n      tags: [\"telemetry\"],\n      responses: {\n        200: {\n          description: \"Traces stored\"\n        }\n      }\n    }),\n    storeTelemetryHandler\n  );\n  return router;\n}\nfunction toolsRouter(bodyLimitOptions, tools) {\n  const router = new Hono();\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all tools\",\n      tags: [\"tools\"],\n      responses: {\n        200: {\n          description: \"List of all tools\"\n        }\n      }\n    }),\n    getToolsHandler\n  );\n  router.get(\n    \"/:toolId\",\n    w({\n      description: \"Get tool by ID\",\n      tags: [\"tools\"],\n      parameters: [\n        {\n          name: \"toolId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Tool details\"\n        },\n        404: {\n          description: \"Tool not found\"\n        }\n      }\n    }),\n    getToolByIdHandler\n  );\n  router.post(\n    \"/:toolId/execute\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Execute a tool\",\n      tags: [\"tools\"],\n      parameters: [\n        {\n          name: \"toolId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                data: { type: \"object\" },\n                runtimeContext: { type: \"object\" }\n              },\n              required: [\"data\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Tool execution result\"\n        },\n        404: {\n          description: \"Tool not found\"\n        }\n      }\n    }),\n    executeToolHandler(tools)\n  );\n  return router;\n}\nasync function upsertVectors(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const body = await c2.req.json();\n    const result = await upsertVectors$1({\n      mastra,\n      vectorName,\n      index: body\n    });\n    return c2.json({ ids: result });\n  } catch (error) {\n    return handleError(error, \"Error upserting vectors\");\n  }\n}\nasync function createIndex(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const body = await c2.req.json();\n    await createIndex$1({\n      mastra,\n      vectorName,\n      index: body\n    });\n    return c2.json({ success: true });\n  } catch (error) {\n    return handleError(error, \"Error creating index\");\n  }\n}\nasync function queryVectors(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const { indexName, queryVector, topK = 10, filter, includeVector = false } = await c2.req.json();\n    const results = await queryVectors$1({\n      mastra,\n      vectorName,\n      query: { indexName, queryVector, topK, filter, includeVector }\n    });\n    return c2.json({ results });\n  } catch (error) {\n    return handleError(error, \"Error querying vectors\");\n  }\n}\nasync function listIndexes(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const indexes = await listIndexes$1({\n      mastra,\n      vectorName\n    });\n    return c2.json({ indexes });\n  } catch (error) {\n    return handleError(error, \"Error listing indexes\");\n  }\n}\nasync function describeIndex(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const indexName = c2.req.param(\"indexName\");\n    if (!indexName) {\n      throw new HTTPException(400, { message: \"Index name is required\" });\n    }\n    const stats = await describeIndex$1({\n      mastra,\n      vectorName,\n      indexName\n    });\n    return c2.json({\n      dimension: stats.dimension,\n      count: stats.count,\n      metric: stats.metric?.toLowerCase()\n    });\n  } catch (error) {\n    return handleError(error, \"Error describing index\");\n  }\n}\nasync function deleteIndex(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const vectorName = c2.req.param(\"vectorName\");\n    const indexName = c2.req.param(\"indexName\");\n    if (!indexName) {\n      throw new HTTPException(400, { message: \"Index name is required\" });\n    }\n    await deleteIndex$1({\n      mastra,\n      vectorName,\n      indexName\n    });\n    return c2.json({ success: true });\n  } catch (error) {\n    return handleError(error, \"Error deleting index\");\n  }\n}\n\n// src/server/handlers/routes/vector/router.ts\nfunction vectorRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.post(\n    \"/:vectorName/upsert\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Upsert vectors into an index\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                indexName: { type: \"string\" },\n                vectors: {\n                  type: \"array\",\n                  items: {\n                    type: \"array\",\n                    items: { type: \"number\" }\n                  }\n                },\n                metadata: {\n                  type: \"array\",\n                  items: { type: \"object\" }\n                },\n                ids: {\n                  type: \"array\",\n                  items: { type: \"string\" }\n                }\n              },\n              required: [\"indexName\", \"vectors\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Vectors upserted successfully\"\n        }\n      }\n    }),\n    upsertVectors\n  );\n  router.post(\n    \"/:vectorName/create-index\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Create a new vector index\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                indexName: { type: \"string\" },\n                dimension: { type: \"number\" },\n                metric: {\n                  type: \"string\",\n                  enum: [\"cosine\", \"euclidean\", \"dotproduct\"]\n                }\n              },\n              required: [\"indexName\", \"dimension\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Index created successfully\"\n        }\n      }\n    }),\n    createIndex\n  );\n  router.post(\n    \"/:vectorName/query\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Query vectors from an index\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                indexName: { type: \"string\" },\n                queryVector: {\n                  type: \"array\",\n                  items: { type: \"number\" }\n                },\n                topK: { type: \"number\" },\n                filter: { type: \"object\" },\n                includeVector: { type: \"boolean\" }\n              },\n              required: [\"indexName\", \"queryVector\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Query results\"\n        }\n      }\n    }),\n    queryVectors\n  );\n  router.get(\n    \"/:vectorName/indexes\",\n    w({\n      description: \"List all indexes for a vector store\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"List of indexes\"\n        }\n      }\n    }),\n    listIndexes\n  );\n  router.get(\n    \"/:vectorName/indexes/:indexName\",\n    w({\n      description: \"Get details about a specific index\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"indexName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Index details\"\n        }\n      }\n    }),\n    describeIndex\n  );\n  router.delete(\n    \"/:vectorName/indexes/:indexName\",\n    w({\n      description: \"Delete a specific index\",\n      tags: [\"vector\"],\n      parameters: [\n        {\n          name: \"vectorName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"indexName\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Index deleted successfully\"\n        }\n      }\n    }),\n    deleteIndex\n  );\n  return router;\n}\nasync function getWorkflowsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflows = await getWorkflowsHandler$1({\n      mastra\n    });\n    return c2.json(workflows);\n  } catch (error) {\n    return handleError(error, \"Error getting workflows\");\n  }\n}\nasync function getWorkflowByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const workflow = await getWorkflowByIdHandler$1({\n      mastra,\n      workflowId\n    });\n    return c2.json(workflow);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow\");\n  }\n}\nasync function createWorkflowRunHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const prevRunId = c2.req.query(\"runId\");\n    const result = await createWorkflowRunHandler$1({\n      mastra,\n      workflowId,\n      runId: prevRunId\n    });\n    return c2.json(result);\n  } catch (e2) {\n    return handleError(e2, \"Error creating run\");\n  }\n}\nasync function startAsyncWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const { inputData } = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    const result = await startAsyncWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      inputData\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error executing workflow\");\n  }\n}\nasync function startWorkflowRunHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const { inputData } = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    await startWorkflowRunHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      inputData\n    });\n    return c2.json({ message: \"Workflow run started\" });\n  } catch (e2) {\n    return handleError(e2, \"Error starting workflow run\");\n  }\n}\nfunction watchWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const logger2 = mastra.getLogger();\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to watch workflow\" });\n    }\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await watchWorkflowHandler$1({\n            mastra,\n            workflowId,\n            runId\n          });\n          const reader = result.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(JSON.stringify(chunkResult.value) + \"\u001e\");\n          }\n        } catch (err) {\n          logger2.error(\"Error in watch stream: \" + (err?.message ?? \"Unknown error\"));\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in watch stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error watching workflow\");\n  }\n}\nasync function streamWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const logger2 = mastra.getLogger();\n    const workflowId = c2.req.param(\"workflowId\");\n    const { inputData } = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await streamWorkflowHandler$1({\n            mastra,\n            workflowId,\n            runId,\n            inputData,\n            runtimeContext\n          });\n          const reader = result.stream.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(JSON.stringify(chunkResult.value) + \"\u001e\");\n          }\n        } catch (err) {\n          logger2.error(\"Error in workflow stream: \" + (err?.message ?? \"Unknown error\"));\n        }\n        await stream6.close();\n      },\n      async (err) => {\n        logger2.error(\"Error in workflow stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error streaming workflow\");\n  }\n}\nasync function streamVNextWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const logger2 = mastra.getLogger();\n    const workflowId = c2.req.param(\"workflowId\");\n    const { inputData } = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    c2.header(\"Transfer-Encoding\", \"chunked\");\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await streamVNextWorkflowHandler$1({\n            mastra,\n            workflowId,\n            runId,\n            inputData,\n            runtimeContext\n          });\n          const reader = result.getReader();\n          stream6.onAbort(() => {\n            void reader.cancel(\"request aborted\");\n          });\n          let chunkResult;\n          while ((chunkResult = await reader.read()) && !chunkResult.done) {\n            await stream6.write(JSON.stringify(chunkResult.value) + \"\u001e\");\n          }\n        } catch (err) {\n          logger2.error(\"Error in workflow VNext stream: \" + (err?.message ?? \"Unknown error\"));\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in workflow VNext stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error streaming workflow\");\n  }\n}\nasync function resumeAsyncWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    const { step, resumeData } = await c2.req.json();\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    const result = await resumeAsyncWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      body: { step, resumeData }\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow step\");\n  }\n}\nasync function resumeWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    const { step, resumeData } = await c2.req.json();\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    await resumeWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      body: { step, resumeData }\n    });\n    return c2.json({ message: \"Workflow run resumed\" });\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow\");\n  }\n}\nasync function getWorkflowRunsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const { fromDate, toDate, limit, offset, resourceId } = c2.req.query();\n    const workflowRuns = await getWorkflowRunsHandler$1({\n      mastra,\n      workflowId,\n      fromDate: fromDate ? new Date(fromDate) : void 0,\n      toDate: toDate ? new Date(toDate) : void 0,\n      limit: limit ? Number(limit) : void 0,\n      offset: offset ? Number(offset) : void 0,\n      resourceId\n    });\n    return c2.json(workflowRuns);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow runs\");\n  }\n}\nasync function getWorkflowRunByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.param(\"runId\");\n    const workflowRun = await getWorkflowRunByIdHandler$1({\n      mastra,\n      workflowId,\n      runId\n    });\n    return c2.json(workflowRun);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow run\");\n  }\n}\nasync function getWorkflowRunExecutionResultHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.param(\"runId\");\n    const workflowRunExecutionResult = await getWorkflowRunExecutionResultHandler$1({\n      mastra,\n      workflowId,\n      runId\n    });\n    return c2.json(workflowRunExecutionResult);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow run execution result\");\n  }\n}\nasync function cancelWorkflowRunHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.param(\"runId\");\n    const result = await cancelWorkflowRunHandler$1({\n      mastra,\n      workflowId,\n      runId\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error canceling workflow run\");\n  }\n}\nasync function sendWorkflowRunEventHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.param(\"runId\");\n    const { event, data } = await c2.req.json();\n    const result = await sendWorkflowRunEventHandler$1({\n      mastra,\n      workflowId,\n      runId,\n      event,\n      data\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error sending workflow run event\");\n  }\n}\nasync function getLegacyWorkflowsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflows = await getLegacyWorkflowsHandler$1({\n      mastra\n    });\n    return c2.json(workflows);\n  } catch (error) {\n    return handleError(error, \"Error getting workflows\");\n  }\n}\nasync function getLegacyWorkflowByIdHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const workflow = await getLegacyWorkflowByIdHandler$1({\n      mastra,\n      workflowId\n    });\n    return c2.json(workflow);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow\");\n  }\n}\nasync function startAsyncLegacyWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const triggerData = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    const result = await startAsyncLegacyWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      triggerData\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error executing workflow\");\n  }\n}\nasync function createLegacyWorkflowRunHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const prevRunId = c2.req.query(\"runId\");\n    const result = await createLegacyWorkflowRunHandler$1({\n      mastra,\n      workflowId,\n      runId: prevRunId\n    });\n    return c2.json(result);\n  } catch (e2) {\n    return handleError(e2, \"Error creating run\");\n  }\n}\nasync function startLegacyWorkflowRunHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const triggerData = await c2.req.json();\n    const runId = c2.req.query(\"runId\");\n    await startLegacyWorkflowRunHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      triggerData\n    });\n    return c2.json({ message: \"Workflow run started\" });\n  } catch (e2) {\n    return handleError(e2, \"Error starting workflow run\");\n  }\n}\nfunction watchLegacyWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const logger2 = mastra.getLogger();\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to watch workflow\" });\n    }\n    return stream(\n      c2,\n      async (stream6) => {\n        try {\n          const result = await watchLegacyWorkflowHandler$1({\n            mastra,\n            workflowId,\n            runId\n          });\n          stream6.onAbort(() => {\n            if (!result.locked) {\n              return result.cancel();\n            }\n          });\n          for await (const chunk of result) {\n            await stream6.write(chunk.toString() + \"\u001e\");\n          }\n        } catch (err) {\n          console.log(err);\n        }\n      },\n      async (err) => {\n        logger2.error(\"Error in watch stream: \" + err?.message);\n      }\n    );\n  } catch (error) {\n    return handleError(error, \"Error watching workflow\");\n  }\n}\nasync function resumeAsyncLegacyWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    const { stepId, context } = await c2.req.json();\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    const result = await resumeAsyncLegacyWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      body: { stepId, context }\n    });\n    return c2.json(result);\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow step\");\n  }\n}\nasync function resumeLegacyWorkflowHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const runtimeContext = c2.get(\"runtimeContext\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const runId = c2.req.query(\"runId\");\n    const { stepId, context } = await c2.req.json();\n    if (!runId) {\n      throw new HTTPException(400, { message: \"runId required to resume workflow\" });\n    }\n    await resumeLegacyWorkflowHandler$1({\n      mastra,\n      runtimeContext,\n      workflowId,\n      runId,\n      body: { stepId, context }\n    });\n    return c2.json({ message: \"Workflow run resumed\" });\n  } catch (error) {\n    return handleError(error, \"Error resuming workflow\");\n  }\n}\nasync function getLegacyWorkflowRunsHandler(c2) {\n  try {\n    const mastra = c2.get(\"mastra\");\n    const workflowId = c2.req.param(\"workflowId\");\n    const { fromDate, toDate, limit, offset, resourceId } = c2.req.query();\n    const workflowRuns = await getLegacyWorkflowRunsHandler$1({\n      mastra,\n      workflowId,\n      fromDate: fromDate ? new Date(fromDate) : void 0,\n      toDate: toDate ? new Date(toDate) : void 0,\n      limit: limit ? Number(limit) : void 0,\n      offset: offset ? Number(offset) : void 0,\n      resourceId\n    });\n    return c2.json(workflowRuns);\n  } catch (error) {\n    return handleError(error, \"Error getting workflow runs\");\n  }\n}\n\n// src/server/handlers/routes/workflows/router.ts\nfunction workflowsRouter(bodyLimitOptions) {\n  const router = new Hono();\n  router.get(\n    \"/legacy\",\n    w({\n      description: \"Get all legacy workflows\",\n      tags: [\"legacyWorkflows\"],\n      responses: {\n        200: {\n          description: \"List of all legacy workflows\"\n        }\n      }\n    }),\n    getLegacyWorkflowsHandler\n  );\n  router.get(\n    \"/legacy/:workflowId\",\n    w({\n      description: \"Get legacy workflow by ID\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Legacy Workflow details\"\n        },\n        404: {\n          description: \"Legacy Workflow not found\"\n        }\n      }\n    }),\n    getLegacyWorkflowByIdHandler\n  );\n  router.get(\n    \"/legacy/:workflowId/runs\",\n    w({\n      description: \"Get all runs for a legacy workflow\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        { name: \"fromDate\", in: \"query\", required: false, schema: { type: \"string\", format: \"date-time\" } },\n        { name: \"toDate\", in: \"query\", required: false, schema: { type: \"string\", format: \"date-time\" } },\n        { name: \"limit\", in: \"query\", required: false, schema: { type: \"number\" } },\n        { name: \"offset\", in: \"query\", required: false, schema: { type: \"number\" } },\n        { name: \"resourceId\", in: \"query\", required: false, schema: { type: \"string\" } }\n      ],\n      responses: {\n        200: {\n          description: \"List of legacy workflow runs from storage\"\n        }\n      }\n    }),\n    getLegacyWorkflowRunsHandler\n  );\n  router.post(\n    \"/legacy/:workflowId/resume\",\n    w({\n      description: \"Resume a suspended legacy workflow step\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                stepId: { type: \"string\" },\n                context: { type: \"object\" }\n              }\n            }\n          }\n        }\n      }\n    }),\n    resumeLegacyWorkflowHandler\n  );\n  router.post(\n    \"/legacy/:workflowId/resume-async\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Resume a suspended legacy workflow step\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                stepId: { type: \"string\" },\n                context: { type: \"object\" }\n              }\n            }\n          }\n        }\n      }\n    }),\n    resumeAsyncLegacyWorkflowHandler\n  );\n  router.post(\n    \"/legacy/:workflowId/create-run\",\n    w({\n      description: \"Create a new legacy workflow run\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"New legacy workflow run created\"\n        }\n      }\n    }),\n    createLegacyWorkflowRunHandler\n  );\n  router.post(\n    \"/legacy/:workflowId/start-async\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Execute/Start a legacy workflow\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                input: { type: \"object\" }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Legacy Workflow execution result\"\n        },\n        404: {\n          description: \"Legacy Workflow not found\"\n        }\n      }\n    }),\n    startAsyncLegacyWorkflowHandler\n  );\n  router.post(\n    \"/legacy/:workflowId/start\",\n    w({\n      description: \"Create and start a new legacy workflow run\",\n      tags: [\"legacyWorkflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                input: { type: \"object\" }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"Legacy Workflow run started\"\n        },\n        404: {\n          description: \"Legacy Workflow not found\"\n        }\n      }\n    }),\n    startLegacyWorkflowRunHandler\n  );\n  router.get(\n    \"/legacy/:workflowId/watch\",\n    w({\n      description: \"Watch legacy workflow transitions in real-time\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      tags: [\"legacyWorkflows\"],\n      responses: {\n        200: {\n          description: \"Legacy Workflow transitions in real-time\"\n        }\n      }\n    }),\n    watchLegacyWorkflowHandler\n  );\n  router.get(\n    \"/\",\n    w({\n      description: \"Get all workflows\",\n      tags: [\"workflows\"],\n      responses: {\n        200: {\n          description: \"List of all workflows\"\n        }\n      }\n    }),\n    getWorkflowsHandler\n  );\n  router.get(\n    \"/:workflowId\",\n    w({\n      description: \"Get workflow by ID\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Workflow details\"\n        },\n        404: {\n          description: \"Workflow not found\"\n        }\n      }\n    }),\n    getWorkflowByIdHandler\n  );\n  router.get(\n    \"/:workflowId/runs\",\n    w({\n      description: \"Get all runs for a workflow\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        { name: \"fromDate\", in: \"query\", required: false, schema: { type: \"string\", format: \"date-time\" } },\n        { name: \"toDate\", in: \"query\", required: false, schema: { type: \"string\", format: \"date-time\" } },\n        { name: \"limit\", in: \"query\", required: false, schema: { type: \"number\" } },\n        { name: \"offset\", in: \"query\", required: false, schema: { type: \"number\" } },\n        { name: \"resourceId\", in: \"query\", required: false, schema: { type: \"string\" } }\n      ],\n      responses: {\n        200: {\n          description: \"List of workflow runs from storage\"\n        }\n      }\n    }),\n    getWorkflowRunsHandler\n  );\n  router.get(\n    \"/:workflowId/runs/:runId/execution-result\",\n    w({\n      description: \"Get execution result for a workflow run\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Workflow run execution result\"\n        },\n        404: {\n          description: \"Workflow run execution result not found\"\n        }\n      }\n    }),\n    getWorkflowRunExecutionResultHandler\n  );\n  router.get(\n    \"/:workflowId/runs/:runId\",\n    w({\n      description: \"Get workflow run by ID\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Workflow run by ID\"\n        },\n        404: {\n          description: \"Workflow run not found\"\n        }\n      }\n    }),\n    getWorkflowRunByIdHandler\n  );\n  router.post(\n    \"/:workflowId/resume\",\n    w({\n      description: \"Resume a suspended workflow step\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                step: {\n                  oneOf: [{ type: \"string\" }, { type: \"array\", items: { type: \"string\" } }]\n                },\n                resumeData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              },\n              required: [\"step\"]\n            }\n          }\n        }\n      }\n    }),\n    resumeWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/resume-async\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Resume a suspended workflow step\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                step: {\n                  oneOf: [{ type: \"string\" }, { type: \"array\", items: { type: \"string\" } }]\n                },\n                resumeData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              },\n              required: [\"step\"]\n            }\n          }\n        }\n      }\n    }),\n    resumeAsyncWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/stream\",\n    w({\n      description: \"Stream workflow in real-time\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                inputData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"workflow run started\"\n        },\n        404: {\n          description: \"workflow not found\"\n        }\n      },\n      tags: [\"workflows\"]\n    }),\n    streamWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/streamVNext\",\n    w({\n      description: \"Stream workflow in real-time using the VNext streaming API\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                inputData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"workflow run started\"\n        },\n        404: {\n          description: \"workflow not found\"\n        }\n      },\n      tags: [\"workflows\"]\n    }),\n    streamVNextWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/create-run\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Create a new workflow run\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"New workflow run created\"\n        }\n      }\n    }),\n    createWorkflowRunHandler\n  );\n  router.post(\n    \"/:workflowId/start-async\",\n    bodyLimit(bodyLimitOptions),\n    w({\n      description: \"Execute/Start a workflow\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                inputData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"workflow execution result\"\n        },\n        404: {\n          description: \"workflow not found\"\n        }\n      }\n    }),\n    startAsyncWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/start\",\n    w({\n      description: \"Create and start a new workflow run\",\n      tags: [\"workflows\"],\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                inputData: { type: \"object\" },\n                runtimeContext: {\n                  type: \"object\",\n                  description: \"Runtime context for the workflow execution\"\n                }\n              }\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"workflow run started\"\n        },\n        404: {\n          description: \"workflow not found\"\n        }\n      }\n    }),\n    startWorkflowRunHandler\n  );\n  router.get(\n    \"/:workflowId/watch\",\n    w({\n      description: \"Watch workflow transitions in real-time\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"query\",\n          required: false,\n          schema: { type: \"string\" }\n        }\n      ],\n      tags: [\"workflows\"],\n      responses: {\n        200: {\n          description: \"workflow transitions in real-time\"\n        }\n      }\n    }),\n    watchWorkflowHandler\n  );\n  router.post(\n    \"/:workflowId/runs/:runId/cancel\",\n    w({\n      description: \"Cancel a workflow run\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      tags: [\"workflows\"],\n      responses: {\n        200: {\n          description: \"workflow run cancelled\"\n        }\n      }\n    }),\n    cancelWorkflowRunHandler\n  );\n  router.post(\n    \"/:workflowId/runs/:runId/send-event\",\n    w({\n      description: \"Send an event to a workflow run\",\n      parameters: [\n        {\n          name: \"workflowId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        },\n        {\n          name: \"runId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: { type: \"object\", properties: { event: { type: \"string\" }, data: { type: \"object\" } } }\n          }\n        }\n      },\n      tags: [\"workflows\"],\n      responses: {\n        200: {\n          description: \"workflow run event sent\"\n        }\n      }\n    }),\n    sendWorkflowRunEventHandler\n  );\n  return router;\n}\n\n// src/server/welcome.ts\nvar html2 = `\n<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Welcome to Mastra</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/inter-ui/3.19.3/inter.min.css\" />\n    <style>\n      body {\n        margin: 0;\n        padding: 0;\n        background-color: #0d0d0d;\n        color: #ffffff;\n        font-family:\n          'Inter',\n          -apple-system,\n          BlinkMacSystemFont,\n          system-ui,\n          sans-serif;\n        min-height: 100vh;\n        display: flex;\n        flex-direction: column;\n      }\n\n      main {\n        flex: 1;\n        display: flex;\n        flex-direction: column;\n        align-items: center;\n        justify-content: center;\n        padding: 2rem;\n        text-align: center;\n      }\n\n      h1 {\n        font-size: 4rem;\n        font-weight: 600;\n        margin: 0 0 1rem 0;\n        background: linear-gradient(to right, #fff, #ccc);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        line-height: 1.2;\n      }\n\n      .subtitle {\n        color: #9ca3af;\n        font-size: 1.25rem;\n        max-width: 600px;\n        margin: 0 auto 3rem auto;\n        line-height: 1.6;\n      }\n\n      .docs-link {\n        background-color: #1a1a1a;\n        padding: 1rem 2rem;\n        border-radius: 0.5rem;\n        display: flex;\n        align-items: center;\n        gap: 1rem;\n        font-family: monospace;\n        font-size: 1rem;\n        color: #ffffff;\n        text-decoration: none;\n        transition: background-color 0.2s;\n      }\n\n      .docs-link:hover {\n        background-color: #252525;\n      }\n\n      .arrow-icon {\n        transition: transform 0.2s;\n      }\n\n      .docs-link:hover .arrow-icon {\n        transform: translateX(4px);\n      }\n    </style>\n  </head>\n  <body>\n    <main>\n      <h1>Welcome to Mastra</h1>\n      <p class=\"subtitle\">\n        From the team that brought you Gatsby: prototype and productionize AI features with a modern JS/TS stack.\n      </p>\n\n      <a href=\"https://mastra.ai/docs\" class=\"docs-link\">\n        Browse the docs\n        <svg\n          class=\"arrow-icon\"\n          width=\"20\"\n          height=\"20\"\n          viewBox=\"0 0 24 24\"\n          fill=\"none\"\n          stroke=\"currentColor\"\n          strokeWidth=\"2\"\n        >\n          <path d=\"M5 12h14M12 5l7 7-7 7\" />\n        </svg>\n      </a>\n    </main>\n  </body>\n</html>\n`;\n\n// src/server/index.ts\nfunction getToolExports(tools) {\n  try {\n    return tools.reduce((acc, toolModule) => {\n      Object.entries(toolModule).forEach(([key, tool]) => {\n        if (tool instanceof Tool) {\n          acc[key] = tool;\n        }\n      });\n      return acc;\n    }, {});\n  } catch (err) {\n    console.error(\n      `Failed to import tools\nreason: ${err.message}\n${err.stack.split(\"\\n\").slice(1).join(\"\\n\")}\n    `,\n      err\n    );\n  }\n}\nasync function createHonoServer(mastra, options = {\n  tools: {}\n}) {\n  const app = new Hono();\n  const server = mastra.getServer();\n  const a2aTaskStore = new InMemoryTaskStore();\n  app.use(\"*\", async function setTelemetryInfo(c2, next) {\n    const requestId = c2.req.header(\"x-request-id\") ?? randomUUID();\n    const span = Telemetry.getActiveSpan();\n    if (span) {\n      span.setAttribute(\"http.request_id\", requestId);\n      span.updateName(`${c2.req.method} ${c2.req.path}`);\n      const newCtx = Telemetry.setBaggage({\n        \"http.request_id\": { value: requestId }\n      });\n      await new Promise((resolve) => {\n        Telemetry.withContext(newCtx, async () => {\n          await next();\n          resolve(true);\n        });\n      });\n    } else {\n      await next();\n    }\n  });\n  app.onError((err, c2) => errorHandler(err, c2, options.isDev));\n  app.use(\"*\", async function setContext(c2, next) {\n    let runtimeContext = new RuntimeContext();\n    if (c2.req.method === \"POST\" || c2.req.method === \"PUT\") {\n      const contentType = c2.req.header(\"content-type\");\n      if (contentType?.includes(\"application/json\")) {\n        try {\n          const clonedReq = c2.req.raw.clone();\n          const body = await clonedReq.json();\n          if (body.runtimeContext) {\n            runtimeContext = new RuntimeContext(Object.entries(body.runtimeContext));\n          }\n        } catch {\n        }\n      }\n    }\n    c2.set(\"runtimeContext\", runtimeContext);\n    c2.set(\"mastra\", mastra);\n    c2.set(\"tools\", options.tools);\n    c2.set(\"taskStore\", a2aTaskStore);\n    c2.set(\"playground\", options.playground === true);\n    c2.set(\"isDev\", options.isDev === true);\n    return next();\n  });\n  const serverMiddleware = mastra.getServerMiddleware?.();\n  if (serverMiddleware && serverMiddleware.length > 0) {\n    for (const m2 of serverMiddleware) {\n      app.use(m2.path, m2.handler);\n    }\n  }\n  if (server?.cors === false) {\n    app.use(\"*\", timeout(server?.timeout ?? 3 * 60 * 1e3));\n  } else {\n    const corsConfig = {\n      origin: \"*\",\n      allowMethods: [\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\", \"OPTIONS\"],\n      credentials: false,\n      maxAge: 3600,\n      ...server?.cors,\n      allowHeaders: [\"Content-Type\", \"Authorization\", \"x-mastra-client-type\", ...server?.cors?.allowHeaders ?? []],\n      exposeHeaders: [\"Content-Length\", \"X-Requested-With\", ...server?.cors?.exposeHeaders ?? []]\n    };\n    app.use(\"*\", timeout(server?.timeout ?? 3 * 60 * 1e3), cors(corsConfig));\n  }\n  app.use(\"*\", authenticationMiddleware);\n  app.use(\"*\", authorizationMiddleware);\n  const bodyLimitOptions = {\n    maxSize: server?.bodySizeLimit ?? 4.5 * 1024 * 1024,\n    // 4.5 MB,\n    onError: (c2) => c2.json({ error: \"Request body too large\" }, 413)\n  };\n  const routes = server?.apiRoutes;\n  if (server?.middleware) {\n    const normalizedMiddlewares = Array.isArray(server.middleware) ? server.middleware : [server.middleware];\n    const middlewares = normalizedMiddlewares.map((middleware2) => {\n      if (typeof middleware2 === \"function\") {\n        return {\n          path: \"*\",\n          handler: middleware2\n        };\n      }\n      return middleware2;\n    });\n    for (const middleware2 of middlewares) {\n      app.use(middleware2.path, middleware2.handler);\n    }\n  }\n  if (routes) {\n    for (const route of routes) {\n      const middlewares = [];\n      if (route.middleware) {\n        middlewares.push(...Array.isArray(route.middleware) ? route.middleware : [route.middleware]);\n      }\n      if (route.openapi) {\n        middlewares.push(w(route.openapi));\n      }\n      const handler = \"handler\" in route ? route.handler : await route.createHandler({ mastra });\n      if (route.method === \"GET\") {\n        app.get(route.path, ...middlewares, handler);\n      } else if (route.method === \"POST\") {\n        app.post(route.path, ...middlewares, handler);\n      } else if (route.method === \"PUT\") {\n        app.put(route.path, ...middlewares, handler);\n      } else if (route.method === \"DELETE\") {\n        app.delete(route.path, ...middlewares, handler);\n      } else if (route.method === \"PATCH\") {\n        app.patch(route.path, ...middlewares, handler);\n      } else if (route.method === \"ALL\") {\n        app.all(route.path, ...middlewares, handler);\n      }\n    }\n  }\n  if (server?.build?.apiReqLogs) {\n    app.use(logger());\n  }\n  app.get(\n    \"/.well-known/:agentId/agent-card.json\",\n    w({\n      description: \"Get agent configuration\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      responses: {\n        200: {\n          description: \"Agent configuration\"\n        }\n      }\n    }),\n    getAgentCardByIdHandler\n  );\n  app.post(\n    \"/a2a/:agentId\",\n    w({\n      description: \"Execute agent via A2A protocol\",\n      tags: [\"agents\"],\n      parameters: [\n        {\n          name: \"agentId\",\n          in: \"path\",\n          required: true,\n          schema: { type: \"string\" }\n        }\n      ],\n      requestBody: {\n        required: true,\n        content: {\n          \"application/json\": {\n            schema: {\n              type: \"object\",\n              properties: {\n                method: {\n                  type: \"string\",\n                  enum: [\"message/send\", \"message/stream\", \"tasks/get\", \"tasks/cancel\"],\n                  description: \"The A2A protocol method to execute\"\n                },\n                params: {\n                  type: \"object\",\n                  oneOf: [\n                    {\n                      // MessageSendParams\n                      type: \"object\",\n                      properties: {\n                        id: {\n                          type: \"string\",\n                          description: \"Unique identifier for the task being initiated or continued\"\n                        },\n                        sessionId: {\n                          type: \"string\",\n                          description: \"Optional identifier for the session this task belongs to\"\n                        },\n                        message: {\n                          type: \"object\",\n                          description: \"The message content to send to the agent for processing\"\n                        },\n                        pushNotification: {\n                          type: \"object\",\n                          nullable: true,\n                          description: \"Optional pushNotification information for receiving notifications about this task\"\n                        },\n                        historyLength: {\n                          type: \"integer\",\n                          nullable: true,\n                          description: \"Optional parameter to specify how much message history to include in the response\"\n                        },\n                        metadata: {\n                          type: \"object\",\n                          nullable: true,\n                          description: \"Optional metadata associated with sending this message\"\n                        }\n                      },\n                      required: [\"id\", \"message\"]\n                    },\n                    {\n                      // TaskQueryParams\n                      type: \"object\",\n                      properties: {\n                        id: { type: \"string\", description: \"The unique identifier of the task\" },\n                        historyLength: {\n                          type: \"integer\",\n                          nullable: true,\n                          description: \"Optional history length to retrieve for the task\"\n                        },\n                        metadata: {\n                          type: \"object\",\n                          nullable: true,\n                          description: \"Optional metadata to include with the operation\"\n                        }\n                      },\n                      required: [\"id\"]\n                    },\n                    {\n                      // TaskIdParams\n                      type: \"object\",\n                      properties: {\n                        id: { type: \"string\", description: \"The unique identifier of the task\" },\n                        metadata: {\n                          type: \"object\",\n                          nullable: true,\n                          description: \"Optional metadata to include with the operation\"\n                        }\n                      },\n                      required: [\"id\"]\n                    }\n                  ]\n                }\n              },\n              required: [\"method\", \"params\"]\n            }\n          }\n        }\n      },\n      responses: {\n        200: {\n          description: \"A2A response\"\n        },\n        400: {\n          description: \"Missing or invalid request parameters\"\n        },\n        404: {\n          description: \"Agent not found\"\n        }\n      }\n    }),\n    getAgentExecutionHandler\n  );\n  app.get(\n    \"/api\",\n    w({\n      description: \"Get API status\",\n      tags: [\"system\"],\n      responses: {\n        200: {\n          description: \"Success\"\n        }\n      }\n    }),\n    rootHandler\n  );\n  app.get(\n    \"/api/model-providers\",\n    w({\n      description: \"Get all model providers with available keys\",\n      tags: [\"agents\"],\n      responses: {\n        200: {\n          description: \"All model providers with available keys\"\n        }\n      }\n    }),\n    getModelProvidersHandler\n  );\n  app.route(\"/api/agents\", agentsRouter(bodyLimitOptions));\n  app.route(\"/api/networks\", vNextNetworksRouter(bodyLimitOptions));\n  app.route(\"/api/networks\", networksRouter(bodyLimitOptions));\n  if (options.isDev) {\n    app.route(\"/api/agents\", agentsRouterDev(bodyLimitOptions));\n  }\n  app.route(\"/api/mcp\", mcpRouter(bodyLimitOptions));\n  app.route(\"/api/memory\", memoryRoutes(bodyLimitOptions));\n  app.route(\"/api/telemetry\", telemetryRouter());\n  app.route(\"/api/observability\", observabilityRouter());\n  app.route(\"/api/workflows\", workflowsRouter(bodyLimitOptions));\n  app.route(\"/api/logs\", logsRouter());\n  app.route(\"/api/scores\", scoresRouter(bodyLimitOptions));\n  app.route(\"/api/tools\", toolsRouter(bodyLimitOptions, options.tools));\n  app.route(\"/api/vector\", vectorRouter(bodyLimitOptions));\n  if (options?.isDev || server?.build?.openAPIDocs || server?.build?.swaggerUI) {\n    app.get(\n      \"/openapi.json\",\n      h(app, {\n        includeEmptyPaths: true,\n        documentation: {\n          info: { title: \"Mastra API\", version: \"1.0.0\", description: \"Mastra API\" }\n        }\n      })\n    );\n  }\n  if (options?.isDev || server?.build?.swaggerUI) {\n    app.get(\n      \"/swagger-ui\",\n      w({\n        hide: true\n      }),\n      middleware({ url: \"/openapi.json\" })\n    );\n  }\n  if (options?.playground) {\n    app.get(\n      \"/refresh-events\",\n      w({\n        hide: true\n      }),\n      handleClientsRefresh\n    );\n    app.post(\n      \"/__refresh\",\n      w({\n        hide: true\n      }),\n      handleTriggerClientsRefresh\n    );\n    app.use(\"/assets/*\", async (c2, next) => {\n      const path = c2.req.path;\n      if (path.endsWith(\".js\")) {\n        c2.header(\"Content-Type\", \"application/javascript\");\n      } else if (path.endsWith(\".css\")) {\n        c2.header(\"Content-Type\", \"text/css\");\n      }\n      await next();\n    });\n    app.use(\n      \"/assets/*\",\n      serveStatic({\n        root: \"./playground/assets\"\n      })\n    );\n  }\n  app.get(\"*\", async (c2, next) => {\n    if (c2.req.path.startsWith(\"/api/\") || c2.req.path.startsWith(\"/swagger-ui\") || c2.req.path.startsWith(\"/openapi.json\")) {\n      return await next();\n    }\n    const path = c2.req.path;\n    if (path.includes(\".\") && !path.endsWith(\".html\")) {\n      return await next();\n    }\n    if (options?.playground) {\n      let indexHtml = await readFile(join(process.cwd(), \"./playground/index.html\"), \"utf-8\");\n      indexHtml = indexHtml.replace(\n        `'%%MASTRA_TELEMETRY_DISABLED%%'`,\n        `${Boolean(process.env.MASTRA_TELEMETRY_DISABLED)}`\n      );\n      const serverOptions = mastra.getServer();\n      const port = serverOptions?.port ?? (Number(process.env.PORT) || 4111);\n      const host = serverOptions?.host ?? \"localhost\";\n      indexHtml = indexHtml.replace(`'%%MASTRA_SERVER_HOST%%'`, `'${host}'`);\n      indexHtml = indexHtml.replace(`'%%MASTRA_SERVER_PORT%%'`, `'${port}'`);\n      return c2.newResponse(indexHtml, 200, { \"Content-Type\": \"text/html\" });\n    }\n    return c2.newResponse(html2, 200, { \"Content-Type\": \"text/html\" });\n  });\n  if (options?.playground) {\n    app.use(\n      \"*\",\n      serveStatic({\n        root: \"./playground\"\n      })\n    );\n  }\n  return app;\n}\nasync function createNodeServer(mastra, options = { tools: {} }) {\n  const app = await createHonoServer(mastra, options);\n  const serverOptions = mastra.getServer();\n  const port = serverOptions?.port ?? (Number(process.env.PORT) || 4111);\n  const server = serve(\n    {\n      fetch: app.fetch,\n      port,\n      hostname: serverOptions?.host\n    },\n    () => {\n      const logger2 = mastra.getLogger();\n      const host = serverOptions?.host ?? \"localhost\";\n      logger2.info(` Mastra API running on port http://${host}:${port}/api`);\n      if (options?.playground) {\n        const playgroundUrl = `http://${host}:${port}`;\n        logger2.info(`\\u{1F468}\\u200D\\u{1F4BB} Playground available at ${playgroundUrl}`);\n      }\n      if (process.send) {\n        process.send({\n          type: \"server-ready\",\n          port,\n          host\n        });\n      }\n    }\n  );\n  await mastra.startEventEngine();\n  return server;\n}\n\nexport { createHonoServer, createNodeServer, getToolExports };\n//# sourceMappingURL=index.js.map\n//# sourceMappingURL=index.js.map","// @ts-ignore\n// @ts-ignore\nimport { evaluate } from '@mastra/core/eval';\nimport { AvailableHooks, registerHook } from '@mastra/core/hooks';\nimport { TABLE_EVALS } from '@mastra/core/storage';\nimport { checkEvalStorageFields } from '@mastra/core/utils';\nimport { mastra } from '#mastra';\nimport { createNodeServer, getToolExports } from '#server';\nimport { tools } from '#tools';\n// @ts-ignore\nawait createNodeServer(mastra, {\n  playground: true,\n  isDev: true,\n  tools: getToolExports(tools),\n});\n\nregisterHook(AvailableHooks.ON_GENERATION, ({ input, output, metric, runId, agentName, instructions }) => {\n  evaluate({\n    agentName,\n    input,\n    metric,\n    output,\n    runId,\n    globalRunId: runId,\n    instructions,\n  });\n});\n\nregisterHook(AvailableHooks.ON_EVALUATION, async traceObject => {\n  const storage = mastra.getStorage();\n  if (storage) {\n    // Check for required fields\n    const logger = mastra?.getLogger();\n    const areFieldsValid = checkEvalStorageFields(traceObject, logger);\n    if (!areFieldsValid) return;\n\n    await storage.insert({\n      tableName: TABLE_EVALS,\n      record: {\n        input: traceObject.input,\n        output: traceObject.output,\n        result: JSON.stringify(traceObject.result || {}),\n        agent_name: traceObject.agentName,\n        metric_name: traceObject.metricName,\n        instructions: traceObject.instructions,\n        test_info: null,\n        global_run_id: traceObject.globalRunId,\n        run_id: traceObject.runId,\n        created_at: new Date().toISOString(),\n      },\n    });\n  }\n});\n"],"names":["sharedPostgresStorage","jobApplicationAgent","jobApplicationWorkflow","jobSearchTool","resumeParserTool","applicationFillerTool","sheetsTrackerTool","githubPushTool","logger","method","url","error","mastra","inngest","registerCronWorkflow"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAGO;AAAgD;AAGvD;;ACFO;AAAoB;AAMrB;AACM;AACK;AACF;AAC0B;AAEzC;;ACRA;AAAM;AACY;AAKX;AAGL;AAA8B;AACzB;AACU;AACD;AACiB;AAC7B;AAEJ;AAKA;AAuDO;AACL;AAAkB;AACK;AACsC;AAEzD;AACA;AACA;AAAO;AACT;AAEF;AACF;AAEO;AAAsB;AAC3B;AAEF;AAIE;AAEA;AACA;AACE;AACE;AAAA;AAEF;AACA;AACE;AAAe;AACjB;AAEF;AACE;AAAgB;AAElB;AACA;AAKE;AAAY;AAEd;AAA4B;AAClB;AACuB;AAC/B;AAEJ;;AC3HA;AAA4B;AACc;AAE1C;AAEO;AAAsC;AACrC;;AACQ;AAAA;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAgBQ;AACf;AACL;AACA;AACA;AACA;AACF;AACmB;AACR;AACE;AACQ;AACjB;AACc;AAChB;AACS;AAEb;;AC3CA;AAAkC;AAC5B;AACS;AACS;AACiE;AACqB;AAC3G;AACsB;AACE;AACL;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACjB;AACiB;AACpB;AAEC;AACA;AAEA;AAEA;AAEA;AAAoD;AAClD;AACQ;AAC2D;AACnE;AACC;AACW;AACsB;AACxB;AACV;AAGF;AAGA;AAAiB;AACf;AACS;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACV;AACA;AACS;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACV;AAGF;AAAO;AACC;AACa;AACrB;AAEJ;AAEA;AAAmC;AAC7B;AACS;AACS;AACoE;AACjE;AACL;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACjB;AACiB;AACpB;AACsB;AACE;AACL;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACjB;AACiB;AACE;AACI;AACN;AACC;AACA;AACG;AACW;AACF;AAC7B;AACiB;AACW;AACX;AACE;AACC;AACG;AACtB;AACwB;AACE;AACT;AACK;AACP;AACf;AACyB;AACV;AACO;AACU;AAChC;AACH;AACF;AAEC;AACA;AAEA;AAEA;AAEA;AAAoD;AAClD;AACQ;AACoC;AAC5C;AACC;AACW;AACwB;AAC1B;AACV;AAGF;AAGA;AAAuB;AACP;AACN;AACC;AACA;AACG;AACA;AACF;AACV;AACS;AACG;AACV;AACS;AACE;AACC;AACG;AACf;AACF;AACuF;AAC5E;AACT;AACU;AACK;AACP;AACR;AACF;AACU;AACR;AACQ;AACO;AAC+B;AAC9C;AACF;AAGF;AAAO;AACL;AACA;AACY;AACd;AAEJ;AAEA;AAAmC;AAC7B;AACS;AACS;AACG;AACL;AACE;AACC;AACG;AACR;AACO;AACA;AACJ;AACjB;AACmB;AACI;AACN;AACC;AACA;AACG;AACW;AACF;AAC7B;AACiB;AACW;AACX;AACE;AACC;AACG;AACtB;AACwB;AACE;AACT;AACK;AACP;AACf;AACyB;AACV;AACO;AACU;AAChC;AACH;AACiH;AACnH;AACsB;AACF;AACD;AACc;AACV;AACvB;AAEC;AACA;AAGA;AACE;AACA;AAAO;AACI;AACA;AACc;AACV;AACf;AAGF;AAA+D;AAC9C;AACf;AAGF;AAGA;AACA;AAEA;AACE;AAA4D;AAC5C;AACD;AACD;AAGd;AAAoD;AAClD;AACQ;AACiH;AACzH;AACC;AACW;AACyD;AAC3D;AACV;AAIF;AACA;AACE;AAAA;AAGF;AAAwB;AACR;AACD;AACC;AACF;AACA;AACZ;AACkD;AACwD;AAG5G;AAA0D;AAC1C;AACN;AACT;AAIH;AAAsE;AACpE;AACQ;AACoG;AAC5G;AACC;AACW;AACoB;AACtB;AACV;AAGF;AAEA;AAAuE;AACrE;AACgB;AAChB;AAGF;AAAO;AACI;AACwR;AACjS;AACA;AACF;AAEJ;AAEO;AAA8C;AAC/C;AACS;AACW;AAAA;AACD;AACF;AACD;AACc;AACV;AAE1B;;AC5SO;AAA0B;AACtBA;AACD;AAAEC;AAAoB;AACnB;AAAEC;AAAuB;AACxB;AACc;AAChB;AACG;AACF;AACLC;AACAC;AACAC;AACAC;AACAC;AACF;AACD;AACH;AACS;AAAA;AAAA;AAAA;AASL;AAAA;AAGS;AACb;AACQ;AACA;AACA;AAGF;AACA;AACAC;AAA2B;AAAgBC;AAAmBC;AAC9D;AACE;AAAW;AAEXF;AAA4B;AACZC;AACHC;AACXC;AAEF;AACE;AAGE;AAA2C;AAASA;AAAO;AAC7D;AAGA;AAA2C;AAASA;AAAO;AAG7D;AAAMA;AACR;AACF;AAES;AAAA;AAET;AACQ;AACE;AACc;AAAEC;AAA0B;AAAEA;AAAQC;AAAS;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASvE;AAAA;AAEJ;AAOqB;AACP;AACC;AAEjB;AAIAC;AAOA;AACE;AAGF;AAIA;AACE;AAGF;;AC5KA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAQA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9EA;AACA;AACA;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAGA;AACA;AACA;;AClHA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AClBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1CA;AACA;;ACDA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzMA;AAIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AChHA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACj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vDA;AASA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1MA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtDA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9JA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtBA;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACZA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrFA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjBA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACxCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrBA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACx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vdA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACTA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACb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xvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9siBA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AClDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjJA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACxEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjjBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrIA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC5MA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9IA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvJA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC/GA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvjBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrRA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC/qUA;AACA;AAQA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","x_google_ignoreList":[6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55]}